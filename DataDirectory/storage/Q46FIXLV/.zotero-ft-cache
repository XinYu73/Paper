8QGHUVWDQGLQJ 0ROHFXODU6LPXODWLRQ
)URP$OJRULWKPVWR$SSOLFDWLRQV

&RPSXWDWLRQDO6FLHQFH
)URP7KHRU\WR$SSOLFDWLRQV
6HULHV(GLWRUV 'DDQ)UHQNHO )20,QVWLWXWHIRU$WRPLFDQG0ROHFXODU3K\VLFV $PVWHUGDP7KH1HWKHUODQGV 0LFKDHO.OHLQ /DERUDWRU\IRU5HVHDUFKRQWKH6WUXFWXUHRI0DWWHU 8QLYHUVLW\RI3HQQV\OYDQLD86$ 0LFKHOH3DUULQHOOR &6&66ZLVV&HQWHUIRU6FLHQWLÀF&RPSXWLQJ(7+=XULFK 6ZLW]HUODQG %HUHQG6PLW 'HSDUWPHQWRI&KHPLFDO(QJLQHHULQJ 8QLYHUVLW\RI$PVWHUGDP $PVWHUGDP7KH1HWKHUODQGV %ULQJLQJWRJHWKHUWKHRULHVDQGWHFKQLTXHVIURPDYDULHW\RIÀHOGVDQGSUR YLGLQJGHWDLOVRQDSSO\LQJWKHVHWKHRULHVLQDPXOWLWXGHRIDSSOLFDWLRQVWKLV VHULHV SURYLGHV WLPHO\ DQG WKRURXJK FRYHUDJH RI D EURDG UDQJH RI WRSLFDO DQGLQWHUGLVFLSOLQDU\DUHDV7KHERRNVLGHQWLI\VLPLODULWLHVDFURVVGLVFLSOLQHV WKHUHE\KHOSLQJUHVHDUFKHVWDNHWKHLUZRUNLQQHZGLUHFWLRQV ZZZDFDGHPLFSUHVVFRPFRPSXWDWLRQDOVFLHQFH
9ROXPHLQWKH &20387$7,21$/6&,(1&(6(5,(6

8QGHUVWDQGLQJ 0ROHFXODU6LPXODWLRQ
)URP$OJRULWKPVWR$SSOLFDWLRQV
'DDQ)UHQNHO
)20,QVWLWXWHIRU$WRPLFDQG0ROHFXODU3K\VLFV $PVWHUGDP7KH1HWKHUODQGV
'HSDUWPHQWRI&KHPLFDO(QJLQHHULQJ )DFXOW\RI6FLHQFHV
8QLYHUVLW\RI$PVWHUGDP $PVWHUGDP7KH1HWKHUODQGV
%HUHQG6PLW
'HSDUWPHQWRI&KHPLFDO(QJLQHHULQJ )DFXOW\RI6FLHQFHV
8QLYHUVLW\RI$PVWHUGDP $PVWHUGDP7KH1HWKHUODQGV
6DQ'LHJR6DQ)UDQFLVFR1HZ<RUN %RVWRQ/RQGRQ6\GQH\7RN\R

7KLVERRNLVSULQWHGRQDFLGIUHHSDSHU
&RS\ULJKWE\$&$'(0,&35(66
$OO5LJKWV5HVHUYHG 1RSDUWRIWKLVSXEOLFDWLRQPD\EHUHSURGXFHGRUWUDQVPLWWHGLQ DQ\IRUPRUE\DQ\PHDQVHOHFWURQLFRUPHFKDQLFDOLQFOXGLQJ SKRWRFRS\LQJUHFRUGLQJRUDQ\LQIRUPDWLRQVWRUDJHDQGUHWULHYDO
V\VWHPZLWKRXWSHUPLVVLRQLQZULWLQJIURPWKHSXEOLVKHU
5HTXHVWVIRUSHUPLVVLRQWRPDNHFRSLHVRIDQ\SDUWRIWKH ZRUNVKRXOGEHPDLOHGWRWKHIROORZLQJDGGUHVV3HUPLVVLRQV GHSDUWPHQW+DUFRXUW,QF6HD+DUERU'ULYH2UODQGR
)ORULGD86$
$FDGHPLF3UHVV $GLYLVLRQRI+DUFRXUW,QF %6WUHHW6XLWH6DQ'LHJR&DOLIRUQLD86$ KWWSZZZDFDGHPLFSUHVVFRP
$FDGHPLF3UHVV $GLYLVLRQRI+DUFRXUW,QF +DUFRXUW3ODFH-DPHVWRZQ5RDG/RQGRQ1:%<8. KWWSZZZDFDGHPLFSUHVVFRP
,6%1
/LEUDU\RI&RQJUHVV&DWDORJ1XPEHU
$FDWDORJXHUHFRUGIRUWKLVERRNLVDYDLODEOHIURPWKH%ULWLVK/LEUDU\
&RYHULOOXVWUDWLRQDGLPHWK\ORFWDQHPROHFXOHLQVLGHD SRUHRID721W\SH]HROLWHÀJXUHE\'DYLG'XEEHOGDP
7\SHVHWE\WKHDXWKRUV 3ULQWHGDQGERXQGLQ*UHDW%ULWDLQE\03*%RRNV/WG%RGPLQ&RUQZDOO
03

Contents

Preface to the Second Edition

xiii

Preface

xv

List of Symbols

xix

1 Introduction

1

Part I Basics

7

2 Statistical Mechanics

9

2.1 Entropy and Temperature . . . . . . . . . . . . . . . . . . . . . 9

2.2 Classical Statistical Mechanics . . . . . . . . . . . . . . . . . . . 13

2.2.1 Ergodicity . . . . . . . . . . . . . . . . . . . . . . . . . . 15

2.3 Questions and Exercises . . . . . . . . . . . . . . . . . . . . . . 17

3 Monte Carlo Simulations

23

3.1 The Monte Carlo Method . . . . . . . . . . . . . . . . . . . . . 23

3.1.1 Importance Sampling . . . . . . . . . . . . . . . . . . . 24

3.1.2 The Metropolis Method . . . . . . . . . . . . . . . . . . 27

3.2 A Basic Monte Carlo Algorithm . . . . . . . . . . . . . . . . . . 31

3.2.1 The Algorithm . . . . . . . . . . . . . . . . . . . . . . . 31

3.2.2 Technical Details . . . . . . . . . . . . . . . . . . . . . . 32

3.2.3 Detailed Balance versus Balance . . . . . . . . . . . . . 42

3.3 Trial Moves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

3.3.1 Translational Moves . . . . . . . . . . . . . . . . . . . . 43

3.3.2 Orientational Moves . . . . . . . . . . . . . . . . . . . . 48

3.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

3.5 Questions and Exercises . . . . . . . . . . . . . . . . . . . . . . 58

vi

Contents

4 Molecular Dynamics Simulations

63

4.1 Molecular Dynamics: The Idea . . . . . . . . . . . . . . . . . . 63

4.2 Molecular Dynamics: A Program . . . . . . . . . . . . . . . . . 64

4.2.1 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . 65

4.2.2 The Force Calculation . . . . . . . . . . . . . . . . . . . 67

4.2.3 Integrating the Equations of Motion . . . . . . . . . . . 69

4.3 Equations of Motion . . . . . . . . . . . . . . . . . . . . . . . . 71

4.3.1 Other Algorithms . . . . . . . . . . . . . . . . . . . . . . 74

4.3.2 Higher-Order Schemes . . . . . . . . . . . . . . . . . . . 77

4.3.3 Liouville Formulation of Time-Reversible Algorithms . 77

4.3.4 Lyapunov Instability . . . . . . . . . . . . . . . . . . . . 81

4.3.5 One More Way to Look at the Verlet Algorithm... . . . . 82

4.4 Computer Experiments . . . . . . . . . . . . . . . . . . . . . . . 84

4.4.1 Diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.4.2 Order-Ò Algorithm to Measure Correlations . . . . . . 90

4.5 Some Applications . . . . . . . . . . . . . . . . . . . . . . . . . 97

4.6 Questions and Exercises . . . . . . . . . . . . . . . . . . . . . . 105

Part II Ensembles

109

5 Monte Carlo Simulations in Various Ensembles

111

5.1 General Approach . . . . . . . . . . . . . . . . . . . . . . . . . . 112

5.2 Canonical Ensemble . . . . . . . . . . . . . . . . . . . . . . . . 112

5.2.1 Monte Carlo Simulations . . . . . . . . . . . . . . . . . 113

5.2.2 Justiﬁcation of the Algorithm . . . . . . . . . . . . . . . 114

5.3 Microcanonical Monte Carlo . . . . . . . . . . . . . . . . . . . . 114

5.4 Isobaric-Isothermal Ensemble . . . . . . . . . . . . . . . . . . . 115

5.4.1 Statistical Mechanical Basis . . . . . . . . . . . . . . . . 116

5.4.2 Monte Carlo Simulations . . . . . . . . . . . . . . . . . 119

5.4.3 Applications . . . . . . . . . . . . . . . . . . . . . . . . . 122

5.5 Isotension-Isothermal Ensemble . . . . . . . . . . . . . . . . . . 125

5.6 Grand-Canonical Ensemble . . . . . . . . . . . . . . . . . . . . 126

5.6.1 Statistical Mechanical Basis . . . . . . . . . . . . . . . . 127

5.6.2 Monte Carlo Simulations . . . . . . . . . . . . . . . . . 130

5.6.3 Justiﬁcation of the Algorithm . . . . . . . . . . . . . . . 130

5.6.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . 133

5.7 Questions and Exercises . . . . . . . . . . . . . . . . . . . . . . 135

6 Molecular Dynamics in Various Ensembles

139

6.1 Molecular Dynamics at Constant Temperature . . . . . . . . . 140

6.1.1 The Andersen Thermostat . . . . . . . . . . . . . . . . . 141

6.1.2 Nose´-Hoover Thermostat . . . . . . . . . . . . . . . . . 147

Contents

vii

6.1.3 Nose´-Hoover Chains . . . . . . . . . . . . . . . . . . . . 155 6.2 Molecular Dynamics at Constant Pressure . . . . . . . . . . . . 158 6.3 Questions and Exercises . . . . . . . . . . . . . . . . . . . . . . 160

Part III Free Energies and Phase Equilibria

165

7 Free Energy Calculations

167

7.1 Thermodynamic Integration . . . . . . . . . . . . . . . . . . . . 168

7.2 Chemical Potentials . . . . . . . . . . . . . . . . . . . . . . . . . 172

7.2.1 The Particle Insertion Method . . . . . . . . . . . . . . . 173

7.2.2 Other Ensembles . . . . . . . . . . . . . . . . . . . . . . 176

7.2.3 Overlapping Distribution Method . . . . . . . . . . . . 179

7.3 Other Free Energy Methods . . . . . . . . . . . . . . . . . . . . 183

7.3.1 Multiple Histograms . . . . . . . . . . . . . . . . . . . . 183

7.3.2 Acceptance Ratio Method . . . . . . . . . . . . . . . . . 189

7.4 Umbrella Sampling . . . . . . . . . . . . . . . . . . . . . . . . . 192

7.4.1 Nonequilibrium Free Energy Methods . . . . . . . . . . 196

7.5 Questions and Exercises . . . . . . . . . . . . . . . . . . . . . . 199

8 The Gibbs Ensemble

201

8.1 The Gibbs Ensemble Technique . . . . . . . . . . . . . . . . . . 203

8.2 The Partition Function . . . . . . . . . . . . . . . . . . . . . . . 204

8.3 Monte Carlo Simulations . . . . . . . . . . . . . . . . . . . . . . 205

8.3.1 Particle Displacement . . . . . . . . . . . . . . . . . . . 205

8.3.2 Volume Change . . . . . . . . . . . . . . . . . . . . . . . 206

8.3.3 Particle Exchange . . . . . . . . . . . . . . . . . . . . . . 208

8.3.4 Implementation . . . . . . . . . . . . . . . . . . . . . . . 208

8.3.5 Analyzing the Results . . . . . . . . . . . . . . . . . . . 214

8.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220

8.5 Questions and Exercises . . . . . . . . . . . . . . . . . . . . . . 223

9 Other Methods to Study Coexistence

225

9.1 Semigrand Ensemble . . . . . . . . . . . . . . . . . . . . . . . . 225

9.2 Tracing Coexistence Curves . . . . . . . . . . . . . . . . . . . . 233

10 Free Energies of Solids

241

10.1 Thermodynamic Integration . . . . . . . . . . . . . . . . . . . . 242

10.2 Free Energies of Solids . . . . . . . . . . . . . . . . . . . . . . . 243

10.2.1 Atomic Solids with Continuous Potentials . . . . . . . 244

10.3 Free Energies of Molecular Solids . . . . . . . . . . . . . . . . . 245

10.3.1 Atomic Solids with Discontinuous Potentials . . . . . . 248

10.3.2 General Implementation Issues . . . . . . . . . . . . . . 249

10.4 Vacancies and Interstitials . . . . . . . . . . . . . . . . . . . . . 263

viii

Contents

10.4.1 Free Energies . . . . . . . . . . . . . . . . . . . . . . . . 263 10.4.2 Numerical Calculations . . . . . . . . . . . . . . . . . . 266

11 Free Energy of Chain Molecules

269

11.1 Chemical Potential as Reversible Work . . . . . . . . . . . . . . 269

11.2 Rosenbluth Sampling . . . . . . . . . . . . . . . . . . . . . . . . 271

11.2.1 Macromolecules with Discrete Conformations . . . . . 271

11.2.2 Extension to Continuously Deformable Molecules . . . 276

11.2.3 Overlapping Distribution Rosenbluth Method . . . . . 282

11.2.4 Recursive Sampling . . . . . . . . . . . . . . . . . . . . 283

11.2.5 Pruned-Enriched Rosenbluth Method . . . . . . . . . . 285

Part IV Advanced Techniques

289

12 Long-Range Interactions

291

12.1 Ewald Sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292

12.1.1 Point Charges . . . . . . . . . . . . . . . . . . . . . . . . 292

12.1.2 Dipolar Particles . . . . . . . . . . . . . . . . . . . . . . 300

12.1.3 Dielectric Constant . . . . . . . . . . . . . . . . . . . . . 301

12.1.4 Boundary Conditions . . . . . . . . . . . . . . . . . . . 303

12.1.5 Accuracy and Computational Complexity . . . . . . . 304

12.2 Fast Multipole Method . . . . . . . . . . . . . . . . . . . . . . . 306

12.3 Particle Mesh Approaches . . . . . . . . . . . . . . . . . . . . . 310

12.4 Ewald Summation in a Slab Geometry . . . . . . . . . . . . . . 316

13 Biased Monte Carlo Schemes

321

13.1 Biased Sampling Techniques . . . . . . . . . . . . . . . . . . . . 322

13.1.1 Beyond Metropolis . . . . . . . . . . . . . . . . . . . . . 323

13.1.2 Orientational Bias . . . . . . . . . . . . . . . . . . . . . . 323

13.2 Chain Molecules . . . . . . . . . . . . . . . . . . . . . . . . . . . 331

13.2.1 Conﬁgurational-Bias Monte Carlo . . . . . . . . . . . . 331

13.2.2 Lattice Models . . . . . . . . . . . . . . . . . . . . . . . 332

13.2.3 Off-lattice Case . . . . . . . . . . . . . . . . . . . . . . . 336

13.3 Generation of Trial Orientations . . . . . . . . . . . . . . . . . . 341

13.3.1 Strong Intramolecular Interactions . . . . . . . . . . . . 342

13.3.2 Generation of Branched Molecules . . . . . . . . . . . . 350

13.4 Fixed Endpoints . . . . . . . . . . . . . . . . . . . . . . . . . . . 353

13.4.1 Lattice Models . . . . . . . . . . . . . . . . . . . . . . . 353

13.4.2 Fully Flexible Chain . . . . . . . . . . . . . . . . . . . . 355

13.4.3 Strong Intramolecular Interactions . . . . . . . . . . . . 357

13.4.4 Rebridging Monte Carlo . . . . . . . . . . . . . . . . . . 357

13.5 Beyond Polymers . . . . . . . . . . . . . . . . . . . . . . . . . . 360

13.6 Other Ensembles . . . . . . . . . . . . . . . . . . . . . . . . . . 365

Contents

ix

13.6.1 Grand-Canonical Ensemble . . . . . . . . . . . . . . . . 365 13.6.2 Gibbs Ensemble Simulations . . . . . . . . . . . . . . . 370 13.7 Recoil Growth . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374 13.7.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 376 13.7.2 Justiﬁcation of the Method . . . . . . . . . . . . . . . . 379 13.8 Questions and Exercises . . . . . . . . . . . . . . . . . . . . . . 383

14 Accelerating Monte Carlo Sampling

389

14.1 Parallel Tempering . . . . . . . . . . . . . . . . . . . . . . . . . 389

14.2 Hybrid Monte Carlo . . . . . . . . . . . . . . . . . . . . . . . . 397

14.3 Cluster Moves . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399

14.3.1 Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . 399

14.3.2 Early Rejection Scheme . . . . . . . . . . . . . . . . . . 405

15 Tackling Time-Scale Problems

409

15.1 Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 410

15.1.1 Constrained and Unconstrained Averages . . . . . . . 415

15.2 On-the-Fly Optimization: Car-Parrinello Approach . . . . . . 421

15.3 Multiple Time Steps . . . . . . . . . . . . . . . . . . . . . . . . . 424

16 Rare Events

431

16.1 Theoretical Background . . . . . . . . . . . . . . . . . . . . . . 432

16.2 Bennett-Chandler Approach . . . . . . . . . . . . . . . . . . . . 436

16.2.1 Computational Aspects . . . . . . . . . . . . . . . . . . 438

16.3 Diffusive Barrier Crossing . . . . . . . . . . . . . . . . . . . . . 443

16.4 Transition Path Ensemble . . . . . . . . . . . . . . . . . . . . . 450

16.4.1 Path Ensemble . . . . . . . . . . . . . . . . . . . . . . . 451

16.4.2 Monte Carlo Simulations . . . . . . . . . . . . . . . . . 454

16.5 Searching for the Saddle Point . . . . . . . . . . . . . . . . . . . 462

17 Dissipative Particle Dynamics

465

17.1 Description of the Technique . . . . . . . . . . . . . . . . . . . 466

17.1.1 Justiﬁcation of the Method . . . . . . . . . . . . . . . . 467

17.1.2 Implementation of the Method . . . . . . . . . . . . . . 469

17.1.3 DPD and Energy Conservation . . . . . . . . . . . . . . 473

17.2 Other Coarse-Grained Techniques . . . . . . . . . . . . . . . . 476

Part V Appendices

479

A Lagrangian and Hamiltonian

481

A.1 Lagrangian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483

A.2 Hamiltonian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486

A.3 Hamilton Dynamics and Statistical Mechanics . . . . . . . . . 488

x

Contents

A.3.1 Canonical Transformation . . . . . . . . . . . . . . . . . 489 A.3.2 Symplectic Condition . . . . . . . . . . . . . . . . . . . 490 A.3.3 Statistical Mechanics . . . . . . . . . . . . . . . . . . . . 492

B Non-Hamiltonian Dynamics

495

Æ Î Ì B.1 Theoretical Background . . . . . . . . . . . . . . . . . . . . . . 495
B.2 Non-Hamiltonian Simulation of the , , Ensemble . . . . . 497

B.2.1 The Nose´-Hoover Algorithm . . . . . . . . . . . . . . . 498

Æ È Ì B.2.2 Nose´-Hoover Chains . . . . . . . . . . . . . . . . . . . . 502
B.3 The , , Ensemble . . . . . . . . . . . . . . . . . . . . . . . . 505

C Linear Response Theory

509

C.1 Static Response . . . . . . . . . . . . . . . . . . . . . . . . . . . 509

C.2 Dynamic Response . . . . . . . . . . . . . . . . . . . . . . . . . 511

C.3 Dissipation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513

C.3.1 Electrical Conductivity . . . . . . . . . . . . . . . . . . . 516

C.3.2 Viscosity . . . . . . . . . . . . . . . . . . . . . . . . . . . 518

C.4 Elastic Constants . . . . . . . . . . . . . . . . . . . . . . . . . . 519

D Statistical Errors

525

D.1 Static Properties: System Size . . . . . . . . . . . . . . . . . . . 525

D.2 Correlation Functions . . . . . . . . . . . . . . . . . . . . . . . . 527

D.3 Block Averages . . . . . . . . . . . . . . . . . . . . . . . . . . . 529

E Integration Schemes

533

E.1 Higher-Order Schemes . . . . . . . . . . . . . . . . . . . . . . . 533

E.2 Nose´-Hoover Algorithms . . . . . . . . . . . . . . . . . . . . . 535

E.2.1 Canonical Ensemble . . . . . . . . . . . . . . . . . . . . 536

E.2.2 The Isothermal-Isobaric Ensemble . . . . . . . . . . . . 540

F Saving CPU Time

545

F.1 Verlet List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545

F.2 Cell Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550

F.3 Combining the Verlet and Cell Lists . . . . . . . . . . . . . . . 550

F.4 Efﬁciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552

G Reference States

559

G.1 Grand-Canonical Ensemble Simulation . . . . . . . . . . . . . 559

H Statistical Mechanics of the Gibbs Ensemble

563

H.1 Free Energy of the Gibbs Ensemble . . . . . . . . . . . . . . . . 563

H.1.1 Basic Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . 563

H.1.2 Free Energy Density . . . . . . . . . . . . . . . . . . . . 565

H.2 Chemical Potential in the Gibbs Ensemble . . . . . . . . . . . . 570

Contents

xi

I Overlapping Distribution for Polymers

573

J Some General Purpose Algorithms

577

K Small Research Projects

581

K.1 Adsorption in Porous Media . . . . . . . . . . . . . . . . . . . . 581

K.2 Transport Properties in Liquids . . . . . . . . . . . . . . . . . . 582

K.3 Diffusion in a Porous Media . . . . . . . . . . . . . . . . . . . . 583

K.4 Multiple-Time-Step Integrators . . . . . . . . . . . . . . . . . . 584

K.5 Thermodynamic Integration . . . . . . . . . . . . . . . . . . . . 585

L Hints for Programming

587

Bibliography

589

Author Index

619

Index

628

This Page Intentionally Left Blank

Preface to the Second Edition
Why did we write a second edition? A minor revision of the ﬁrst edition would have been adequate to correct the (admittedly many) typographical mistakes. However, many of the nice comments that we received from students and colleagues alike, ended with a remark of the type: “unfortunately, you don’t discuss topic x”. And indeed, we feel that, after only ﬁve years, the simulation world has changed so much that the title of the book was no longer covered by the contents.
The ﬁrst edition was written in 1995 and since then several new techniques have appeared or matured. Most (but not all) of the major changes in the second edition deal with these new developments. In particular, we have included a section on:
¯ Transition path sampling and diffusive barrier crossing to simulate rare events
¯ Dissipative particle dynamic as a course-grained simulation technique ¯ Novel schemes to compute the long-ranged forces ¯ Discussion on Hamiltonian and non-Hamiltonian dynamics in the con-
text of constant-temperature and constant-pressure Molecular Dynamics simulations ¯ Multiple-time-step algorithms as an alternative for constraints ¯ Defects in solids ¯ The pruned-enriched Rosenbluth sampling, recoil growth, and concerted rotations for complex molecules ¯ Parallel tempering for glassy Hamiltonians
We have updated some of the examples to include also recent work. Several new Examples have been added to illustrate recent applications.
We have taught several courses on Molecular Simulation, based on the ﬁrst edition of this book. As part of these courses, Dr. Thijs Vlugt prepared many Questions, Exercises, and Case Studies, most of which have been included in the present edition. Some additional exercises can be found on

xiv

Preface to the Second Edition

the Web. We are very grateful to Thijs Vlugt for the permission to reproduce this material.
Many of the advanced Molecular Dynamics techniques described in this book are derived using the Lagrangian or Hamilton formulations of classical mechanics. However, many chemistry and chemical engineering students are not familiar with these formalisms. While a full description of classical mechanics is clearly beyond the scope of the present book, we have added an Appendix that summarizes the necessary essentials of Lagrangian and Hamiltonian mechanics.
Special thanks are due to Giovanni Ciccotti, Rob Groot, Gavin Crooks, Thijs Vlugt, and Peter Bolhuis for their comments on parts of the text. In addition, we thank everyone who pointed out mistakes and typos, in particular Drs. J.B. Freund, R. Akkermans, and D. Moroni.

Preface
This book is not a computer simulation cookbook. Our aim is to explain the physics that is behind the “recipes” of molecular simulation. Of course, we also give the recipes themselves, because otherwise the book would be too abstract to be of much practical use. The scope of this book is necessarily limited: we do not aim to discuss all aspects of computer simulation. Rather, we intend to give a uniﬁed presentation of those computational tools that are currently used to study the equilibrium properties and, in particular, the phase behavior of molecular and supramolecular substances. Moreover, we intentionally restrict the discussion to simulations of classical many-body systems, even though some of the techniques mentioned can be applied to quantum systems as well. And, within the context of classical many-body systems, we restrict our discussion to the modeling of systems at, or near, equilibrium.
The book is aimed at readers who are active in computer simulation or are planning to become so. Computer simulators are continuously confronted with questions concerning the choice of technique, because a bewildering variety of computational tools is available. We believe that, to make a rational choice, a good understanding of the physics behind each technique is essential. Our aim is to provide the reader with this background.
We should state at the outset that we consider some techniques to be more useful than others, and therefore our presentation is biased. In fact, we believe that the reader is well served by the fact that we do not present all techniques as equivalent. However, whenever we express our personal preference, we try to back it up with arguments based in physics, applied mathematics, or simply experience. In fact, we mix our presentation with practical examples that serve a twofold purpose: ﬁrst, to show how a given technique works in practice, and second, to give the reader a ﬂavor of the kind of phenomena that can be studied by numerical simulation.
The reader will also notice that two topics are discussed in great detail, namely simulation techniques to study ﬁrst-order phase transitions, and various aspects of the conﬁgurational-bias Monte Carlo method. The reason why we devote so much space to these topics is not that we consider them

xvi

Preface

to be more important than other subjects that get less coverage, but rather because we feel that, at present, the discussion of both topics in the literature is rather fragmented.
The present introduction is written for the nonexpert. We have done so on purpose. The community of people who perform computer simulations is rapidly expanding as computer experiments become a general research tool. Many of the new simulators will use computer simulation as a tool and will not be primarily interested in techniques. Yet, we hope to convince those readers who consider a computer simulation program a black box, that the inside of the black box is interesting and, more importantly, that a better understanding of the working of a simulation program may greatly improve the efﬁciency with which the black box is used.
In addition to the theoretical framework, we discuss some of the practical tricks and rules of thumb that have become “common” knowledge in the simulation community and are routinely used in a simulation. Often, it is difﬁcult to trace back the original motivation behind these rules. As a result, some “tricks” can be very useful in one case yet result in inefﬁcient programs in others. In this book, we discuss the rationale behind the various tricks, in order to place them in a proper context. In the main text of the book we describe the theoretical framework of the various techniques. To illustrate how these ideas are used in practice we provide Algorithms, Case Studies and Examples.

Algorithms
The description of an algorithm forms an essential part of this book. Such a description, however, does not provide much information on how to implement the algorithm efﬁciently. Of course, details about the implementation of an algorithm can be obtained from a listing of the complete program. However, even in a well-structured program, the code contains many lines that, although necessary to obtain a working program, tend to obscure the essentials of the algorithm that they express. As a compromise solution, we provide a pseudo-code for each algorithm. These pseudo-codes contain only those aspects of the implementation directly related to the particular algorithm under discussion. This implies that some aspects that are essential for using this pseudo-code in an actual program have to be added. For example, the pseudo-codes consider only the Ü directions; similar lines have to be added for the Ý and Þ direction if the code is going to be used in a simulation. Furthermore, we have omitted the initialization of most variables.

Case Studies
In the Case Studies , the algorithms discussed in the main text are combined in a complete program. These programs are used to illustrate some elemen-

Preface

xvii

tary aspects of simulations. Some Case Studies focus on the problems that can occur in a simulation or on the errors that are sometimes made. The complete listing of the FORTRAN codes that we have used for the Case Studies is accessible to the reader through the Internet.1
Examples
In the Examples, we demonstrate how the techniques discussed in the main text are used in an application. We have tried to refer as much as possible to research topics of current interest. In this way, the reader may get some feeling for the type of systems that can be studied with simulations. In addition, we have tried to illustrate in these examples how simulations can contribute to the solution of “real” experimental or theoretical problems.
Many of the topics that we discuss in this book have appeared previously in the open literature. However, the Examples and Case Studies were prepared speciﬁcally for this book. In writing this material, we could not resist including a few computational tricks that, to our knowledge, have not been reported in the literature.
In computer science it is generally assumed that any source code over 200 lines contains at least one error. The source codes of the Case Studies contain over 25,000 lines of code. Assuming we are no worse than the average programmer this implies that we have made at least 125 errors in the source code. If you spot these errors and send them to us, we will try to correct them (we can not promise this!). It also implies that, before you use part of the code yourself, you should convince yourself that the code is doing what you expect it to do.
In the light of the previous paragraph, we must add the following disclaimer:
We make no warranties, express or implied, that the programs contained in this work are free of error, or that they will meet your requirements for any particular application. They should not be relied on for solving problems whose incorrect solution could result in injury, damage, or loss of property. The authors and publishers disclaim all liability for direct or consequential damages resulting from your use of the programs.
Although this book and the included programs are copyrighted, we authorize the readers of this book to use parts of the programs for their own use, provided that proper acknowledgment is made.
Finally, we gratefully acknowledge the help and collaboration of many of our colleagues. In fact, many dozens of our colleagues collaborated with us on topics described in the text. Rather than listing them all here, we mention their names at the appropriate place in the text. Yet, we do wish to
1http://molsim.chem.uva.nl/frenkel smit

xviii

Preface

express our gratitude for their input. Moreover, Daan Frenkel should like to acknowledge numerous stimulating discussions with colleagues at the FOM Institute for Atomic and Molecular Physics in Amsterdam and at the van ’t Hoff Laboratory of Utrecht University, while Berend Smit gratefully acknowledges discussions with colleagues at the University of Amsterdam and Shell. In addition, several colleagues helped us directly with the preparation of the manuscript, by reading the text or part thereof. They are Giovanni Ciccotti, Mike Deem, Simon de Leeuw, Toine Schlijper, Stefano Ruffo, Maria-Jose Ruiz, Guy Verbist and Thijs Vlugt. In addition, we thank Klaas Esselink and Sami Karaborni for the cover ﬁgure. We thank them all for their efforts. In addition we thank the many readers who have drawn our attention to errors and omissions in the ﬁrst print. But we stress that the responsibility for the remainder of errors in the text is ours alone.

List of Symbols

acc´Ó Òµ
b
Î
f
´Öµ
À ¾¯ ´p rµ j k
Ã Ã´Ó Òµ Ä
Ä´q q˙ µ
Ò Ñ Å Æ
Æ ´Óµ ÆÆ Î Ì ÆÆ È Ì ÆÓ Î Ì Ç Ü´ Òµ

dynamical variable
Ó Ò acceptance probability of a move from to

(2.2.6)

trial position or orientation

concentration

concentration of species

speciﬁc heat at constant volume

(4.4.3)

dimensionality

diffusion coefﬁcient

total energy

number of degrees of freedom

fugacity component

(9.1.9)

force on particle

Helmholtz free energy

(2.1.15)

radial distribution function

Gibbs free energy

(5.4.9)

Planck’s constant

Hamiltonian

ﬂux of species

wave vector

Boltzmann’s constant

kinetic energy
Ó Ò ﬂow of conﬁgurations from to

box length

Lagrangian

(A.1.2)

total number of (pseudo-)atoms in a molecule (chain length)

new conﬁguration or conformation

mass

total number of Monte Carlo samples

number of particles
Ó prob. density to ﬁnd a system in conﬁguration

prob. density for canonical ensemble

(5.2.2)

prob. density for isobaric-isothermal ensemble

(5.4.8)

prob. density for grand-canonical ensemble

(5.6.6)

old conﬁguration or conformation
Ü terms of order Ò or smaller

xx
p
È
P
Õ
q
É É´ÆÎ Ìµ É´ÆÈ Ìµ É´ Î Ìµ
r
Ö
Ranf
×
s
Ë Ø Ì Í ´Óµ Ù Ù´Öµ
v
Î
vir
Û Ï´Óµ Ï ´Óµ
«´Ó Òµ ¬ ¡Ø
¯
Ì
£ ´Ó Òµ
«¬ ¾
ª´ µ

List of Symbols

momentum of a particle pressure total linear momentum electric charge on particle generalized coordinates
mass associated with time scaling coordinate ×
canonical partition function isothermal-isobaric partition function grand-canonical partition function Cartesian coordinate of particle cut-off radius of the potential random number uniform in [0, 1] time-scaling coordinate in Nose scheme scaled coordinate of particle entropy time temperature
potential energy of conﬁguration Ó
potential energy per particle pair potential Cartesian velocity of particle volume virial Rosenbluth factor of (pseudo-)atom
total Rosenbluth factor conﬁguration Ó normalized total Rosenbluth factor conﬁguration Ó
conﬁgurational part of the partition function

(6.1.3) (5.2.1) (5.4.7) (5.6.5)
(6.1.3) (5.4.2)
(3.4.2)

probability of generating conf. Ò starting from Ó reciprocal temperature (½ Ì)
Molecular Dynamics time step coordinate in phase space characteristic energy in pair potential shear viscosity chemical potential thermal conductivity thermal de Broglie wavelength
transition probability from Ó to Ò
number density characteristic distance in pair potential electrical conductivity
«¬ component of the stress tensor
variance in dynamical variable thermodynamic friction coefﬁcient fugacity coefﬁcient of component quantum: degeneracy of energy level classical: phase space subvolume with energy

(3.1.14)
(4.4.12) (4.4.14) (5.4.1) (3.1.13)
(4.4.16) (4.4.13) (6.1.25) (9.1.15) (2.1.1) (2.2.9)

List of Symbols

xxi

orientation of a molecule

¡¡¡ ¡ ¡ ¡ sub

ensemble average average under condition indicated by sub

Super- and subscripts

£

reduced units (default, usually omitted)

r«

« component of vector r

r

vector r associated with particle

ex

excess part of quantity

id

ideal gas part of quantity

u

unit vector u

Symbol List: Algorithms

b(j) beta box delx dt ell eni enn eno etot f k kv l n ncycle nhis NINT npart nsamp o p phi pi r2 ranf() rho rc2 switch

trial orientation/position
reciprocal temperature (½ Ì)
simulation box length maximum displacement time step in an MD simulation chain length energy of atom energy of the new conﬁguration energy of the old conﬁguration total energy of the system force total number of trial orientations/position bond vibration energy constant bond length selected trial position total number of MC cycles number of bins in a histogram nearest integer total number of particles number of MC cycles of MD steps between two samples particle number of the old conﬁguration pressure bond-bending angle
¿½ ½
distance squared between two atoms
random number ¾ ¼½
density cutoff radius squared (of the potential) = 0 initialization; = 1 sample; = 2 print result

xxii
t temp tempa theta tmax tors v(i) vmax vol w wn, wo x(i) xm(i) xn xn(i) xo xt(j) ubb utors
¯ ¢ ¡¡¡
le lt ge gt and or
+ * ** / sqrt

List of Symbols
time in a MD simulation temperature instantaneous temperature, from kinetic energy torsional angle maximum simulation time torsion energy velocity of atom maximum displacement volume volume simulation box Rosenbluth factor (new or old) Rosenbluth factor n(ew)/o(ld) conﬁguration position of atom position of atom at previous time step new conﬁguration of a particle positions of atoms that have been grown old conﬁguration of a particle th trial position for a given atom bond-bending energy torsion energy
vector dot product vector cross product length of the vector
test: true if is less than or equal to test: true if is less than test: true if is greater than or equal to test: true if is greater than test: true if both and are true test: true if or are true
continuation symbol multiplication to the power division square root

Chapter 1
Introduction
(Pre)history of Computer Simulation
It usually takes decades rather than years before a fundamentally new invention ﬁnds widespread application. For computer simulation, the story is rather different. Computer simulation started as a tool to exploit the electronic computing machines that had been developed during and after the Second World War. These machines had been built to perform the very heavy computation involved in the development of nuclear weapons and code breaking. In the early 1950s, electronic computers became partly available for nonmilitary use and this was the beginning of the discipline of computer simulation. W. W. Wood [1] recalls: “When the Los Alamos MANIAC became operational in March 1952, Metropolis was interested in having as broad a spectrum of problems as possible tried on the machine, in order to evaluate its logical structure and demonstrate the capabilities of the machine.”
The strange thing about computer simulation is that it is also a discovery, albeit a delayed discovery that grew slowly after the introduction of the technique. In fact, discovery is probably not the right word, because it does not refer to a new insight into the working of the natural world but into our description of nature. Working with computers has provided us with a new metaphor for the laws of nature: they carry as much (and as little) information as algorithms. For any nontrivial algorithm (i.e., loosely speaking, one that cannot be solved analytically), you cannot predict the outcome of a computation simply by looking at the program, although it often is possible to make precise statements about the general nature (e.g., the symmetry) of the result of the computation. Similarly, the basic laws of nature as we know them have the unpleasant feature that they are expressed in terms of equations we cannot solve exactly, except in a few very special cases. If we wish to study the motion of more than two interacting bodies, even the relatively

2

Chapter 1. Introduction

simple laws of Newtonian mechanics become essentially unsolvable. That is to say, they cannot be solved analytically, using only pencil and the back of the proverbial envelope. However, using a computer, we can get the answer to any desired accuracy. Most of materials science deals with the properties of systems of many atoms or molecules. Many almost always means more than two; usually, very much more. So if we wish to compute the properties of a liquid (to take a particularly nasty example), there is no hope of ﬁnding the answer exactly using only pencil and paper.
Before computer simulation appeared on the scene, there was only one way to predict the properties of a molecular substance, namely by making use of a theory that provided an approximate description of that material. Such approximations are inevitable precisely because there are very few systems for which the equilibrium properties can be computed exactly (examples are the ideal gas, the harmonic crystal, and a number of lattice models, such as the two-dimensional Ising model for ferromagnets). As a result, most properties of real materials were predicted on the basis of approximate theories (examples are the van der Waals equation for dense gases, the Debye-Hu¨ ckel theory for electrolytes, and the Boltzmann equation to describe the transport properties of dilute gases). Given sufﬁcient information about the intermolecular interactions, these theories will provide us with an estimate of the properties of interest. Unfortunately, our knowledge of the intermolecular interactions of all but the simplest molecules is also quite limited. This leads to a problem if we wish to test the validity of a particular theory by comparing directly to experiment. If we ﬁnd that theory and experiment disagree, it may mean that our theory is wrong, or that we have an incorrect estimate of the intermolecular interactions, or both.
Clearly, it would be very nice if we could obtain essentially exact results for a given model system without having to rely on approximate theories. Computer simulations allow us to do precisely that. On the one hand, we can now compare the calculated properties of a model system with those of an experimental system: if the two disagree, our model is inadequate; that is, we have to improve on our estimate of the intermolecular interactions. On the other hand, we can compare the result of a simulation of a given model system with the predictions of an approximate analytical theory applied to the same model. If we now ﬁnd that theory and simulation disagree, we know that the theory is ﬂawed. So, in this case, the computer simulation plays the role of the experiment designed to test the theory. This method of screening theories before we apply them to the real world is called a computer experiment. This application of computer simulation is of tremendous importance. It has led to the revision of some very respectable theories, some of them dating back to Boltzmann. And it has changed the way in which we construct new theories. Nowadays it is becoming increasingly rare that a theory is applied to the real world before being tested by computer simula-

Chapter 1. Introduction

3

Ì

P

1 1.03 ¦ 0.04

2 1.99 ¦ 0.03

3 2.98 ¦ 0.05

4 4.04 ¦ 0.03

5 5.01 ¦ 0.04

Table 1.1: Simulated equation of state of an ideal gas

tion. The simulation then serves a twofold purpose: it gives the theoretician a feeling for the physics of the problem, and it generates some “exact” results that can be used to test the quality of the theory to be constructed. Computer experiments have become standard practice, to the extent that they now provide the ﬁrst (and often the last) test of a new theoretical result.
But note that the computer as such offers us no understanding, only numbers. And, as in a real experiment, these numbers have statistical errors. So what we get out of a simulation is never directly a theoretical relation. As in a real experiment, we still have to extract the useful information. To take a not very realistic example, suppose we were to use the computer to measure the pressure of an ideal gas as a function of density. This example is unrealistic because the volume dependence of the ideal-gas pressure has, in fact, been well known since the work of Boyle and Gay-Lussac. The BoyleGay-Lussac law states that the product of volume and pressure of an ideal gas is constant. Now suppose we were to measure this product by computer simulation. We might, for instance, ﬁnd the set of experimental results in Table 1.1. The data suggest that È equals Ì, but no more than that. It is left to us to infer the conclusions.
The early history of computer simulation (see, e.g., ref. [2]) illustrates this role of computer simulation. Some areas of physics appeared to have little need for simulation because very good analytical theories were available (e.g., to predict the properties of dilute gases or of nearly harmonic crystalline solids). However, in other areas, few if any exact theoretical results were known, and progress was much hindered by the lack of unambiguous tests to assess the quality of approximate theories. A case in point was the theory of dense liquids. Before the advent of computer simulations, the only way to model liquids was by mechanical simulation [3–5] of large assemblies of macroscopic spheres (e.g., ball bearings). Then the main problem becomes how to arrange these balls in the same way as atoms in a liquid. Much work on this topic was done by the famous British scientist J. D. Bernal, who built and analyzed such mechanical models for liquids. Actually, it would be fair to say that the really tedious work of analyzing the resulting threedimensional structures was done by his research students, such as the unfor-

4

Chapter 1. Introduction

tunate Miss Wilkinson whose research assignment was to identify all distinct local packing geometries of plastic foam spheres: she found that there were at least 197. It is instructive to see how Bernal built some of his models. The following quote from the 1962 Bakerian lecture describes Bernal’s attempt to build a ball-and-spoke model of a liquid [5]:
. . . I took a number of rubber balls and stuck them together with rods of a selection of different lengths ranging from 2.75 to 4 inch. I tried to do this in the ﬁrst place as casually as possible, working in my own ofﬁce, being interrupted every ﬁve minutes or so and not remembering what I had done before the interruption. However,. . . .
Subsequent models were made, for instance, by pouring thousands of steel balls from ball bearings into a balloon. It should be stressed that these mechanical models for liquids were in some respects quite realistic. However, the analysis of the structures generated by mechanical simulation was very laborious and, in the end, had to be performed by computer anyway.
In view of the preceding, it is hardly surprising that, when electronic computers were, for the ﬁrst time, made available for unclassiﬁed research, numerical simulation of dense liquids was one of the ﬁrst problems to be tackled. In fact, the ﬁrst simulation of a liquid was carried out by Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller on the MANIAC computer at Los Alamos [6], using (or, more properly, introducing) the Metropolis Monte Carlo (MC) method. The name Monte Carlo simulation had been coined earlier by Metropolis and Ulam (see Ref. [7]), because the method makes heavy use of computer-generated random numbers. Almost at the same time, Fermi, Pasta, and Ulam [8] performed their famous numerical study of the dynamics of an anharmonic, one-dimensional crystal. The ﬁrst proper Molecular Dynamics (MD) simulations were reported in 1956 by Alder and Wainwright [9] at Livermore, who studied the dynamics of an assembly of hard spheres. The ﬁrst MD simulation of a model for a “real” material was reported in 1959 (and published in 1960) by the group led by Vineyard at Brookhaven [10], who simulated radiation damage in crystalline Cu (for a historical account, see [11]). The ﬁrst MD simulation of a real liquid (argon) was reported in 1964 by Rahman at Argonne [12]. After that, computers were increasingly becoming available to scientists outside the US government labs, and the practice of simulation started spreading to other continents [13–16]. Much of the methodology of computer simulations has been developed since then, although it is fair to say that the basic algorithms for MC and MD have hardly changed since the 1950s.
The most common application of computer simulations is to predict the properties of materials. The need for such simulations may not be immediately obvious. After all it is much easier to measure the freezing point of water than to extract it from a computer simulation. The point is, of course, that

Chapter 1. Introduction

5

it is easy to measure the freezing point of water at 1 atmosphere but often very difﬁcult and therefore expensive to measure the properties of real materials at very high pressures or temperatures. The computer does not care: it does not go up in smoke when you ask it to simulate a system at 10,000 K. In addition, we can use computer simulation to predict the properties of materials that have not yet been made. And ﬁnally, computer simulations are increasingly used in data analysis. For instance, a very efﬁcient technique for obtaining structural information about macromolecules from 2D-NMR is to feed the experimental data into a Molecular Dynamics simulation and let the computer ﬁnd the structure that is both energetically favorable and compatible with the available NMR data.
Initially, such simulations were received with a certain amount of skepticism, and understandably so. Simulation did not ﬁt into the existing idea that whatever was not experiment had to be theory. In fact, many scientists much preferred to keep things the way they were: theory for the theoreticians and experiments for the experimentalists and no computers to confuse the issue. However, this position became untenable, as is demonstrated by the following autobiographical quote of George Vineyard [11], who was the ﬁrst to study the dynamics of radiation damage by numerical simulation:
. . . In the summer of 1957 at the Gordon Conference on Chemistry and Physics of Metals, I gave a talk on radiation damage in metals . . . . After the talk there was a lively discussion . . . . Somewhere the idea came up that a computer might be used to follow in more detail what actually goes on in radiation damage cascades. We got into quite an argument, some maintaining that it wasn’t possible to do this on a computer, others that it wasn’t necessary. John Fisher insisted that the job could be done well enough by hand, and was then goaded into promising to demonstrate. He went off to his room to work. Next morning he asked for a little more time, promising to send me the results soon after he got home. After about two weeks, not having heard from him, I called and he admitted that he had given up. This stimulated me to think further about how to get a high-speed computer into the game in place of John Fisher. . . .
Finally, computer simulation can be used as a purely exploratory tool. This sounds strange. One would be inclined to say that one cannot “discover” anything by simulation because you can never get out what you have not put in. Computer discoveries, in this respect, are not unlike mathematical discoveries. In fact, before computers were actually available this kind of numerical charting of unknown territory was never considered.
The best way to explain it is to give an explicit example. In the mid1950s, one of the burning questions in statistical mechanics was this: can crystals form in a system of spherical particles that have a harsh short-range

6

Chapter 1. Introduction

repulsion, but no mutual attraction whatsoever? In a very famous computer simulation, Alder and Wainwright [17] and Wood and Jacobson [18] showed that such a system does indeed have a ﬁrst-order freezing transition. This is now accepted wisdom, but at the time it was greeted with skepticism. For instance, at a meeting in New Jersey in 1957, a group of some 15 very distinguished scientists (among whom were 2 Nobel laureates) discussed the issue. When a vote was taken as to whether hard spheres can form a stable crystal, it appeared that half the audience simply could not believe this result. However, the work of the past 30 years has shown that harsh repulsive forces really determine the structural properties of a simple liquid and that attractive forces are in a sense of secondary importance.
Suggested Reading
As stated at the outset, the present book does not cover all aspects of computer simulation. Readers who are interested in aspects of computer simulation not covered in this book are referred to one of the folowing books
¯ Allen and Tildesley, Computer Simulation of Liquids [19]
¯ Haile, Molecular Dynamics Simulations: Elementary Methods [20]
¯ Landau and Binder, A Guide to Monte Carlo Simulations in Statistical Physics [21]
¯ Rapaport, The Art of Molecular Dynamics Simulation [22]
¯ Newman and Barkema, Monte Carlo Methods in Statistical Physics [23]
Also of interest in this context are the books by Hockney and Eastwood [24], Hoover [25, 26], Vesely [27], and Heermann [28] and the book by Evans and Morriss [29] for the theory and simulation of transport phenomena. The latter book is out of print and has been made available in electronic form.1
A general discussion of Monte Carlo sampling (with examples) can be found in Koonin’s Computational Physics [30]. As the title indicates, this is a textbook on computational physics in general, as is the book by Gould and Tobochnik [31]. In contrast, the book by Kalos and Whitlock [32] focuses speciﬁcally on the Monte Carlo method. A good discussion of (quasi) random-number generators can be found in Numerical Recipes [33], while Ref. [32] gives a detailed discussion of tests for random-number generators. A discussion of Monte Carlo simulations with emphasis on techniques relevant for atomic and molecular systems may be found in two articles by Valleau and Whittington in Modern Theoretical Chemistry [34, 35]. The books by Binder [36,37] and Mouritsen [38] emphasize the application of MC simulations to discrete systems, phase transitions and critical phenomena. In addition, there exist several very useful proceedings of summer schools [39–42] on computer simulation.
1See http://rsc.anu.edu.au/˜evans/evansmorrissbook.htm

Part I
Basics

This Page Intentionally Left Blank

Chapter 2
Statistical Mechanics
The topic of this book is computer simulation. Computer simulation allows us to study properties of many-particle systems. However, not all properties can be directly measured in a simulation. Conversely, most of the quantities that can be measured in a simulation do not correspond to properties that are measured in real experiments. To give a speciﬁc example: in a Molecular Dynamics simulation of liquid water, we could measure the instantaneous positions and velocities of all molecules in the liquid. However, this kind of information cannot be compared to experimental data, because no real experiment provides us with such detailed information. Rather, a typical experiment measures an average property, averaged over a large number of particles and, usually, also averaged over the time of the measurement. If we wish to use computer simulation as the numerical counterpart of experiments, we must know what kind of averages we should aim to compute. In order to explain this, we need to introduce the language of statistical mechanics. This we shall do here. We provide the reader with a quick (and slightly dirty) derivation of the basic expressions of statistical mechanics. The aim of these derivations is only to show that there is nothing mysterious about concepts such as phase space, temperature and entropy and many of the other statistical mechanical objects that will appear time and again in the remainder of this book.
2.1 Entropy and Temperature
Most of the computer simulations that we discuss are based on the assumption that classical mechanics can be used to describe the motions of atoms and molecules. This assumption leads to a great simpliﬁcation in almost all calculations, and it is therefore most fortunate that it is justiﬁed in many cases of practical interest. Surprisingly, it turns out to be easier to derive the

10

Chapter 2. Statistical Mechanics

basic laws of statistical mechanics using the language of quantum mechan-

ics. We will follow this route of least resistance. In fact, for our derivation,

we need only little quantum mechanics. Speciﬁcally, we need the fact that a

quantum mechanical system can be found in different states. For the time be-

ing, we limit ourselves to quantum states that are eigenvectors of the Hamil-

À tonian of the system (i.e., energy eigenstates). For any such state , we

À have that

=

, where is the energy of state . Most exam-

ples discussed in quantum mechanics textbooks concern systems with only

a few degrees of freedom (e.g., the one-dimensional harmonic oscillator or a

particle in a box). For such systems, the degeneracy of energy levels will be

Ç small. However, for the systems that are of interest to statistical mechanics
(i.e., systems with ´½¼¾¿µ particles), the degeneracy of energy levels is as-

tronomically large. In what follows, we denote by ª´ Î Æµ the number of

eigenstates with energy of a system of Æ particles in a volume Î. We now

express the basic assumption of statistical mechanics as follows: a system

with ﬁxed Æ, Î, and is equally likely to be found in any of its ª´ µ eigen-

states. Much of statistical mechanics follows from this simple (but highly

nontrivial) assumption.

To see this, let us ﬁrst consider a system with total energy that con-

sists of two weakly interacting subsystems. In this context, weakly interacting

means that the subsystems can exchange energy but that we can write the

total energy of the system as the sum of the energies ½ and ¾ of the sub-

systems. There are many ways in which we can distribute the total energy

¢ over the two subsystems such that ½ · ¾ . For a given choice of ½, the
total number of degenerate states of the system is ª½´ ½µ ª¾´ ¾µ. Note

that the total number of states is not the sum but the product of the number

of states in the individual systems. In what follows, it is convenient to have

a measure of the degeneracy of the subsystems that is additive. A logical

choice is to take the (natural) logarithm of the degeneracy. Hence:

ln ª´ ½ ¹ ½µ ln ª½´ ½µ · ln ª¾´ ¹ ½µ

(2.1.1)

We assume that subsystems 1 and 2 can exchange energy. What is the most likely distribution of the energy? We know that every energy state of the total system is equally likely. But the number of eigenstates that correspond to a given distribution of the energy over the subsystems depends very strongly on the value of ½. We wish to know the most likely value of ½, that is, the one that maximizes ln ª´ ½ ¹ ½µ. The condition for this maximum is that

ln ª´ ½ ¹ ½µ
¼

½

ÆÎ

(2.1.2)

or, in other words,

ln ª½´ ½µ

½

Æ½ Î½

ln ª¾´ ¾µ

¾

Æ¾ Î¾

(2.1.3)

2.1 Entropy and Temperature

11

We introduce the shorthand notation

¬´ Î Æµ

ln ª´ Î

Æµ ÆÎ

With this deﬁnition, we can write equation (2.1.3) as

¬´ ½ Î½ Æ½µ ¬´ ¾ Î¾ Æ¾µ

(2.1.4) (2.1.5)

Clearly, if initially we put all energy in system 1 (say), there will be energy transfer from system 1 to system 2 until equation (2.1.3) is satisﬁed. From that moment on, no net energy ﬂows from one subsystem to the other, and we say that the two subsystems are in (thermal) equilibrium. When this
equilibrium is reached, ln ª of the total system is at a maximum. This suggests that ln ª is somehow related to the thermodynamic entropy Ë of the
system. After all, the second law of thermodynamics states that the entropy
of a system Æ, Î, and is at its maximum when the system is in thermal equilibrium. There are many ways in which the relation between ln ª and
entropy can be established. Here we take the simplest route; we simply
deﬁne the entropy to be equal to ln ª. In fact, for (unfortunate) historical reasons, entropy is not simply equal to ln ª; rather we have

Ë´ÆÎ µ ln ª´ÆÎ µ

(2.1.6)

1w0h¹e¾r¿e

is Boltzmann’s constant, which in S.I. J/K. With this identiﬁcation, we see that

units has the value 1.38066 our assumption that all de-

generate eigenstates of a quantum system are equally likely immediately

implies that, in thermal equilibrium, the entropy of a composite system is

at a maximum. It would be a bit premature to refer to this statement as the

second law of thermodynamics, as we have not yet demonstrated that the

present deﬁnition of entropy is, indeed, equivalent to the thermodynamic

deﬁnition. We simply take an advance on this result.

The next thing to note is that thermal equilibrium between subsystems 1
and 2 implies that ¬½ = ¬¾. In everyday life, we have another way to express
the same thing: we say that two bodies brought into thermal contact are in
equilibrium if their temperatures are the same. This suggests that ¬ must

be related to the absolute temperature. The thermodynamic deﬁnition of

temperature is

½Ì Ë ÎÆ

(2.1.7)

If we use the same deﬁnition here, we ﬁnd that

¬ ½ ´ Ìµ

(2.1.8)

Now that we have deﬁned temperature, we can consider what happens if we have a system (denoted by ) that is in thermal equilibrium with a large heat

12

Chapter 2. Statistical Mechanics

bath ( ). The total system is closed; that is, the total energy

· is

ﬁxed (we assume that the system and the bath are weakly coupled, so that

we may ignore their interaction energy). Now suppose that the system is

prepared in one speciﬁc quantum state with energy . The bath then has

an energy

¹ and the degeneracy of the bath is given by ª ´ ¹

µ. Clearly, the degeneracy of the bath determines the probability È to ﬁnd

system in state :

È

Èªª´

¹ ´¹

µ

µ

(2.1.9)

To compute ª ´ ¹ µ, we expand ln ª ´ ¹ µ around ¼:

ln ª ´ ¹ µ ln ª ´ µ ¹ ln ª ´ µ · Ç´½ µ

(2.1.10)

or, using equations (2.1.6) and (2.1.7),
ln ª ´ ¹ µ ln ª ´ µ ¹

Ì · Ç´½ µ

(2.1.11)

If we insert this result in equation (2.1.9), we get

È È

exp´¹ Ìµ exp´¹ Ìµ

(2.1.12)

This is the well-known Boltzmann distribution for a system at temperature
Ì. Knowledge of the energy distribution allows us to compute the average energy of the system at the given temperature Ì:

È

È exp´¹ Ìµ

È exp´¹ Ìµ

È ¹ ln

exp´¹
½Ì

Ìµ

¹

ln É ½Ì

(2.1.13) (2.1.14)

where, in the last line, we have deﬁned the partition function É. If we com-
pare equation (2.1.13) with the thermodynamic relation

Ì ½Ì

where is the Helmholtz free energy, we see that is related to the partition
function É:

¹ Ì ln É ¹ Ì ln exp´¹ Ìµ

(2.1.15)

2.2 Classical Statistical Mechanics

13

Strictly speaking, is ﬁxed only up to a constant. Or, what amounts to the same thing, the reference point of the energy can be chosen arbitrarily. In what follows, we can use equation (2.1.15) without loss of generality. The relation between the Helmholtz free energy and the partition function is often more convenient to use than the relation between ln ª and the entropy. As a consequence, equation (2.1.15) is the workhorse of equilibrium statistical mechanics.

2.2 Classical Statistical Mechanics

Thus far, we have formulated statistical mechanics in purely quantum mech-

anical terms. The entropy is related to the density of states of a system with

energy , volume Î, and number of particles Æ. Similarly, the Helmholtz

free energy is related to the partition function É, a sum over all quantum

states of the Boltzmann factor exp´¹

Ìµ. To be speciﬁc, let us consider

the average value of some observable . We know the probability that a

system at temperature Ì will be found in an energy eigenstate with energy

and we can therefore compute the thermal average of

È Èexp´¹

Ìµ

as

exp´¹

Ìµ

(2.2.1)

where

denotes the expectation value of the operator in quan-

tum state . This equation suggests how we should go about computing

thermal averages: ﬁrst we solve the Schro¨ dinger equation for the (many-

body) system of interest, and next we compute the expectation value of the

operator for all those quantum states that have a nonnegligible statisti-

cal weight. Unfortunately, this approach is doomed for all but the simplest

systems. First of all, we cannot hope to solve the Schro¨dinger equation for

an arbitrary many-body system. And second, even if we could, the number

of quantum states that contribute to the average in equation (2.2.1) would be so astronomically large (Ç´½¼½¼¾ µ) that a numerical evaluation of all ex-

pectation values would be unfeasible. Fortunately, equation (2.2.1) can be

simpliﬁed to a more workable expression in the classical limit. To this end,

we ﬁrst rewrite equation (2.2.1) in a form that is independent of the speciﬁc

basis set. We note that exp´¹

Ìµ = exp´¹À Ìµ , where À is

the Hamiltonian of the system. Using this relation, we can write
ÈÈ exp´¹À Ìµ exp´¹À Ìµ

Tr exp´¹À Ìµ Tr exp´¹À Ìµ

(2.2.2)

14

Chapter 2. Statistical Mechanics

where Tr denotes the trace of the operator. As the value of the trace of an

operator does not depend on the choice of the basis set, we can compute

thermal averages using any basis set we like. Preferably, we use simple

basis sets, such as the set of eigenfunctions of the position or the momen-
À tum operator. Next, we use the fact that the Hamiltonian is the sum of Ã Í a kinetic part and a potential part . The kinetic energy operator is a

quadratic function of the momenta of all particles. As a consequence, mo-

mentum eigenstates are also eigenfunctions of the kinetic energy operator.

Similarly, the potential energy operator is a function of the particle coordi-

Í nates. Matrix elements of therefore are most conveniently computed in a

À Ã Í basis set of position eigenfunctions. However,

· itself is not diag-

Ã Í onal in either basis set nor is exp ¹¬´ · µ . However, if we could replace

À Ã Í exp´¹¬ µ by exp´¹¬ µ exp´¹¬ µ, then we could simplify equation (2.2.2)

considerably. In general, we cannot make this replacement because

exp´¹¬Ãµ exp´¹¬Íµ exp ¹¬ Ã · Í · Ç´ Ã Í µ

Ã Í where

is the commutator of the kinetic and potential energy opera-

Ç Ã Í tors while ´

µ is meant to note all terms containing commutators and

Ã Í higher-order commutators of and . It is easy to verify that the commuta-

Ã Í tor

is of order ¯ (¯

´¾ µ, where is Planck’s constant). Hence, in

Ç Ã Í the limit ¯ ¼, we may ignore the terms of order ´

µ. In that case, we

can write

Tr exp´¹¬Àµ Tr exp´¹¬Íµ exp´¹¬Ãµ

(2.2.3)

If we use the notation Ö for eigenvectors of the position operator and
for eigenvectors of the momentum operator, we can express equation (2.2.3) as

Tr exp´¹¬Àµ

Ö ¹¬Í Ö Ö

Ö

¹¬Ã

Ö

(2.2.4)

All matrix elements can be evaluated directly:
Ö exp´¹¬Íµ Ö exp ¢¹¬Í´rÆµ£ Í where ´rÆµ on the right-hand side is no longer an operator but a function
of the coordinates of all Æ particles. Similarly,

exp´¹¬Ãµ
where Ô ¯ , and Ö

Æ
exp ¹¬ Ô¾ ´¾Ñ µ
½
Ö ½ ÎÆ

2.2 Classical Statistical Mechanics

15

where Î is the volume of the system and Æ the number of particles. Finally,

we can replace the sum over states by an integration over all coordinates and

momenta. The ﬁnal result is

À¬ Tr exp´¹ µ

½ ÆÆ

Æ
dpÆdrÆ exp ¹¬

¯
Í Ô¾ ´¾Ñ µ · ´rÆµ

Éclassical

(2.2.5)

where is the dimensionality of the system and the last line deﬁnes the clas-
sical partition function. The factor ½ Æ has been inserted afterward to take the indistinguishability of identical particles into account. Every Æ-particle
quantum state corresponds to a volume Æ in classical phase space, but

not all such volumes correspond to distinct quantum states. In particular, all

points in phase space that only differ in the labeling of the particles corre-

À spond to the same quantum state (for more details, see, e.g., [43]).
Similarly, we can derive the classical limit for Tr exp´¹¬ µ , and ﬁnally,

we can write the classical expression for the thermal average of the observ-

able as

Í Í Ê dpÆÊddrpÆÆedxrpÆ¨e¹x¬p¢ªÈ¹¬Ô¾È´¾ÔÑ¾

µ · ´rÆµ£© ´p«Æ
´¾Ñ µ · ´rÆµ

qÆµ

(2.2.6)

Equations (2.2.5) and (2.2.6) are the starting point for virtually all classical simulations of many-body systems.

2.2.1 Ergodicity
Thus far, we have discussed the average behavior of many-body systems in a purely static sense: we introduced only the assumption that every quantum state of a many-body system with energy is equally likely to be occupied. Such an average over all possible quantum states of a system is called an ensemble average. However, this is not the way we usually think about the average behavior of a system. In most experiments we perform a series of measurements during a certain time interval and then determine the average of these measurements. In fact, the idea behind Molecular Dynamics simulations is precisely that we can study the average behavior of a many-particle system simply by computing the natural time evolution of that system numerically and averaging the quantity of interest over a sufﬁciently long time. To take a speciﬁc example, let us consider a ﬂuid consisting of atoms. Suppose that we wish to compute the average density of
the ﬂuid at a distance Ö from a given atom , ´Öµ. Clearly, the instanta-
neous density depends on the coordinates r of all particles in the system. As time progresses, the atomic coordinates will change (according to Newton’s equations of motion), and hence the density around atom will change.

16

Chapter 2. Statistical Mechanics

Patro´oÖmvirsdÆe(´rd¼Æµt´h¼paµÆt p´w¼Æµe´¼hØµµa).vIwenesapkeMnciooﬁwlee,dcuattlhalereaDinsyittniianalmpciorciosnrcsdiiipmnlaeut,leatsthiaoenntd,imwmeeoemmveeoanlustuatirooenftaholefl

time-averaged density ´Öµ of a system of Æ atoms, in a volume Î, at a

constant total energy :

´Öµ

½
Ølim½ Ø

Ø ¼ dØ¼

´Ö Ø ¼µ

(2.2.7)

Note that, in writing down this equation, we have implicitly assumed that,

for Ø sufﬁciently long, the time average does not depend on the initial con-

ditions. This is, in fact, a subtle assumption that is not true in general (see,

e.g., [44]). However, we shall disregard subtleties and simply assume that,

once we have speciﬁed Æ, Î, and , time averages do not depend on the

initial coordinates and momenta. If that is so, then we would not change

our result for ´Öµ if we average over many different initial conditions; that

is, we consider the hypothetical situation where we run a large number of

Molecular Dynamics simulations at the same values for Æ, Î, and , but

with different initial coordinates and momenta,

´Öµ

initial conditions

Ølim½

½ Ø

Ø ¼ dØ¼

Æ Æ ´Ö r ´¼µ p ´¼µ Ø ¼µ

number of initial conditions

(2.2.8)

We now consider the limiting case where we average over all initial conditions compatible with the imposed values of Æ, Î, and . In that case, we can replace the sum over initial conditions by an integral:

´rÆ´¼µ pÆ´¼µµ
initial conditions
number of initial conditions

Ê drÆdpÆ ´rÆ´¼µ pÆ´¼µµ
ª´ÆÎ µ

(2.2.9)

wwhheilreeª´dÆeÎnotesµa=nÊardbritÆradrpyÆfu(nwcetihoanvoefigthneoirneidtiaalccoonosrtadnintafatecstorrÆ1)´.¼Tµ hpeÆs´u¼bµ-,

script on the integral indicates that the integration is restricted to a shell of

constant energy . Such a “phase space” average, corresponds to the clas-

sical limit of the ensemble average discussed in the previous sections.2 We

ª ÆÎ 1If we consider a quantum mechanical system, then ´

µ is simply the number of

Æ Î quantum states of that system, for given , , and . In the classical limit, the number of

Æ quantum states ofÊa d-dimensional system of distinguishable, structureless particles is given

ª ÆÎ Æ Æ Æ Æ by ´

µ = ´ dp dr µ

. For indistinguishable particles, we should divide the latter

Æ expression by a factor .

Æ Î 2Here we consider the classical equivalent of the so-called microcanonical ensemble, i.e., the
ensemble of systems with ﬁxed , , and . The classical expression for phase space integrals

in the microcanonical ensemble can be derived from the quantum mechanical expression in-

ÆÎ Ì volving a sum over quantum states in much the same way that we used to derive the classical

constant

(“canonical”) ensemble from the corresponding quantum mechanical expres-

sion.

2.3 Questions and Exercises

17

denote an ensemble average by ¡ ¡ ¡ to distinguish it from a time average, denoted by a bar. If we switch the order of the time averaging and the averaging over initial conditions, we ﬁnd

½ ´Öµ

½

lim
Ø

Ø

dØ ¼

´Ö rÆ´¼µ pÆ´¼µ Ø ¼µ ÆÎ

(2.2.10)

However, the ensemble average in this equation does not depend on the time Ø¼. This is so, because there is a one-to-one correspondence between the initial phase space coordinates of a system and those that specify the state of the system at a later time Ø¼ (see e.g., [44, 45]). Hence, averaging over all initial phase space coordinates is equivalent to averaging over the timeevolved phase space coordinates. For this reason, we can leave out the time averaging in equation (2.2.10), and we ﬁnd

´Öµ

´Öµ ÆÎ

(2.2.11)

This equation states that, if we wish to compute the average of a function of the coordinates and momenta of a many-particle system, we can either compute that quantity by time averaging (the “MD” approach) or by ensemble averaging (the “MC” approach). It should be stressed that the preceding paragraphs are meant only to make equation (2.2.11) plausible, not as a proof. In fact, that would have been quite impossible because equation (2.2.11) is not true in general. However, in what follows, we shall simply assume that the “ergodic hypothesis”, as equation (2.2.11) is usually referred to, applies to the systems that we study in computer simulations. The reader, however, should be aware that many examples of systems are not ergodic in practice, such as glasses and metastable phases, or even in principle, such as nearly harmonic solids.

2.3 Questions and Exercises
Question 1 (Number of Con gurations)
1. Consider a system consisting of subsystems ½ and ¾, for which ª½ ½¼¾¼ and ª¾ ½¼¾¾. What is the number of con gurations available to the combined system? Also, compute the entropies Ë, Ë½, and Ë¾.
2. By what factor does the number of available con gurations increase when ½¼ m¿ of air at ½ ¼ atm and ¿¼¼ K is allowed to expand by ¼ ¼¼½% at constant temperature?
3. By what factor does the number of available con gurations increase when ½ ¼ kJ is added to a system containing ¾ ¼ mol of particles at constant volume and Ì ¿¼¼ K?

18

Chapter 2. Statistical Mechanics

½ 4. A sample consisting of ve molecules has a total energy ¯. Each molecule is able to occupy states of energy ¯ , with ¼½¾ ¡ ¡ ¡ . Draw up a table with columns by the energy of the states and write beneath them all con gurations that are consistent with the total energy. Identify the type of con guration that is most probable.

Question 2 (Thermodynamic Variables in the Canonical Ensemble) Starting with an expression for the Helmholtz free energy ( ) as a function of ÆÎ Ì
¹ ln É ´ÆÎ Ìµ
¬

one can derive all thermodynamic properties. Show this by deriving equations for Í, Ô, and Ë.

Question 3 (Ideal Gas (Part 1)) The canonical partition function of an ideal gas consisting of monoatomic particles is equal to

É ´ÆÎ Ìµ
Ô

½ ¿ÆÆ

exp ¹¬À

ÎÆ ¿ÆÆ

in which

¾ Ñ ¬ and

Õ½ ¡ ¡ ¡ ÕÆ Ô½ ¡ ¡ ¡ ÔÆ.

Derive expressions for the following thermodynamic properties:

¯ ´ÆÎ Ìµ (hint: ln ´Æ µ Æ ln ´Æµ ¹ Æ)

´ ¯ Ô ÆÎ Ìµ (which leads to the ideal gas law !!!)

¯ ´ÆÎ Ìµ (which leads to

¼ · ÊÌ ln )

´ ¯ Í ÆÎ Ìµ and Ë ´ÆÎ Ìµ

¯ Ú (heat capacity at constant volume) ¯ Ô (heat capacity at constant pressure)

uestion 4 (Ising Model) Consider a system of Æ spins arranged on a lattice. In the presence of a magnetic eld, À, the energy of the system is

Æ
Í ¹ À × ¹Â × ×
½

in which Â is called the coupling constant (Â ¼) and × ¦½. The second summation is a summation over all pairs ( ¢ Æ for a periodic system, is the dimensionality of the system). This system is called the Ising model.

1. Show that for positive Â, and À ¼, the lowest energy of the Ising model is

equal to

Í¼ ¹ ÆÂ

in which is the dimensionality of the system.

2.3 Questions and Exercises

19

2. Show that the free energy per spin of a 1D Ising model with zero eld is equal

to when Æ

´¬Æ µ

ln ´¾ cosh ´¬Âµµ

Æ

¹

¬

½. The function cosh ´Üµ is de ned as

cosh ´Üµ

exp ¹Ü · exp Ü ¾

(2.3.1)

3. Derive equations for the energy and heat capacity of this system.

Question 5 (The Photon Gas) An electromagnetic eld in thermal equilibrium

can be described as a phonon gas. From the quantum theory of the electromagnetic

eld, it is found that the total energy of the system (Í) can be written as the sum of

photon energies:

Æ

Æ

Í

Ò¯

Ò¯

½

½

½ in which ¯ is the characteristic energy of a photon with frequency , , Ò
¼½¾ ¡ ¡ ¡ is the so-called occupancy number of mode , and Æ is the number of eld modes (here we take Æ to be nite).

1. Show that the canonical partition function of the system can be written as

Æ
É

½

½ ½ ¹ exp ¹¬¯

(2.3.2)

Hint: you will have to use the following identity for Ü ½:

½

½

Ü
¼

½¹Ü

(2.3.3)

For the product of partition functions of two independent systems and

we can write

É ¢É É

(2.3.4)

when

« and

.

2. Show that the average occupancy number of state , Ò , is equal to

Ò

ln É

½

´¹¬¯ µ exp ¬¯ ¹ ½

½ 3. Describe the behavior of Ò when Ì

and when Ì ¼.

(2.3.5)

20

Chapter 2. Statistical Mechanics

Question 6 (Ideal Gas (Part 2)) An ideal gas is placed in a constant gravitational
Æ Þ Å Þ Å ÑÆ eld. The potential energy of gas molecules at height is , where Æ is the total mass of molecules. The temperature in the system is uniform and the
system in nitely large. We assume that the system is locally in equilibrium, so we
are allowed to use a local partition function.

Î 1. Show that the grand-canonical partition function of a system in volume at Þ height is equal to

É´Î

Ì Þµ

½ exp ¬ Æ Æ ¼ ¿ÆÆ

exp ¹¬ ´À¼ · Å Þµ (2.3.6)

À Þ ¼ in which ¼ is the Hamiltonian of the system at

.

Þ 2. Explain that a change in is equivalent to a change in chemical potential, . Þ Use this to show that the pressure of the gas at height is equal to

Ô ´Þµ Ô ´Þ ¼µ ¢ exp ¹¬Ñ Þ

(2.3.7)

(Hint: you will need the formula for the chemical potential of an ideal gas.)

Exercise 1 (Distribution of Particles)
Æ Î Consider an ideal gas of particles in a volume at constant energy . Ô Let us divide the volume in identical compartments. Every compartment Ò contains molecules such that

Ô
ÆÒ
½

(2.3.8)

Ô An interesting quantity is the distribution of molecules over the compart-

ments. Because the energy is constant, every possible eigenstate of the

system will be equally likely. This means that in principle it is possible that

one of the compartments is empty.

1. On the book’s website you can ﬁnd a program that calculates the distri-
Ô bution of molecules among the compartments. Run the program Ô for different numbers of compartments ( ) and total number of gas Æ molecules ( ). Note that the code has to be completed ﬁrst (see the Ü Ü ﬁle distribution.f). The output of the program is the probability of ﬁnd-
ing particles in a particular compartment as a function of . This is
printed in the ﬁle output.dat.

2. What is the probability that one of the compartments is empty?

Ô ¾ Æ Æ ¾ 3. Consider the case

and even. The probability of ﬁnding

·

Ò ½ Æ ¾ Ò ½ molecules in compartment and

¹ ½ molecules in compart-

¾ ment is given by

È ´Ò½µ

Æ ´Æ ¾ ¹ Ò½µ ´Æ ¾ · Ò½µ ¾Æ

(2.3.9)

2.3 Questions and Exercises

21

Compare your numerical results with the analytical expression for dif-

Æ ferent values of . Show that this distribution is a Gaussian for small

Ò Æ Ü¼ ½ ½ . Hint: For

, it might be useful to use Stirling’s approxima-

tion:

Ü ¾ Ü Ü ´ µ ½¾ Ü· ½¾ exp ¹

(2.3.10)

Exercise 2 (Bolt mann Distribution)

Æ ¼¯¾¯ Consider a system of energy levels with energies

¯ ¼ and

.

¡ ¡ ¡ ´Æ ¹ ½µ ¯

1. Calculate, using the given program, the occupancy of each level for different values of the temperature. What happens at high temperatures?

2. Change the program in such a way that the degeneracy of energy level
½ equals · . What do you see?

3. Modify the program in such a way that the occupation of the energy
Õ levels as well as the partition function ( ) is calculated for a hetero Á nuclear linear rotor with moment of inertia . Compare your result with

the approximate result

Õ

¾Á ¬¯¾

(2.3.11)

for different temperatures. Note that the energy levels of a linear rotor

are
with Â ¼½¾

Í

Â

´Â

·

½µ

¯¾
¾Á

(2.3.12)

½ ¡ ¡ ¡ Â ¾Â ½ . The degeneracy of level equals · .

Exercise 3 (Coupled Harmonic Oscillators)

Æ Í Consider a system of harmonic oscillators with a total energy . A single

¼¯¾¯ ½ ¯ ¼ harmonic oscillator has energy levels

¡¡¡ (

). All harmonic

oscillators in the system can exchange energy.

Í 1. Invent a computational scheme to update the system at constant total energy ( ). Compare your scheme with the scheme that is incorporated in the computer code that you can ﬁnd on the book’s website (see the ﬁle harmonic.f).

2. Make a plot of the energy distribution of the ﬁrst oscillator as a function

Í Æ of the number of oscillators for a constant value of

(output.dat).

Æ Which distribution is recovered when becomes large? What is the

Æ ½ function of the other ¹ harmonic oscillators? Explain.

3. Compare this distribution with the canonical distribution of a single oscillator at the same average energy (use the option NVT).

4. How does this exercise relate to the derivation of the Boltzmann distri-
Ì bution for a system at temperature ?

22

Chapter 2. Statistical Mechanics

Exercise 4 (Random alk on a 1D Lattice) Consider the random walk of a single particle on a line. The particle performs jumps of ﬁxed length1. Assuming that the probability for forward or backward
Æ jumps is equal, the mean-squared displacement of a particle after jumps Æ Æ is equal to . The probability that, after jumps, the net distance covered Ò by the particle equals is given by

ln È ´ÒÆ µ

½ ¾ ln

¾ Ò¾ Æ ¹ ¾Æ

Ü 1. Derive this equation using Stirling’s approximation for ln .

2. Compare your numerical result for the root mean-squared displace-
È ÒÆ ment with the theoretical prediction (the computed function ´ µ,
see the ﬁle output.dat). What is the diffusivity of this system?

¼ 3. Modify the program in such a way that the probability to jump in the forward direction equals . What happens?

Æ Å ¢ Å Exercise 5 (Random alk on a 2D Lattice)
Consider the random walk of particles on a

lattice. Two particles

cannot occupy the same lattice site. On this lattice, periodic boundaries are

used. This means that when a particle leaves the lattices it returns on the
Å opposite side of the lattice; i.e., the coordinates are given modulo .

1. What is the fraction of occupied sites ( ) of the lattice as a function of
Å and Æ?

Å ¿¾ 2. Make a plot of the diffusivity as a function of for

. For low

values of , the diffusivity can be approximated by

½¼ ´ ¹ µ

Why is this equation reasonable at low densities? Why does it break down at higher densities?
3. Modify the program in such a way that the probability to jump in one direction is larger than the probability to jump in the other direction. Explain the results.
4. Modify the program in such a way that periodic boundary conditions are used in one direction and reﬂecting boundary conditions in the other. What happens?

Chapter 3
Monte Carlo Simulations

In the present chapter, we describe the basic principles of the Monte Carlo
Æ Î Ì method. In particular, we focus on simulations of systems of a ﬁxed number
of particles ( ) in a given volume ( ) at a temperature ( ).

3.1 The Monte Carlo Method

In the previous chapter, we introduced some of the basic concepts of (classical) statistical mechanics. Our next aim is to indicate where the Monte Carlo
É method comes in. We start from the classical expression for the partition
function , equation (2.2.5):

É dpÆdrÆ exp ¹À´rÆ pÆµ Ì

(3.1.1)

Æ where rÆ stands for the coordinates À responding momenta. The function

of

´aqllÆ

ppÆaµrtiiscltehse,

and pÆ for the cor-
Hamiltonian of the

system. It expresses the total energy of an isolated system as a function of

À Ã Í the coordinates and momenta of the constituent particles:

· , where

Ã Í is the kinetic energy of the system and is the potential energy. Finally,

is a constant of proportionality, chosen such that the sum over quantum

¼ Æ ½ Æ states
limit ¯

in

equation (2.1.15) approaches . For instance, for a system

the of

classical partition identical atoms,

functio´ n¿iÆn

the
µ.

The classical equation corresponding to equation (2.2.1) is
Ê dpÆdrÆ ´pÆ rÆµ exp ¹¬À´pÆ rÆµ Ê dpÆdrÆ exp ¹¬À´pÆ rÆµ

(3.1.2)

¬ ½ Ì where

. In this equation, the observable has been expressed

Ã as a function of coordinates and momenta. As is a quadratic function of

24

Chapter 3. Monte Carlo Simulations

the momenta the integration over momenta can be carried out analytically.

Hence, averages of functions that depend on momenta only are usually easy

ttioonevs alu´arÆteµ..1

The difﬁcult problem is the computation of averages of funcOnly in a few exceptional cases can the multidimensional in-

tegral over particle coordinates be computed analytically; in all other cases

numerical techniques must be used.

Having thus deﬁned the nature of the numerical problem that we must

solve, let us next look at possible solutions. It might appear that the most

straightforward approach would be to evaluate in equation (3.1.2) by

numerical quadrature, for instance using Simpson’s rule. It is easy to see,

however, that such a method is completely useless even if the number of
Æ independent coordinates ( is the dimensionality of the system) is still ½¼¼ very small Ç´ µ. Suppose that we plan to carry out the quadrature by Æ evaluating the integrand on a mesh of points in the -dimensional conﬁgÑ uration space. Let us assume that we take equidistant points along each

Ñ ceovoarludainteadteisaxthise.nTehqeutaoltatol numÆb.erFoofr

points all but

at which the the smallest

integrand must be systems this num-

Ñ ber becomes astronomically large, even for small values of . For instance,

if we take ½¼¼ particles in
to evaluate the integrand

tahtr½e¼e¾d½¼impeoninsitosn! sC, oamndpÑutation, sthoefnswucehwmoaugldnihtuadvee

cannot be performed in the known universe. And this is fortunate, because

the answer that would be obtained would have been subject to a large sta-

tistical error. After all, numerical quadratures work best on functions that

are smooth over distances corresponding to the mesh size. But for most in-

termolecular potentials, the Boltzmann factor in equation (3.1.2) is a rapidly

varying function of the particle coordinates. Hence an accurate quadrature
Ñ requires a small mesh spacing (i.e., a large value of ). Moreover, when

evaluating the integrand for a dense liquid (say), we would ﬁnd that for the

overwhelming majority of points this Boltzmann factor is vanishingly small.

For instance, for a ﬂuid of ½¼¼ hard ½ mann factor would be nonzero for

sopuhteorfesevaet rtyhe½¼f¾re¼ezcionngﬁpgouirnatt,itohnes!Boltz-

The preceding example clearly demonstrates that better numerical tech-

niques are needed to compute thermal averages. One such a technique is

the Monte Carlo method or, more precisely, the Monte Carlo importance-

sampling algorithm introduced in 1953 by Metropolis et al. [6]. The applica-

tion of this method to the numerical simulation of dense molecular systems

is the subject of the present chapter.

3.1.1 Importance Sampling
Before discussing importance sampling, let us ﬁrst look at the simplest Monte Carlo technique, that is, random sampling. Suppose we wish to evaluate
1This is not the case when hard constraints are used, see section 11.2.1.

3.1 The Monte Carlo Method

25

numerically a one-dimensional integral Á:

Á

dÜ ´Üµ

(3.1.3)

Instead of using a conventional quadrature where the integrand is evaluated at predetermined values of the abscissa, we could do something else. Note that equation (3.1.3) can be rewritten as

Á ´ ¹ µ ´Üµ

(3.1.4)

where ´Üµ denotes the unweighted average of ´Üµ over the interval

.

In brute force Monte Carlo, this average is determined by evaluating ´Üµ at

½ a large number (say, Ä) of Ü values randomly distributed over the interval

. It is clear that, as Ä

, this procedure should yield the correct value

for Á. However, as with the conventional quadrature procedure, this method

is of little use to evaluate averages such as in equation (3.1.2) because most of

the computing is spent on points where the Boltzmann factor is negligible.

Clearly, it would be much preferable to sample many points in the region

where the Boltzmann factor is large and few elsewhere. This is the basic

idea behind importance sampling.

How should we distribute our sampling through conﬁguration space?

To see this, let us ﬁrst consider a simple, one-dimensional example. Suppose

we wish to compute the deﬁnite integral in equation (3.1.3) by Monte Carlo

sampling, but with the sampling points distributed nonuniformly over the

interval

(for convenience we assume ¼ and ½), according to

´ µ some nonnegative probability density Û Ü . Clearly, we can rewrite equa-

tion (3.1.3) as

Á

½ ¼

dÜ

Û´Üµ

´Üµ Û´Üµ

(3.1.5)

´ µ Let us assume that we know that Û Ü is the derivative of another (nonnegative, nondecreasing) function Ù´Üµ, with Ù´¼µ ¼ and Ù´½µ ½ (these ´ µ boundary conditions imply that Û Ü is normalized). Then Á can be written

as

Á

½ ¼

dÙ

Ü´Ùµ Û Ü´Ùµ

(3.1.6)

´ µ In equation (3.1.6) we have written Ü Ù to indicate that, if we consider Ù as

the integration variable, then Ü must be expressed as a function of Ù. The

next step is to generate Ä random values of Ù uniformly distributed in the

interval ¼½ . We then obtain the following estimate for Á:

Ä
½

Ü´Ù µ

´ µ Á
Ä ½ÛÜ Ù

(3.1.7)

26

Chapter 3. Monte Carlo Simulations

What have we gained by rewriting Á in this way? The answer depends cru-

cially on our choice for Û´Üµ. To see this, let us estimate

¾ Á

,

the

variance

in

ÁÄ, where ÁÄ denotes the estimate for Á obtained from equation (3.1.7) with Ä

random sample points:

¾

½Ä Ä

Á Ä¾ ½ ½

Ü´Ù µ ¹Û
Û Ü´Ù µ

Ü´Ù µ ¹Û
Û Ü´Ù µ

(3.1.8)

½ where the angular brackets denote the true average, that is, the one that

would be obtained in the limit Ð

. As different samples and are

assumed to be totally independent, all cross terms in equation (3.1.8) vanish,

and we are left with

¾ Á

Ä¶

½

Ü´Ù µ

ª « Ä¾ ½

¹ Û Ü´Ù µ

½ ´ Ûµ¾ ¹ Û ¾

¾·
Û

Ä

(3.1.9)

Equation (3.1.9) shows that the variance in Á still goes as ½ Ä, but the mag-

nitude of this variance can be reduced greatly by choosing Û´Üµ such that

´Üµ Û´Üµ is a smooth function of Ü. Ideally, we should have ´Üµ Û´Üµ con-

stant, in which case the variance would vanish altogether. In contrast, if Û´Üµ

is constant, as is the case for the brute force Monte Carlo sampling, then the

relative error in Á can become very large. For instance, if we are sampling

in a (multidimensional) conﬁguration space small fraction is accessible (for instance,

o½f¼v¹o¾lu¼m, seeeªp, roefvwiohuischseoctniolyn)a,

then the relative error that results in a brute force MC sampling will be of

order ½ ´Ä µ. As the integrand in equation (3.1.2) is nonzero only for those

conﬁgurations where the Boltzmann factor is nonzero, it would clearly be

advisable to carry out a nonuniform Monte Carlo sampling of conﬁguration

space, such that the weight function Û is approximately proportional to the

Boltzmann factor. Unfortunately, the simple importance sampling scheme

described previously cannot be used to sample multidimensional integrals

over conﬁguration space, such as equation (3.1.2). The reason is simply that

we do not know how to construct a transformation such as the one from

equation (3.1.5) to equation (3.1.6) that would enable us to generate points

in conﬁguration space with a probability density proportional to the Boltz-

mann factor. In fact, a necessary (but not nearly sufﬁcient) condition for the

solution to the latter problem is that we must be able to compute analytically

the partition function of the system under study. If we could do that for the

systems of interest to us, there would be hardly any need for computer sim-

ulation.

3.1 The Monte Carlo Method

27

3.1.2 The Metropolis Method

The closing lines sible to evaluate

oanf tihnetepgrreavli,osuuscshecatsioÊndsruÆggeexspt

¹ th¬aÍt i´triÆs

in µ,

general not posby direct Monte

Carlo sampling. However, in many cases, we are not interested in the con-

ﬁgurational part of the partition function itself but in averages of the type

Ê

drÊÆdreÆxpex¹p¬¹ Í¬´rÍÆ´µrÆ

´rÆ
µ

µ

(3.1.10)

Hence, we wish to know the ratio of two integrals. What Metropolis et al. [6] showed is that it is possible to devise an efﬁcient Monte Carlo scheme to sample such a ratio.2 To understand the Metropolis method, let us ﬁrst look more closely at the structure of equation (3.1.10). In what follows we denote the conﬁgurational part of the partition function by :

drÆ exp ¹¬Í Æ ´r µ

(3.1.11)

Note that the ratio exp´¹¬Í
sity of ﬁnding the system in

µ
a

in equation conﬁguration

(3.1.10) around

irsÆt.hLe eptruosbadbeinliotytedtehnis-

probability density by

Æ ´rÆµ exp ¹¬Í Æ ´r µ

Clearly, Æ ´rÆµ is nonnegative.

conSﬁugpupraotsieonnoswpatcheaat cwcoeradriengsotmo ethhoiswparobbleabtoilirtaynddiosmtrilbyugteionnerÆat´erpÆoµi.ntTshiins

means that, on around a point

arÆveirsaegqeu, tahletonuÄmÆb´erÆr oµ,f

points where

Ò generated per unit volume Ä is the total number of points

that we have generated. In other words,

½ Ä

Ä Ò ½

Æ ´r µ

(3.1.12)

By now the reader is almost certainly confused about the difference, if any, between equation (3.1.12) and equation (3.1.7) of section 3.1.1. The difference
bipesxolitptnhhga¹tea¬xinÍppo´t¹ hriÆne¬tµÍcia;n´strehÆaaoµ(tfhieaysqn,pudweare)tvi.koonnIluno(wm3c.1oeo.n7ndt)lrrywaÆstetha,kreionnruoenewlaqdtuairvapÆetri.ioboInurnit(ton3ht.oe1ht.ep1trrh2o)webwaoabrbedsiloksitlnwuyoteowefkpsonranoomblwy--
ability of visiting different points in conﬁguration space. This may sound
2An interesting account of the early history of the Metropolis method may be found in refs. [1, 46].

28

Chapter 3. Monte Carlo Simulations

Nile

Nile

Figure 3.1: Measuring the depth of the Nile: a comparison of conventional quadrature (left), with the Metropolis scheme (right).

rather abstract: let us therefore try to clarify the difference with the help of a simple example (see Figure 3.1). In this ﬁgure, we compare two ways of measuring the depth of the river Nile, by conventional quadrature (left) and by Metropolis sampling; that is, the construction of an importance-weighted random walk (right). In the conventional quadrature scheme, the value of the integrand is measured at a predetermined set of points. As the choice of these points does not depend on the value of the integrand, many points may be located in regions where the integrand vanishes. In contrast, in the Metropolis scheme, a random walk is constructed through that region of space where the integrand is nonnegligible (i.e., through the Nile itself). In this random walk, a trial move is rejected if it takes you out of the water and is accepted otherwise. After every trial move (accepted or not), the depth of the water is measured. The (unweighted) average of all these measurements yields an estimate of the average depth of the Nile. This, then, is the essence of the Metropolis method. In principle, the conventional quadrature scheme would also give results for the total area of the Nile. In the importance sampling scheme, however, information on the total area cannot be obtained directly, since this quantity is similar to .

Let us next consider how to generate points in conﬁguration space with

a relative proach is

ﬁprrsotbtaobpilriteypaprreopthoertsiyosntaelmtointhaecBoonlﬁtzgmuraantnionfacrÆto,r.wThhicehgweneedraelnaopte-

by Ó (old), that has a nonvanishing Boltzmann factor exp ¹¬Í Ó´ µ . This

conﬁguration, for with no hard-core

example, overlaps.

may Next,

correspond to we generate a

naerwegturilaalrccornyﬁsgtaullrianteiolnatrt¼iÆce,

3.1 The Monte Carlo Method

29

which we denote by Ò (new), by adding a small random displacement ¡ to Ó. The Boltzmann factor of this trial conﬁguration is exp ¹¬Í Ò´ µ . We

must now decide whether we will accept or reject the trial conﬁguration.

Many rules for making this decision satisfy the constraint that on average
Æthe probability of ﬁnding the system in a conﬁguration Ò is proportional to
Ò´ µ. Here we discuss only the Metropolis scheme, because it is simple and
generally applicable. Let us now “derive” the Metropolis scheme to determine the transition
probability ´Ó Òµ to go from conﬁguration Ó to Ò. It is convenient to
start with a thought experiment (actually a thought simulation). We carry
out a very large number (say Å) Monte Carlo simulations in parallel, where Å is much larger than the total number of accessible conﬁgurations. We
Æ denote the number of points in any conﬁguration Ó by Ñ´Óµ. We wish that,
on average, Ñ´Óµ is proportional to Ó´ µ. The matrix elements ´Ó Òµ

must satisfy one obvious condition: they do not destroy such an equilibrium

distribution once it is reached. This means that, in equilibrium, the average
number of accepted trial moves that result in the system leaving state Ó must be exactly equal to the number of accepted trial moves from all other states Ò to state Ó. It is convenient to impose a much stronger condition; namely, that in equilibrium the average number of accepted moves from Ó to any other state Ò is exactly canceled by the number of reverse moves. This detailed

balance condition implies the following:

Æ Æ Ó Ó Ò ´ µ ´

µ

Ò Ò Ó ´ µ ´

µ

(3.1.13)

Many possible forms of the transition matrix ´Ó Òµ satisfy equation (3.1.13). Let us look how ´Ó Òµ is constructed in practice. We recall that

a Monte Carlo move consists of two stages. First, we perform a trial move
from state Ó to state Ò. We denote the transition matrix that determines the probability of performing a trial move from Ó to Ò by «´Ó Òµ, where « is

usually referred to as the underlying matrix of the Markov chain [47]. The next stage is the decision to either accept or reject this trial move. Let us
denote the probability of accepting a trial move from Ó to Ò by acc´Ó Òµ.

Clearly,

¢ ´Ó Òµ «´Ó Òµ acc´Ó Òµ

(3.1.14)

In the original Metropolis scheme, « is chosen to be a symmetric matrix («´Ó Òµ «´Ò Óµ). However, in later sections we shall see several examples where « is not symmetric. If « is symmetric, we can rewrite equation (3.1.13) in terms of the acc´Ó Òµ:

Æ ¢ Æ ¢ Ó Ó ´ µ acc´ Òµ

Ò Ò ´ µ acc´ Óµ

(3.1.15)

From equation (3.1.15) follows

Æ acc´Ó Òµ

Ò´ µ

Æ acc´Ò Óµ

Ó´ µ

Í Í ¬ Ò Ó exp ¹

´ µ¹ ´ µ

(3.1.16)

30

Chapter 3. Monte Carlo Simulations

Again, many choices for acc´Ó Òµ satisfy this condition (and the obvious condition that the probability acc´Ó Òµ cannot exceed ½). The choice of
Metropolis et al. is

acc´Ó Òµ

Æ Æ Æ Æ ´Òµ ´Óµ if ´Òµ

´Óµ

Æ Æ ½

if ´Òµ

´Óµ

(3.1.17)

Other choices for acc´Ó Òµ are possible (for a discussion, see for instance
[19]), but the original choice of Metropolis et al. appears to result in a more efﬁcient sampling of conﬁguration space than most other strategies that have been proposed.
In summary, then, in the Metropolis scheme, the transition probability
for going from state Ó to state Ò is given by

´Ó Òµ ´Ó Óµ

È Æ Æ «´Ó Òµ
«´Ó Òµ ´Òµ ´Óµ ½ ¹ Ò Ó ´Ó Òµ

Æ ´Òµ Æ ´Òµ

Æ ´Óµ Æ ´Óµ

(3.1.18)

Note that we still have not speciﬁed the matrix «, except for the fact that it
must be symmetric. This reﬂects considerable freedom in the choice of our trial moves. We will come back to this point in subsequent sections.
One thing that we have not yet explained is how to decide whether a trial move is to be accepted or rejected. The usual procedure is as follows.
Í Í Suppose that we have generated a trial move from state Ó to state Ò, with
´Òµ ´Óµ. According to equation (3.1.16) this trial move should be ac-
cepted with a probability

Í Í acc´Ó Òµ exp ¹¬ ´Òµ ¹ ´Óµ ½

In order to decide whether to accept or reject the trial move, we generate
a random number, denoted by Ranf, from a uniform distribution in the in-
terval ¼½ . Clearly, the probability that Ranf is less than acc´Ó Òµ is equal to acc´Ó Òµ. We now accept the trial move if Ranf acc´Ó Òµ
and reject it otherwise. This rule guarantees that the probability to accept a
trial move from Ó to Ò is indeed equal to acc´Ó Òµ. Obviously, it is very
important that our random-number generator does indeed generate num-
bers uniformly in the interval ¼½ . Otherwise the Monte Carlo sampling
will be biased. The quality of random-number generators should never be
taken for granted. A good discussion of random-number generators can be
found in Numerical Recipes [33] and in Monte Carlo Methods by Kalos and
Whitlock [32].
Thus far, we have not mentioned another condition that ´Ó Òµ should
satisfy, namely that it is ergodic (i.e., every accessible point in conﬁguration
space can be reached in a ﬁnite number of Monte Carlo steps from any other
point). Although some simple MC schemes are guaranteed to be ergodic,

3.2 A Basic Monte Carlo Algorithm

31

these are often not the most efﬁcient schemes. Conversely, many efﬁcient Monte Carlo schemes have either not been proven to be ergodic or, worse, been proven to be nonergodic. The solution is usually to mix the efﬁcient, nonergodic scheme with an occasional trial move of the less-efﬁcient but ergodic scheme. The method as a whole will then be ergodic (at least, in principle).
At this point, we should stress that, in the present book, we focus on Monte Carlo methods to model phenomena that do not depend on time. In the literature one can also ﬁnd dynamic Monte Carlo schemes. In such dynamic algorithms, Monte Carlo methods are used to generate a numerical solution of the master equation that is supposed to describe the time evolution of the system under study. These dynamic techniques fall outside the scope of this book. The reader interested in dynamic MC schemes is referred to the relevant literature, for example Ref. [48] and references therein.

3.2 A Basic Monte Carlo Algorithm
It is difﬁcult to talk about Monte Carlo or Molecular Dynamics programs in abstract terms. The best way to explain how such programs work is to write them down. This will be done in the present section.
Most Monte Carlo or Molecular Dynamics programs are only a few hundred to several thousand lines long. This is very short compared to, for instance, a typical quantum-chemistry code. For this reason, it is not uncommon that a simulator will write many different programs that are tailormade for speciﬁc applications. The result is that there is no such thing as a standard Monte Carlo or Molecular Dynamics program. However, the cores of most MD/MC programs are, if not identical, at least very similar. Next, we shall construct such a core. It will be very rudimentary, and efﬁciency has been traded for clarity. But it should demonstrate how the Monte Carlo method works.
3.2.1 The Algorithm
The prime purpose of the kind of Monte Carlo or Molecular Dynamics program that we shall be discussing is to compute equilibrium properties of classical many-body systems. From now on, we shall refer to such programs simply as MC or MD programs, although it should be remembered that there exist many other applications of the Monte Carlo method (and, to a lesser extent, of the Molecular Dynamics method). Let us now look at a simple Monte Carlo program.
In the previous section, the Metropolis method was introduced as a Markov process in which a random walk is constructed in such a way that the

32

Chapter 3. Monte Carlo Simulations

pfarcotboarbeixlipty¹o¬fÍv´isriÆtiµn.gTahpearertaicrue lmarapnoyiwntaryÆs

is to

proportional to the Boltzmann construct such a random walk.

In the approach introduced by Metropolis et al. [6], the following scheme is

proposed:

1. Select a particle at random, and calculate its energy Í´rÆµ.

2.

Give new

etnheerpgyarÍti´crle¼Æaµr. andom

displacement,

Ö

¼

Ö · ¡, and calculate its

3. Accept the move from rÆ to r Æ¼ with probability

acc´Ó

Òµ

min

Æ Æ ½ ¬ Í Í exp ¹

r r ¼

´

µ¹ ´ µ

(3.2.1)

An implementation of this basic Metropolis scheme is shown in Algorithms 1 and 2.

3.2.2 Technical Details
In this section, we discuss a number of computational tricks that are of great practical importance for the design of an efﬁcient simulation program. It should be stressed that most of these tricks, although undoubtedly very useful, are not unique and have no deep physical signiﬁcance. But this does not imply that the use of such computational tools is free of risks or subtleties. Ideally, schemes to save computer time should not affect the results of a simulation in a systematic way. Yet, in some cases, time-saving tricks do have a measurable effect on the outcome of a simulation. This is particularly true for the different procedures used to avoid explicit calculation of intermolecular interactions between particles that are far apart. Fortunately, once this is recognized, it is usually possible to estimate the undesirable side effect of the time-saving scheme and correct for it.

Boundary Conditions
Monte Carlo and Molecular Dynamics simulations of atomic or molecular systems aim to provide information about the properties of a macroscopic sample. Yet, the number of degrees of freedom that can be conveniently handled in present-day computers ranges from a few hundred to a few million. Most simulations probe the structural and thermodynamical properties of a system of a few hundred to a few thousand particles. Clearly, this number is still far removed from the thermodynamic limit. To be more precise, for such small systems it cannot be safely assumed that the choice of the boundary conditions (e.g., free or hard or periodic) has a negligible effect on the
properties of the system. In fact, in a three-dimensional Æ-particle system

3.2 A Basic Monte Carlo Algorithm

33

Algorithm 1 (Basic Metropolis Algorithm)

PROGRAM mc
do icycl=1,ncycl call mcmove if (mod(icycl,nsamp).eq.0)
+ call sample enddo end

basic Metropolis algorithm
perform ncycl MC cycles displace a particle
sample averages

Comments to this algorithm:
1. Subroutine mcmove attempts to displace a randomly selected particle (see Algorithm 2).
2. Subroutine sample samples quantities every nsampth cycle.

Algorithm 2 (Attempt to Displace a Particle)

SUBROUTINE mcmove
o=int(ranf()*npart)+1 call ener(x(o),eno) xn=x(o)+(ranf()-0.5)*delx call ener(xn,enn) if (ranf().lt.exp(-beta + *(enn-eno)) x(o)=xn return end

attempts to displace a particle
select a particle at random energy old conﬁguration give particle random displacement energy new conﬁguration acceptance rule (3.2.1) accepted: replace x(o) by xn

Comments to this algorithm:
1. Subroutine ener calculates the energy of a particle at the given position. 2. Note that, if a con guration is rejected, the old con guration is retained. 3. The ranf() is a random number uniform in ¼½ .

34

Chapter 3. Monte Carlo Simulations

Figure 3.2: Schematic representation of periodic boundary conditions.

Æ wpriothpofrrteieonbaolutnoda¹ri½es¿,

the fraction of all . For instance, in a

molecules that is at simple cubic crystal

the surface is of 1000 atoms,

½¼ some 49% of all atoms are at the surface, and for atoms this fraction has

decreased to only 6%.

Æ In order to simulate bulk phases it is essential to choose boundary condi-
tions that mimic the presence of an inﬁnite bulk surrounding our -particle

Æ model system. This is usually achieved by employing periodic boundary
conditions. The volume containing the particles is treated as the primitive

cell of an inﬁnite periodic lattice of identical cells (see Figure 3.2). A given

particle ( , say) now interacts with all other particles in this inﬁnite periodic

system, that is, all other particles in the same periodic cell and all particles

(including its own periodic image) in all other cells. For instance, if we as-

Æ sume that all intermolecular interactions are pairwise additive, then the total
potential energy of the particles in any one periodic box is

Ítot

½ ¾

¼Ù´ r · nÄ µ

n

Ä where is the diameter of the periodic box (assumed cubic, for convenience)

and n is an arbitrary vector of three integer numbers, while the prime over

the sum indicates that the term with

is to be excluded when n 0.

In this very general form, periodic boundary conditions are not particularly

useful, because to simulate bulk behavior, we had to rewrite the potential energy as an inﬁnite sum rather than a ﬁnite one.3 In practice, however, we

3In fact, in the ﬁrst MC simulation of three-dimensional Lennard-Jones particles, Wood and Parker [49] discuss the use of such inﬁnite sums in relation to the now conventional approach discussed here.

3.2 A Basic Monte Carlo Algorithm

35

are often dealing with short-range interactions. In that case it is usually permissible to truncate all intermolecular interactions beyond a certain cutoff distance Ö . How this is done in practice is discussed next.
Although the use of periodic boundary conditions proves to be a surprisingly effective method for simulating homogeneous bulk systems, one should always be aware that the use of such boundary conditions may lead to spurious correlations not present in a truly macroscopic bulk system. In particular, one consequence of the periodicity of the model system is that only those ﬂuctuations are allowed that have a wavelength compatible with the periodic lattice: the longest wavelength that still ﬁts in the periodic box is the one for which Ä. If long wavelength ﬂuctuations are expected to be important (as, for instance, in the vicinity of a continuous phase transition), then one should expect problems with the use of periodic boundary conditions. Another unphysical effect that is a manifestation of the use of
periodic boundary conditions is that the radial distribution function ´Öµ of
a dense atomic ﬂuid is found to be not exactly isotropic [50].
Finally, it is useful to point out one common misconception about periodic boundary conditions, namely, the idea that the boundary of the periodic box itself has any special signiﬁcance. It has none. The origin of the periodic lattice of primitive cells may be chosen anywhere, and this choice will not affect any property of the model system under study. In contrast, what is ﬁxed is the shape of the periodic cell and its orientation.

Truncation of Interactions

Let us now consider the case that we perform a simulation of a system with short-range interactions. In this context, short-ranged means that the total potential energy of a given particle is dominated by interactions with neighboring particles that are closer than some cutoff distance Ö . The error that results when we ignore interactions with particles at larger distances can be made arbitrarily small by choosing Ö sufﬁciently large. If we use periodic boundary conditions, the case that Ö is less than Ä ¾ (half the diameter of the periodic box) is of special interest because in that case we need to consider the interaction of a given particle only with the nearest periodic image of any other particles (see the dotted box in Figure 3.2). If the intermolecular potential is not rigorously zero for Ö Ö , truncation of the intermolecular
Í interactions at Ö will result in a systematic error in tot. If the intermolec-
ular interactions decay rapidly, one may correct for the systematic error by
Í adding a tail contribution to tot:

Í tot

Ù ´Ö µ · Æ

½ dÖ Ù´Öµ Ö¾

¾Ö

(3.2.2)

36

Chapter 3. Monte Carlo Simulations

where Ù stands for the truncated potential energy function and is the

average number density. In writing down this expression, it is implicitly
assumed that the radial distribution function ´Öµ ½ for Ö Ö . Clearly, the

nearest periodic image convention can be applied only if the tail correction is small. From equation (3.2.2) it can be seen that the tail correction to the
mpootreenrtiaapl iednlyertghyanisÖi¹n¿ﬁ(ninitethurneeledssimtheenspiootnesn).tiTalheisnceorgnydiftuionnctiisosnatÙis´ﬁÖµeddeifcathyes

long-range interaction between molecules is dominated by dispersion forces.

However, for the very important case of Coulomb and dipolar interactions, the tail correction diverges and hence the nearest-image convention cannot be used for such systems. In such cases, the interactions with all periodic

images should be taken into account explicitly. Ways to do this are described in Chapter 12.1.

Several factors make truncation of the potential a tricky business. First of

all, it should be realized that, although the absolute value of the potential en-

ergy function decreases with interparticle separation Ö, for sufﬁciently large

Ö, the number of neighbors is a rapidly increasing function of Ö. In fact, the

ansumÖ b¹e½r,

of particles at a distance Ö of a
where is the dimensionality

given of the

atom increases system. As an

asymptotically example, let us

compute the effect of truncating the pair potential for a simple example — the three-dimensional Lennard-Jones ﬂuid. The pair potential for this rather popular model system is given by

Ùlj´Öµ

¯

½¾
Ö ¹Ö

(3.2.3)

The average potential energy (in three dimensions) of any given atom is

given by

½
Ù ´½ ¾µ dÖ Ö ¾ ´ÖµÙ´Öµ
¼

where ´Öµ denotes the average number density at a distance Ö from a given
atom . The factor (1/2) has been included to correct for double counting of
intermolecular interactions. If we truncate the potential at a distance Ö , we ignore the tail contribution Ùtail:

½
Ùtail ´½ ¾µ dÖ Ö ¾ ´ÖµÙ´Öµ
Ö

(3.2.4)

where we have dropped the subscript , because all atoms in the system are
identical. To simplify the calculation of Ùtail, we assume that for Ö Ö , the density ´Öµ is equal to the average number density . If Ù´Öµ is the Lennard-

3.2 A Basic Monte Carlo Algorithm

37

Jones potential, we ﬁnd for Ùtail

Ùtail

½

¾

½ ¾

½

½ dÖ Ö¾Ù´Öµ

Ö½
¯ dÖ Ö¾
Ö

Ö ½¾ ¹ Ö

¿

¯¿

½ ¿

Ö

¿
¹Ö

(3.2.5)

For a cutoff distance Ö = 2.5 the potential has decayed to a value that is

about 1/60th of the well depth. This seems to be a very small value, but

in fact the tail correction is usually nonnegligible. For instance, at a density

¹ ¿ ½, we ﬁnd Ùtail

¼ ¿ ¯. This number is certainly not negligible

compared to the total potential energy per atom (almost 10% at a typical liquid density); hence although we can truncate the potential at 2.5 , we cannot ignore the effect of this truncation.

There are several ways to truncate potentials in a simulation. Although the methods are designed to yield similar results, it should be realized that they yield results that may differ signiﬁcantly, in particular in the vicinity of

critical points [51–53] (see Figure 3.3). Often used methods to truncate the potential are

1. Simple truncation 2. Truncation and shift 3. Minimum image convention.

Simple Truncation The simplest method to truncate potentials is to ignore
all interaction beyond Ö , the potential that is simulated is

´ µ Ùtrunc Ö

¬ Ùlj´Öµ Ö
¼Ö

Ö Ö

(3.2.6)

As already explained, this may result in an appreciable error in our estimate of the potential energy of the true Lennard-Jones potential (3.2.3). Moreover,
as the potential changes discontinuously at Ö , a truncated potential is not
particularly suitable for a Molecular Dynamics simulation. It can, however, be used in Monte Carlo simulations. In that case, one should be aware that there is an “impulsive” contribution to the pressure due to the discontinuous
change of the potential at Ö . That contribution can by no means be ignored.

38

Chapter 3. Monte Carlo Simulations

1.3 1.1

T

0.9

0.7

truncated and shifted tail correction

0.5 0.0 0.2 0.4 0.6 0.8 1.0
ρ

Figure 3.3: Vapor-liquid coexistence curves of various three-dimensional Lennard-Jones ﬂuids: effect of the truncation of the potential on the location of the critical point (large black dots). The upper curve gives the phase envelope for the full Lennard-Jones potential (i.e., a truncated potential with tail correction); the lower curve gives the envelope for the potential that is used in most Molecular Dynamics simulations (truncated and shifted potential with Ö ¾ ), data from [53].

For instance, for the three-dimensional Lennard-Jones system,

¡Èimp

¾ ´Ö µ¯ ¿

¿

Ö

¿
¹ Ö

¾¯ ¿

¿

Ö

¿
¹ Ö

(3.2.7)

It is rare, however, to see this impulsive correction to the pressure applied

in simulations of systems with truncated potentials. Usually, it is simply as-

sumed that we can correct for the truncation of the intermolecular potential

by adding the correction given by equation (3.2.5) to the potential energy.

The corresponding correction to the pressure is

¡Ètail

½ ´½ ¾µ ¾ dÖ Ö¾r ¡ f´Öµ

Ö

½ ¾¯ ¿ ¾

¿

¿Ö

¿
¹ Ö

(3.2.8)

But, as is immediately obvious from a comparison of equations (3.2.7) and (3.2.8), the impulsive correction to the pressure is not equivalent to the tail

3.2 A Basic Monte Carlo Algorithm

39

correction. Rather, the impulsive pressure is the contribution that must be included if one wishes to compute the true pressure of a system with a truncated potential, whereas the tail correction should be included to obtain an estimate of the pressure in a system with untruncated interactions.

Truncated and Shifted In Molecular Dynamics simulations, it is common

to use another procedure: the potential is truncated and shifted, such that

the potential vanishes at the cutoff radius:

Ùtr¹sh´Öµ

¬ Ùlj´Öµ ¹ Ùlj´Ö µ Ö

¼

Ö

Ö Ö

(3.2.9)

In this case, there are no discontinuities in the intermolecular potential and hence no impulsive corrections to the pressure. The advantage of using such a truncated and shifted potential is that the intermolecular forces are always ﬁnite.4 This is important because impulsive forces cannot be handled in those Molecular Dynamics algorithms to integrate the equations of motion that are based on a Taylor expansion of the particle positions. Of course, the potential energy and pressure of a system with a truncated and shifted potential differ from the corresponding properties of both the models with untruncated and with truncated but unshifted pair potentials. But, as before, we can approximately correct for the effect of the modiﬁcation of the intermolecular potential on both the potential energy and the pressure. For the pressure, the tail correction is the same as in equation (3.2.8). For the potential energy, we must add to the long-range correction (3.2.5) a contri-
bution equal to the average number of particles that are within a distance Ö
from a given particle, multiplied by half the value of the (untruncated) pair
potential at Ö . The factor one-half is included to correct for overcounting
of the intermolecular interactions. One should be extremely careful when applying truncated and shifted potentials in models with anisotropic interactions. In that case, truncation should not be carried out at a ﬁxed value of the distance between the molecular centers of mass but at a point where the pair potential has a ﬁxed value, because otherwise the potential cannot be
shifted to ¼ at the point where it is truncated. For Monte Carlo simulations,
this is not serious, but for Molecular Dynamics simulations this would be quite disastrous, as the system would no longer conserve energy, unless the impulsive forces due to the truncating and shifting are taken into account explicitly.

Minimum Image Convention Sometimes the minimum image convention is used. The truncation is in this case not at a spherical cutoff; instead the
4The ﬁrst derivative of the force is discontinuous at the cutoff radius; some authors remove this discontinuity as well (for more details, see [19]).

40

Chapter 3. Monte Carlo Simulations

interaction with the (nearest image) of all the particles in the simulation box is calculated. As a consequence, the potential is not a constant on the surface of a cube around a given particle. Hence, for the same reasons as mentioned in the previous paragraph, the simple minimum image convention should never be used in Molecular Dynamics simulations.
Æ In the preceding, we described some details on how the energy should be
calculated. The implementation of a simple, order- ¾, algorithm to compute the energy will be discussed in section 4.2.2 in the context of a Molecular Dynamics simulation (see Algorithm 5). More advanced schemes to simulate large systems efﬁciently are described in Appendix F.
Initiali ation
To start the simulation, we should assign initial positions to all particles in the system. As the equilibrium properties of the system do not (or, at least, should not) depend on the choice of initial conditions, all reasonable initial conditions are in principle acceptable. If we wish to simulate the solid state of a particular model system, it is logical to prepare the system in the crystal structure of interest. In contrast, if we are interested in the ﬂuid phase, we simply prepare the system in any convenient crystal structure. This crystal subsequently melts, because at the temperature and density of a typical liquid-state point, the solid state is not thermodynamically stable. Actually, one should be careful here, because the crystal structure may be metastable, even if it is not absolutely stable. For this reason, it is unwise to use a crystal structure as the starting conﬁguration of a liquid close to the freezing curve. In such cases, it is better to use the ﬁnal (liquid) conﬁguration of a system at a higher temperature or lower density, where the solid is unstable and has melted spontaneously. In any event, it is usually preferable to use the ﬁnal (well-equilibrated) conﬁguration of an earlier simulation at a nearby state point as the starting conﬁguration for a new run and adjust the temperature and density to the desired values.
The equilibrium properties of a system should not depend on the initial conditions. If such a dependence nevertheless is observed in a simulation, there are two possibilities. The ﬁrst is that our results reﬂect the fact that the system that we simulate really behaves nonergodically. This is the case, for instance, in glassy materials or low-temperature, substitutionally disordered alloys. The second (and much more likely) explanation is the system we simulate is ergodic but our sampling of conﬁguration space is inadequate; in other words, we have not yet reached equilibrium.
Reduced Units
In simulations it is often convenient to express quantities such as temperature, density, pressure, and the like in reduced units. This means that we choose a convenient unit of energy, length and mass and then express all

3.2 A Basic Monte Carlo Algorithm

41

other quantities in terms of these basic units. In the example of a Lennard-
Jones system, we use a pair potential that is of the form Ù´Öµ ¯ ´Ö µ (see
equation (3.2.3)). A natural (though not unique) choice for our basic units is the following:

¯ Unit of length,
¯ Unit of energy, ¯ ¯ Unit of mass, Ñ (the mass of the atoms in the system)

and from these basic units, all other units follow. For instance, our unit of

time is

Ô

Ñ¯

and the unit of temperature is
¯

In terms of these reduced units, denoted with superscript *, the reduced
pair potential Ù£ Ù ¯ is a dimensionless function of the reduced distance Ö£ Ö . For instance, the reduced form for the Lennard-Jones potential is

Ù£lj´Ö£µ

½ ½¾ ½ Ö£ ¹ Ö£

(3.2.10)

aWtennidtthitahtlheeentseeemrgcpoyenrÍva£etunrtieoÍÌn¯£s¹w½,ethcaÌen¯p¹dr½ee.sﬁsnuerethÈe£

follÈow¿i¯n¹g½,rethdeucdeednsuintyits:£

the

po¿,

One may wonder why it is convenient to introduce reduced units. The
most important reason is that (inﬁnitely) many combinations of Ì ¯, and

all correspond to the same state in reduced units. This is the law of corre-

sponding states: the same simulation of a Lennard-Jones model can be used to study Ar at 60 K and a density of 840 kg/m¿ and Xe at 112 K and a density of 1617 kg/m¿. In reduced units, both simulations correspond to the state
point £ ¼ , Ì£ ¼ . If we had not used reduced units, we might have

easily missed the equivalence of these two simulations. Another, practical,

reason for using reduced units is the following: when we work with real (SI)

units, we ﬁnd that the absolute numerical values of the quantities that we

are computing (e.g., the average energy of a particle or its acceleration) are

either much less or much larger than 1. If we multiply several such quanti-

ties using standard ﬂoating-point multiplication, we face a distinct risk that,

at some stage, we will obtain a result that creates an overﬂow or under-

ﬂow. order

Conversely, in 1 (say, between

½re¼d¹u¿ caenddu½n¼i¿t)s.,

almost all quantities of interest are of Hence, if we suddenly ﬁnd a very large

(or very small) number in our simulations (say, ½¼ ¾), then there is a good

chance that we have made an error somewhere. In other words, reduced

42

Chapter 3. Monte Carlo Simulations

Quantity temperature density time pressure

Reduced units
Ì£ ½ £ ½¼ ¡Ø£ ¼ ¼¼ È£ ½

°°°°

Real units

Ì ½½ K

¡Ø

½

½

¼ ¼

k¢g/½m¼¹¿½

s

È ½ MPa

¢ Table
(¯

3.1: Translation of
½½ K, ¿

¼reduc½e¼d¹½u¼nmit,sÅto

real
¼

units
¼¿

for Lennard-Jones kg/mol)

argon

units make it easier to spot errors. Simulation results that are obtained in

reduced units can always be translated back into real units. For instance, if

we wish to compare the results of a simulation on a Lennard-Jones model

¢ at Ì£
¿

¼

½

an½d¼¹È½£¼

½ with m, Å ¼

experimental data for argon (¯ ¼¿ kg/mol), then we can use

the

½½ K,
translation

given in Table 3.1 to convert our simulation parameters to real SI units.5

3.2.3 Detailed Balance versus Balance
Throughout this book we use the condition of detailed balance as a test of the validity of a Monte Carlo scheme. However, as stated before, the detailed-balance condition is sufﬁcient, but not necessary. Manousiouthakis and Deem [54] have shown that the weaker ”balance condition” is a necessary and sufﬁcient requirement. A consequence of this proof is that one has more freedom in developing Monte Carlo moves. For example, in the simple Monte Carlo scheme shown in Algorithm 2 we select a particle at random and give it a random displacement. During the next trial move, the a priori probability to select the same particle is the same. Thus the reverse trial move has the same a priori probability and detailed balance is satisﬁed. An alternative scheme is to attempt moving all particles sequentially, i.e., ﬁrst an attempt to move particle one, followed by an attempt to move particle two, etc. In this sequential scheme, the probability that a single-particle move is followed by its reverse is zero. Hence, this scheme clearly violates detailed balance. However, Manousiouthakis and Deem have shown that such a sequential updating scheme does obey balance and does therefore (usually — see Ref. [54]) result in correct MC sampling.
We stress that the detailed-balance condition remains an important guiding principle in developing novel Monte Carlo schemes. Moreover, most algorithms that do not satisfy detailed balance are simply wrong. This is true in particular for ”composite” algorithms that combine different trial moves. Therefore, we suggest that it is good practice to impose detailed balance
5In what follows we will always use reduced units, unless explicitly indicated otherwise. We, therefore, omit the superscript * to denote reduced units.

3.3 Trial Moves

43

when writing a code. Of course, if subsequently it turns out that the performance of a working program can be improved considerably by using a ”balance-only” algorithm, then it is worth implementing it. At present, we are not aware of examples in the literature where a ”balance-only” algorithm is shown to be much faster than its ”detailed-balance” counterpart.

3.3 Trial Moves
Now that we have speciﬁed the general structure of the Metropolis algorithm, we should consider its implementation. We shall not go into the problem of selecting intermolecular potentials for the model system under study. Rather, we shall simply assume that we have an atomic or molecular model system in a suitable starting conﬁguration and that we have speciﬁed all intermolecular interactions. We must now set up the underlying Markov
chain, that is, the matrix «. In more down to earth terms: we must decide
how we are going to generate trial moves. We should distinguish between trial moves that involve only the molecular centers of mass and those that change the orientation or possibly even the conformation of a molecule.

3.3.1 Translational Moves

We start our discussion with trial moves of the molecular centers of mass. A perfectly acceptable method for creating a trial displacement is to add
random numbers between ¹¡ ¾ and ·¡ ¾ to the ÜÝ , and Þ coordinates of
the molecular center of mass:

Ü¼

Ü · ¡ ´Ranf ¹ ¼ µ

Ý¼

Ý · ¡ ´Ranf ¹ ¼ µ

Þ¼

Þ · ¡ ´Ranf ¹ ¼ µ

(3.3.1)

where Ranf are random numbers uniformly distributed between ¼ and ½. Clearly, the reverse trial move is equally probable (hence, « is symmetric).6 We are now faced with two questions: how large should we choose ¡? and
should we attempt to move all particles simultaneously or one at a time? In the latter case we should pick the molecule that is to be moved at random to ensure that the underlying Markov chain remains symmetric. All

6Although almost all published MC simulations on atomic and molecular systems generate trial displacements in a cube centered around the original center of mass position, this is by no means the only possibility. Sometimes, it is more convenient to generate trial moves in a spherical volume, and it is not even necessary that the distribution of trial moves in such a volume be uniform, as long as it has inversion symmetry. For an example of a case where another sampling scheme is preferable, see ref. [55].

44

Chapter 3. Monte Carlo Simulations

other things being equal, we should choose the most efﬁcient sampling procedure. But, to this end, we must ﬁrst deﬁne what we mean by ef cient sampling. In very vague terms, sampling is efﬁcient if it gives you good value for money. Good value in a simulation corresponds to high statistical accuracy, and “money” is simply money: the money that buys your computer time and even your own time. For the sake of the argument, we assume the average scientiﬁc programmer is poorly paid. In that case we have to worry only about your computer budget.7 Then we could use the following deﬁnition of an optimal sampling scheme: a Monte Carlo sampling scheme can be considered optimal if it yields the lowest statistical error in the quantity to be computed for a given expenditure of computing budget. Usually, computing budget is equivalent to CPU time.

From this deﬁnition it is clear that, in principle, a sampling scheme may be optimal for one quantity but not for another. Actually, the preceding definition is all but useless in practice (as are most deﬁnitions). For instance, it is just not worth the effort to measure the error estimate in the pressure for a number of different Monte Carlo sampling schemes in a series of runs of ﬁxed length. However, it is reasonable to assume that the mean-square error in the observables is inversely proportional to the number of uncorrelated conﬁgurations visited in a given amount of CPU time. And the number of independent conﬁgurations visited is a measure for the distance covered in conﬁguration space. This suggests a more manageable, albeit rather ad hoc, criterion to estimate the efﬁciency of a Monte Carlo sampling scheme: the sum of the squares of all accepted trial displacements divided by computing time. This quantity should be distinguished from the mean-squared displacement per unit of computing time, because the latter quantity goes to ¼ in the absence of diffusion (e.g., in a solid or a glass), whereas the former does not.

Using this criterion it is easy to show that for simulations of condensed

phases it is usually advisable to perform random displacements of one par-

ticle at a time (as we shall see later, the situation is different for correlated

displacements). To see why random single-particle moves are preferred,

econnersgidyefruancstyiostnemÍ ´roÆf

Æ spherical particles, µ. Typically, we expect

interacting through that a trial move will

a potential be rejected

if the potential energy of the system changes by much more than Ì. At

the same time, we try to make the Monte Carlo trial steps as large as is pos-

sible without having a very low acceptance. A displacement that would, on

average, give rise to an increase of the potential energy by Ì would still

have a reasonable acceptance. In the case of a single-particle trial move, we

7Still, we should stress that it is not worthwhile to spend a lot of time developing a fancy computational scheme that will be only marginally better than existing, simpler schemes, unless your program will run very often and speed is crucial.

3.3 Trial Moves

45

then have
¡Í

Í
Ö«

¶

¡Ö«

·

½ ¾

¾Í
Ö« Ö¬

·

¡Ö«¡Ö¬

·

¡

¡

¡

¼ · ´Íµ ¡Ö¾ · Ç´¡ µ

(3.3.2)

where the angle brackets denote averaging over the ensemble and the horizontal bar denotes averaging over random trial moves. The second deriva-
´ µ tive of Í has been absorbed into the function Í , the precise form of which does not concern us here. If we now equate ¡Í on the left-hand side of equation (3.3.2) to Ì, we ﬁnd the following expression for ¡Ö¾:

¡Ö¾ Ì ´Íµ

(3.3.3)

If we attempt to move Æ particles, one at a time, most of the computation
involved is spent on the evaluation of the change in potential energy. Assuming that we use a neighbor list or a similar time-saving device (see Appendix F), the total time spent on evaluating the potential energy change is
proportional to ÒÆ, where Ò is the average number of interaction partners
per molecule. The sum of the mean-squared displacements will be propor-
tional to Æ¡Ö¾ Æ Ì ´Íµ. Hence, the mean-squared displacement per unit of CPU time will be proportional to Ì ´Ò ´Íµµ. Now suppose that we
try to move all particles at once. The cost in CPU time will still be propor-
tional to ÒÆ. But, using the same reasoning as in equations (3.3.2) and (3.3.3),
we estimate that the sum of the mean-squared displacements is smaller by a
factor ½ Æ. Hence the total efﬁciency will be down by this same factor. This
simple argument explains why most simulators use single-particle, rather than collective trial moves. It is important to note that we have assumed that
a collective MC trial move consists of Æ independent trial displacements of
the particles. As will be discussed in section 14.2, efﬁcient collective MC moves can be constructed if the trial displacements of the individual particles are not chosen independently.
Next, consider the choice of the parameter ¡ which determines the size of the trial move. How large should ¡ be? If it is very large, it is likely
that the resulting conﬁguration will have a high energy and the trial move will probably be rejected. If it is very small, the change in potential energy is probably small and most moves will be accepted. In the literature, one often ﬁnds the mysterious statement that an acceptance of approximately 50% should be optimal. This statement is not necessarily true. The optimum acceptance ratio is the one that leads to the most efﬁcient sampling of conﬁguration space. If we express efﬁciency as mean-squared displacement per CPU time, it is easy to see that different Monte Carlo codes will have different optimal acceptance ratios. The reason is that it makes a crucial difference whether the amount of computing required to test whether a trial

46
<∆r2>

Chapter 3. Monte Carlo Simulations

Cost

continuous hard core

∆

∆

Figure 3.4: (left) Typical dependence of the mean-squared displacement of a
particle on the average size ¡ of the trial move. (right) Typical dependence of the computational cost of a trial move on the step-size ¡. For continuous
potentials, the cost is constant, while for hard-core potentials it decreases rapidly with the size of the trial move.

move is accepted depends on the magnitude of the move (see Figure 3.4). In the conventional Metropolis scheme, all continuous interactions have to be computed before a move can be accepted or rejected. Hence, for continuous potentials, the amount of computation does not depend on the size of a trial move. In contrast, for simulations of molecules with hard repulsive cores, a move can be rejected as soon as overlap with any neighbor is detected. In that case, a rejected move is cheaper than an accepted one, and hence the average computing time per trial move goes down as the step size is increased. As a result, the optimal acceptance ratio for hard-core systems is appreciably lower than for systems with continuous interactions. Exactly how much depends on the nature of the program, in particular on whether it is a scalar or a vector code (in the latter case, hard-core systems are treated much like continuous systems), on how the information about neighbor lists is stored, and even on the computational “cost” of random numbers and exponentiation. The consensus seems to be that for hard-core systems the optimum acceptance ratio is closer to 20 than to 50%, but this is just another rule of thumb that should be checked.8
A distinct disadvantage of the efﬁciency criterion discussed previously is that it does not allow us to detect whether the sampling of conﬁguration space is ergodic. To take a speciﬁc example, suppose that our system consists of a number of particles that are trapped in different potential energy min-
8In section 14.3.1, we show how, even in the case of continuous potentials, it is possible to reject trial moves before all interactions have been evaluated. With such a sampling scheme, the distinction between the sampling of hard-core and continuous potentials all but disappears.

3.3 Trial Moves

47

ima. Clearly, we can sample the vicinity of these minima quite well and still have totally inadequate sampling of the whole of the conﬁguration space. A criterion that would detect such nonergodicity has been proposed by Mountain and Thirumalai [56]. These authors consider the difference between the variance of the time average of the (potential) energy of all particles. Let us denote the time average of the energy of particle in time interval Ø by ´Øµ:

´Øµ

½

Ø
dØ ¼

´Ø ¼µ

Ø¼

And the average single-particle energy for this interval is

Æ

½

´Øµ

´Øµ

Æ

½

The variance of interest is

¾ ´Øµ

Æ ½ Æ
½

´Øµ ¹ ´Øµ ¾

½ If all particles sample the whole of conﬁguration space, ¾ ´Øµ will approach

zero as Ø

:

¾ ´Øµ ¾ ´¼µ

Ø

where is a measure for the characteristic time to obtain uncorrelated samples. However, if the system is nonergodic, as in a (spin) glass, will not decay to ¼. The work of Mountain and Thirumalai suggests that a good method for optimizing the efﬁciency of a Monte Carlo scheme is to minimize the product of and the computer time per trial move. Using this scheme, Mountain and Thirumalai concluded that, even for the Lennard-Jones system, a trial move acceptance of 50% is far from optimal. They found that an acceptance probability of 20% was twice as efﬁcient.
Of course, a scheme based on the energy ﬂuctuations of a particle is not very useful to monitor the rate of convergence of simulations of hard-core systems. But the essence of the method is not that one measures the energy but any quantity that is sensitive to the local environment of a particle. For instance, a robust criterion would look at the convergence of the timeaveraged Voronoi signature of a particle. Different environments yield different signatures. Only if every particle samples all environments will the variance of Voronoi signatures decay to ¼.
Of course, in some situations an efﬁciency criterion based on ergodicity is not useful. By construction, it cannot be used to optimize simulations of glasses. But also when studying interfaces (e.g., solid-liquid or liquid-vapor) the ergodicity criterion would suggest that every particle should have ample

48

Chapter 3. Monte Carlo Simulations

time to explore both coexisting phases. This is clearly unnecessary: ice can be in equilibrium with water, even though the time of equilibration is far too short to allow complete exchange of the molecules in the two phases.

3.3.2 Orientational Moves
If we are simulating molecules rather than atoms we must also generate trial moves that change the molecular orientation. As we discussed already, it almost requires an effort for generating translational trial moves with a distribution that does not satisfy the symmetry requirement of the underlying Markov chain. For rotational moves, the situation is very different. It is only too easy to introduce a systematic bias in the orientational distribution function of the molecules by using a nonsymmetrical orientational sampling scheme. Several different strategies to generate rotational displacements are discussed in [19]. Here we only mention one possible approach.

Rigid Linear Molecules
Æ Consider a system consisting of linear molecules. We specify the orien-
tation of the th molecule by a unit vector u . One possible procedure to change u by a small, random amount is the following. First, we generate a unit vector v with a random orientation. This is quite easy to achieve (see Algorithm 42). Next we multiply this random unit vector v by a scale factor ­. The magnitude of ­ determines the magnitude of the trial rotation. We now add ­v to u . Let us denote the resulting sum vector by t: t ­v · u . Note that t is not a unit vector. Finally, we normalize t, and the result is our trial orientation vector u ¼. We still have to ﬁx ­, which determines the acceptance probability for the orientational trial move. The optimum value of ­ is determined by essentially the same criteria as for translational moves. We have not yet indicated whether the translational and orientational trial moves should be performed simultaneously. Both procedures are acceptable. However, if rotation and translation correspond to separate moves, then the selection of the type of move should be probabilistic rather than deterministic.

Rigid Nonlinear Molecules

Only slightly more complex is the case of a nonlinear, rigid molecule. It is

conventional to describe the orientation of nonlinear molecules in terms of

the Eulerian angles ´

µ. However, for most simulations, use of these

angles is less convenient because all rotation operations should then be ex-

pressed in terms of trigonometric functions, and these are computationally

expensive. It is usually better to express the orientation of such a molecule

3.3 Trial Moves

49

in terms of quaternion parameters (for a discussion of quaternions in the context of computer simulation, see [19]). The rotation of a rigid body can be speciﬁed by a quaternion of unit norm . Such a quaternion may be thought of as a unit vector in four-dimensional space:

´Õ¼ Õ½ Õ¾ Õ¿µ with Õ¾¼ · Õ¾½ · Õ¾¾ · Õ¾¿ ½

(3.3.4)

There is a one-to-one correspondence between the quaternion components
Õ« and the Eulerian angles,

Õ¼

·
cos ¾ cos ¾

Õ½

sin ¾ cos

¹
¾

Õ¾

¹
sin ¾ sin ¾

Õ¿

·
cos ¾ sin ¾

(3.3.5)

and the rotation matrix R, which describes the rotation of the molecule-ﬁxed

vector in the laboratory frame, is given by (see, e.g., [57])

R

¼ Õ¾¼ · Õ¾½ ¹ Õ¾¾ ¹ Õ¾¿
¾´Õ½Õ¾ · Õ¼Õ¿µ ¾´Õ½Õ¿ ¹ Õ¼Õ¾µ

¾´Õ½Õ¾ ¹ Õ¼Õ¿µ Õ¾¼ ¹ Õ¾½ · Õ¾¾ ¹ Õ¾¿ ¾´Õ¾Õ¿ · Õ¼Õ½µ

¾´Õ½Õ¿ · Õ¼Õ¾µ ½
¾´Õ¾Õ¿ ¹ Õ¼Õ½µ Õ¾¼ ¹ Õ¾½ ¹ Õ¾¾ · Õ¾¿

(3.3.6)

To generate trial rotations of nonlinear, rigid bodies, we must rotate the vec-

tor ´Õ¼ Õ½ Õ¾ Õ¿µ on the four-dimensional ( ) unit sphere. The procedure just described for the rotation of a ¿ unit vector is easily generalized to .

An efﬁcient method for generating random vectors uniformly on the unit

sphere has been suggested by Vesely [57].

Nonrigid Molecules
If the molecules under consideration are not rigid then we must also consider Monte Carlo trial moves that change the internal degrees of freedom of a molecule. In practice, it makes an important difference whether we have frozen out some of the internal degrees of freedom of a molecule by imposing rigid constraints on, say, bond lengths and possibly even some bond angles. If not, the situation is relatively simple: we can carry out normal trial moves on the Cartesian coordinates of the individual atoms in the molecule (in addition to center-of-mass moves). If some of the atoms are strongly bound, it is advisable to carry out small trial moves on those particles (no rule forbids the use of trial moves of different size for different

50

Chapter 3. Monte Carlo Simulations

atoms, as long as the moves for one particular atom are always sampled

from the same distribution).

However, when the bonds between different atoms become very stiff,

this procedure does not sample conformational changes of the molecule efﬁ-

ciently. In Molecular Dynamics simulations it is common practice to replace

very stiff intramolecular interactions with rigid constraints (see Chapter 15).

For Monte Carlo simulations this is also possible. In fact, elegant techniques

have been developed for this purpose [58]. However, the corresponding MD

techniques [59] are so much easier to use, in particular for large molecules,

that we cannot recommend the use of the Monte Carlo technique for any but

the smallest ﬂexible molecules with internal constraints.

To understand why Monte Carlo simulations of ﬂexible molecules with

a number of stiff (or even rigid) bonds (or bond angles) can become compli-

cated, let us a function

r´erÆtuµr:n

to

the

original

expression

(3.1.2)

for

a

thermal

average

of

Ê

dpÊÆdpdÆrÆdrÆ´reÆxµpex¹p¬¹À¬´pÀÆ´prÆÆ

rÆ
µ

µ

Õ MeraolInifzteewdCecaoarloroerdsdainemaalptienlsignqgwÆn,iotwht hoﬂenerxethibelemCmaaryotebleseica,unfloecrso,ionirstdtaiisnnaccetoe,nsavrebÆnoinbedunttleotnongtpthheerofgoreranmn-

internal angle. We must now express the Hamiltonian in equation (3.1.2) in

terms of these generalized coordinates and their conjugate momenta. This

½ ¾ ÑÖ is done most conveniently by ﬁrst considering the
where Ã is the kinetic energy of the system (Ã

LÈag´rangµ ian˙¾Ä)

Ã ¹ Í, and Í the

potential energy. When we transform from Cartesian coordinates r to gener-

alized coordinates q, Ä changes to

Ä

Æ ½

½ ¾

Ñ

r
Õ«

Õ Õ Õ r ˙ « ˙ ¬ ¹ Í´qÆµ ¬

½¾ q˙ ¡ G ¡ q˙ ¹ Í´qÆµ

(3.3.7)

In the second line of equation (3.3.7) we have deﬁned the matrix G. The momenta conjugate to qÆ are easily derived using

Ô«

Ä
Õ˙ «

Ô Õ This yields « G«¬ ˙ ¬. We can now write down the Hamiltonian À in
terms of the generalized coordinates and conjugate momenta:

½¾ À´p qµ

p ¡ G¹½ ¡ p · Í ´qÆµ

(3.3.8)

3.4 Applications

51

If we now insert this form of the Hamiltonian into equation (3.1.2), and carry

out the (Gaussian) integration over the momenta, we ﬁnd that

Æ Æ Æ Æ Ê
¬ ¬ ¬ ¾ dq

exp ¹

Ê

Í

´qÊ

q µ ´ µ dqÆdpÆ

dp exp´¹

exp´¹
Àµ

p ¡ G¹½ ¡ p

µ

¬ ¬ Ê

dqÆÊedxqpÆ¹dpÍÆ´eqxÆpµ´¹

´qÆ µ
Àµ

G

¾½

(3.3.9)

The problem with equation (3.3.9) is the term G ¾½ . Although the determinant G can be computed fairly easily for small ﬂexible molecules, its evaluation can become quite an unpleasant task in the case of larger molecules.
Thus far we have considered the effect of introducing generalized coordinates only on the form of the expression for thermal averages. If we are considering a situation where some of the generalized coordinates are actually constrained to have a ﬁxed value, then the picture changes again, because such hard constraints are imposed at the level of the Lagrangian equations of motion. Hard constraints therefore lead to a different form for the Hamiltonian in equation (3.3.8) and to another determinant in equation (3.3.9). Again, all this can be taken into account in the Monte Carlo sampling (see [58]). An example of such a Monte Carlo scheme is the concerted rotation algorithm that has been developed by Theodorou and co-workers [60] to simulate polymer melts and glasses (see section 13.4.4). The idea of this algorithm is to select a set of adjacent skeletal bonds in a chain (up to seven bonds). These bonds are given a collective rotation while the rest of the chain is unaffected. By comparison, Molecular Dynamics simulations of ﬂexible molecules with hard constraints have the advantage that these constraints enter directly into the equations of motion (see [59]). The distinction between Molecular Dynamics and Monte Carlo, however, is more apparent than real, since it is possible to use MD techniques to generate collective Monte Carlo moves (see section 14.2). In Chapter 13, we shall discuss other Monte Carlo sampling schemes that are particularly suited for ﬂexible molecules.

3.4 Applications
ÆÎÌ In this section we give several case studies using the basic Monte Carlo
algorithm.
Case Study 1 (Equation of State of the Lennard-Jones Fluid) One of the more important applications of molecular simulation is to compute the phase diagram of a given model system. In fact, in Chapter 8 several numerical techniques that have been developed speciﬁcally to study

52

Chapter 3. Monte Carlo Simulations

phase transitions will be discussed. It may not be immediately obvious to

the reader, however, that there is any need for the sophisticated numerical

schemes presented in Chapter 8. In this Case Study, we illustrate some of

the problems that occur when we use standard Monte Carlo simulation to

determine a phase diagram. As an example, we focus on the vapor-liquid

curve of the Lennard-Jones ﬂuid. Of course, as was already mentioned in

section 3.2.2, the phase behavior is quite sensitive to the detailed form of the

intermolecular potential that is used. In this Case Study, we approximate the

full Lennard-Jones potential as follows:

Ù´Öµ

¬ Ùlj´Öµ Ö
¼Ö

Ö Ö

where the cutoff radius Ö is set to half the box length. The contribution of
the particles beyond this cutoff is estimated with the usual tail corrections;
that is, for the energy

Ùtail ¿

½½

½¿

¿Ö ¹Ö

and for the pressure

Ètail

½ ¿

¾¾ ½ ¿Ö

½¿ ¹Ö

The equation of state of the Lennard-Jones ﬂuid has been investigated by many groups using Molecular Dynamics or Monte Carlo simulations starting with the work of Wood and Parker [49]. A systematic study of the equation of state of the Lennard-Jones ﬂuid was reported by Verlet [13]. Subsequently, many more studies have been published. In 1979, the data available at that time were compiled by Nicolas et al. [61] into an accurate equation of state. This equation has been reﬁtted by Johnson et al. [62] in the light of more recent data. In the present study we compare our numerical results with the equation of state by Johnson et al.
We performed several simulations using Algorithms 1 and 2. During the simulations we determined the energy per particle and the pressure. The pressure was calculated using the virial

È

vir
¬· Î

(3.4.1)

where the virial is deﬁned by
½ vir ¿

f´r µ ¡ r

(3.4.2)

3.4 Applications

53

P P

10.0 8.0 6.0 4.0 2.0 0.0 0.0 0.2 0.4 0.6 0.8 1.0 ρ

0.8
0.4
0.0
-0.4 0.0 0.2 0.4 0.6 0.8 ρ

Figure 3.5: Equation of state of the Lennard-Jones ﬂuid. (left) Isotherm at
Ì ¾ ¼. (right) Isotherm below the critical temperature (Ì ¼ ); the hori-
zontal line is the saturated vapor pressure and the ﬁlled circles indicate the
densities of the coexisting vapor and liquid phases. The solid curve repre-
sents the equation of state of Johnson et al. [62] and the circles are the results
of the simulations (Æ ¼¼). The errors are smaller than the symbol size.

where f´r µ is the intermolecular force. Figure 3.5 (left) compares the pressure as obtained from a simulation above the critical temperature with the equation of state of Johnson et al. [62]. The agreement is excellent (as is to be expected).
Figure 3.5 (right) shows a typical isotherm below the critical temperature. If we cool the system below the critical temperature, we should expect to observe vapor-liquid coexistence. However, conventional Monte Carlo or Molecular Dynamics simulations of small model systems are not suited to study the coexistence between two phases. Using the Johnson equation of state, we predict how the pressure of a macroscopic Lennard-Jones system would behave in the two-phase region (see Figure 3.5). For densities inside the coexistence region the pressure is expected to be constant and equal to the saturated vapor pressure. If we now perform a Monte Carlo simulation of a ﬁnite system (500 LJ particles), we ﬁnd that the computed pressure is not at all constant in the coexistence region (see Figure 3.5). In fact we observe that, over a wide density range, the simulated system is metastable and may even have a negative pressure. The reason is that, in a ﬁnite system, a relatively important free-energy cost is associated with the creation of a liquid-vapor interface. So much so that, for sufﬁciently small systems, it is favorable for the system not to phase separate at all [63]. Clearly these problems will be most severe for small systems and in cases where the in-
terfacial free energy is large. For this reason, standard ÆÎÌ-simulations are
not recommended to determine the vapor-liquid coexistence curve or, for that matter, any strong ﬁrst-order phase transition.

54

Chapter 3. Monte Carlo Simulations

To determine the liquid-vapor coexistence curve we should determine the equation of state for a large number of state points outside the coexistence region. These data can then be ﬁtted to an analytical equation of state. With this equation of state we can determine the vapor-liquid curve (this is exactly the procedure used by Nicolas et al. [61] and Johnson et al. [62]).
Of course, if we simulate a system consisting of a very large number of particles, it is possible to simulate a liquid phase in coexistence with its vapor. However, such simulations are quite time consuming, because it takes a long time to equilibrate a two-phase system.
Case Study 2 (Importance of Detailed Balance) For a Monte Carlo simulation to sample points in conﬁguration space according to their correct Boltzmann weight, it is sufﬁcient, but not necessary, to impose the detailed-balance condition on the sampling algorithm. Of course, as the condition of detailed balance is stronger than strictly necessary, it is not excluded that correct sampling schemes exist that violate detailed balance. However, unless one can actually prove that a non-detailed-balance scheme yields the correct distribution, the use of such schemes is strongly to be discouraged. Even seemingly reasonable schemes may give rise to serious, systematic errors.
Æ Here we give an example of such a scheme. Consider an ordinary ,Î,Ì
move; a new position is generated by giving a randomly selected particle, say , a random displacement:
ÜÒ´ µ ÜÓ´ µ · ¡Ü´Ranf ¹ ¼ µ
where ¡Ü is twice the maximum displacement. We now make a small error
and generate a new position using
ÜÒ´ µ ÜÓ´ µ · ¡Ü´Ranf ¹ ¼ ¼µ wrong
We give the particles only a positive displacement. With such a move detailed balance is violated, since the reverse move — putting the particle back
at ÜÓ — is not possible.
For the Lennard-Jones ﬂuid we can use the program of Case Study 1 to compare the two sampling schemes. The results of these simulations are shown in Figure 3.6. Note that, at ﬁrst sight, the results of the incorrect scheme look reasonable; in fact, at low densities the results of the two schemes do not show signiﬁcant differences. But at high densities the wrong scheme overestimates the pressure. It is important to note that the incorrect scheme leads to a systematic error that does not disappear when we perform longer simulations.
This example illustrates that one can generate numerical results that look reasonable, even with an incorrect sampling scheme. For this reason, it is

3.4 Applications

55

1.0

10.0

P P

0.5

5.0

0.0 0.0

0.2

0.4

ρ

0.0

0.6

0.8

1.0

ρ

Figure 3.6: Equation of state of the Lennard-Jones ﬂuid (Ì ¾ ¼); compari-
son of a displacement scheme that obeys detailed balance (circles) and one that does not (squares). Both simulations have been performed with 500 particles. The solid curve is the equation of state of Johnson et al. [62]. The ﬁgure at the left corresponds to the low-pressure regime. The high-pressure regime is shown in the right-hand ﬁgure.

important always to compare the results obtained with a new Monte Carlo program with known numerical results or, better still, with exact results that may be known in some limiting case (dilute vapor, dense solid, etc.).
In the present example, the error due to the neglect of detailed balance is quite obvious. In many cases, the effects are less clear. The most common source of non-detailed-balance sampling schemes is the following: in many programs, we can choose from a repertoire of trial moves (e.g., translation, rotation, volume changes). It is recommended that these trial moves are not carried out in ﬁxed order, because then the reverse sequence is impossible and detailed balance is no longer satisﬁed.9
In practice one often does not know a priori the optimal maximum displacement in a Monte Carlo simulation. A practical solution is to adjust during the simulation the maximum displacement in such a way that the optimum acceptance probability is obtained. The ideal situation is to determine this optimum during the equilibration. However, if one would keep adjusting the maximum step-size during a production run, then one would violate detailed balance [65]. For example, if from one move to the next, the maximum displacement is decreased, then the a priori probability for a particle to return to its previous position could be zero. Hence, if one would change the maximum displacement after every Monte Carlo step serious errors are to be expected. Of course, if one changes the maximum displacement only a few
9It has been shown [64] that in this case the detailed-balance condition is indeed sufﬁcient but not necessary to maintain equilibrium.

56

Chapter 3. Monte Carlo Simulations

times during the simulation, then the error will be negligible. Yet, it is better to stay on the safe side and never change the maximum displacement during the projection run.

Case Study 3 ( hy Count the Old Con guration Again ) A somewhat counterintuitive feature of the Metropolis sampling scheme is that, if a trial move is rejected, we should once again count the contributions of the old conﬁguration to the average that we are computing (see acceptance rule (3.1.18)). The aim of this Case Study is to show that this recounting is really essential. In the Metropolis scheme the acceptance rule for a
move from Ó to Ò is

acc´Ó Òµ

Í exp ¹¬ Í´Òµ ¹ Í´Óµ Í´Òµ ´Óµ

½

Í´Òµ Í´Óµ

These acceptance rules lead to a transition probability

´Ó Òµ

Í Í Í exp ¹¬ ´Òµ ¹ ´Óµ

´Òµ

½

Í ´Òµ

Í ´Óµ Í ´Óµ

Note that this transition probability must be normalized:

´Ó Òµ ½ Ò
From this normalization it follows that the probability that we accept the old conﬁguration again is by deﬁnition

´Ó Óµ ½ ¹

´Ó Òµ

ÒÓ

This last equation implies that we should count the contribution of the old conﬁguration again.
It is instructive to use the Lennard-Jones program from Case Study 1 to investigate numerically the error that is made when we only include accepted conﬁgurations in our averaging. In essence, this means that in Algorithm 2 we continue attempting to displace the selected particle until a trial move has been accepted.10 In Figure 3.7 we compare the results of the correct scheme with those obtained by the scheme in which we continue to displace a particle until a move is accepted. Again the results look reasonable, but the ﬁgure shows that large, systematic errors are being made.
10It is easy to see that this approach leads to the wrong answer if we try to compute the average energy of a two-level system with energy levels ¼ and ½. If we include only accepted
trial moves in our averaging, we would ﬁnd that =´ ¼ · ½µ ¾, independent of temperature.

3.4 Applications

57

1.0

10.0

P P

0.5

5.0

0.0

0.0

0.0

0.2

0.4

0.6

0.8

1.0

ρ

ρ

Figure 3.7: Equation of state of the Lennard-Jones ﬂuid (Ì ¾ ¼); compar-
ison of a scheme in which particles are displaced until a move is accepted (squares) with the conventional scheme (circles). Both simulations have been performed with 108 particles. The solid curve is the equation of state of Johnson et al. [62]. The left ﬁgure is at low pressure and the right one at high pressure.

One of the important disadvantages of the Monte Carlo scheme is that it does not reproduce the natural dynamics of the particles in the system. However, sometimes this limitation of the method can be made to work to our advantage. In Example 1 we show how the equilibration of a Monte Carlo simulation can be speeded up by many orders of magnitude through the use of unphysical trial moves.

Example 1 (Mixture of Hard Disks)
¹ In a Molecular Dynamics simulation of, for instance, a binary ( ) mixture

of hard disks (see Figure 3.8), the efﬁciency with which conﬁguration space

is sampled is greatly reduced by the fact that cay very slowly (typically the relaxation time

concentra¾ti,ownhﬂeurcetuationiss

dethe

mutual diffusion coefﬁcient and is the wavelength of the concentration ﬂuc-

tuation). This implies that very long runs are needed to ensure equilibration

of the local composition of the mixture. In solids, equilibration may not take

place at all (even on time scales accessible in nature). In contrast, in a Monte

Carlo simulation, it is permissible to carry out trial moves that swap the iden-

tities of two particles of species and . Such moves, even if they have only

a moderate rate of acceptance (a few percent will do), greatly speed up the

sampling of concentration ﬂuctuations.

58

Chapter 3. Monte Carlo Simulations

Figure 3.8: A mixture of hard disks, where the identities of two particles are swapped.

3.5 Questions and Exercises

Question 7 (Reduced Units) Typical sets of Lennard-Jones parameters for argon

and krypton are Ö ¿ ½ ß, ¯ Ö

½½ K and ÃÖ ¿ ¿ ß, ¯ÃÖ

½ ¼ K [19].

1. At the reduced temperature Ì£ ¾ ¼, what is the temperature in kelvin of
argon and krypton?

2. A typical time step for MD is ¡Ø£ ¼ ¼¼½. What is this in SI units for argon

and krypton?

3. If we simulate argon at Ì ¾ K and density

¾¼¼¼ kg/m¿ with a

Lennard-Jones potential, for which conditions of krypton can we use the same

data? If we assume ideal gas behavior, compute the pressure in reduced and

normal units.

4. List the main reasons to use reduced units.

Question 8 (Heat Capacity) Heat capacity can also be calculated from uctuations in the total energy in the canonical ensemble:

Ú

ªÍ¾« ¹ Í ¾
Ì¾

(3.5.1)

1. Derive this equation.
2. In a MC ÆÎÌ simulation, one does not calculate uctuations in the total
energy but in the potential energy. Is it then still possible to calculate the heat capacity? Explain.
3. Heat capacity can be also calculated from differentiating the total energy of a system with respect to temperature. Discuss the advantages or disadvantages of this approach.

3.5 Questions and Exercises

59

Question 9 (A Ne Potential) On the planet Krypton, the pair potential between two Gaia atoms is given by the Lennard-Jones ½¼- potential

Í´Öµ

½¼
¯ Ö ¹Ö

Kryptonians are notoriously lazy and it is therefore up to you to derive the tail corrections for the energy, pressure, and chemical potential. If we use this potential in an MD simulation in the truncated and shifted form we still have a discontinuity in the force. Why? If you compare this potential with the Lennard-Jones potential, will there be any difference in ef ciency of the simulation? (Hint: there are two effects!)

Exercise 6 (Calculation of ) Consider a circle of diameter surrounded by a square of length Ð (Ð ). Random coordinates are generated within the square. The value of can be calculated from the fraction of points that fall within the circle.
1. How can be calculated from the fraction of points that fall in the circle? Remark: the “exact” value of can be computed numerically using
¢ arctan ´½µ.
2. Complete the small Monte Carlo program to calculate using this method.
3. How does the accuracy of the result depend on the ratio Ð and the number of generated coordinates? Derive a formula to calculate the relative standard deviation of the estimate of .
4. Why is this not a very efﬁcient method for computing accurately?

Exercise 7 (The Photon Gas) The average occupancy number of state of the photon gas, Ò , can be calculated analytically; see equation (2.3.5). It is possible to estimate this quantity using a Monte Carlo scheme. In this exercise, we will use the following procedure to calculate Ò :
(i) Start with an arbitrary Ò .
(ii) Decide at random to perform a trial move to increase or decrease Ò by ½.
(iii) Accept the trial move with probability

acc ´o nµ min ´½ exp ¹¬ ´Í ´nµ ¹ Í ´oµµ µ

Of course, Ò cannot become negative!

1. Does this scheme obey detailed balance when Ò ¼?

60

Chapter 3. Monte Carlo Simulations

2. Is the algorithm still correct when trial moves are performed that change
Ò with a random integer from the interval ¹ ? What happens when
only trial moves are performed that change Ò with either ¹¿ or ·¿?
3. Assume that Æ ½ and ¯ ¯. Write a small Monte Carlo program to calculate Ò as a function of ¬¯. Compare your result with the analytical solution.
4. Modify the program in such a way that the averages are not updated when a trail move is rejected. Why does this lead to erroneous results? At which values of ¬ does this error become more pronounced?
5. Modify the program in such a way that the distribution of Ò is calculated as well. Compare this distribution with the analytical expression.

Exercise 8 (Monte Carlo Simulation of a Lennard-Jones System) In this exercise, we study a 3D Lennard-Jones system. See also Case Study 1.

1. In the code that you can ﬁnd on the book’s website, the pressure of the system is not calculated. Modify the code in such a way that the average pressure can be calculated. You will only have to make some changes in the subroutine ener.f.

2. Perform a simulation at Ì ¾ ¼ and at various densities. Up to what

density is the ideal gas law

¬Ô

(3.5.2)

a good approximation?

3. The program produces a sequence of snapshots of the state of the system. Try to visualize these snapshots using, for example, the program MOLMOL.

4. For the heat capacity at constant volume one can derive

ª« Í¾ ¹

Í¾

Ú

Ì¾

in which Í is the total energy of the system. Derive a formula for the dimensionless heat capacity. Modify the program (only in mc nvt.f) in such a way that Ú is calculated.
5. Instead of performing trial moves in which one particle at a time is displaced, one can make trial moves in which all particles are displaced. Compare the maximum displacements of these moves when 50% of all displacements are accepted.
6. Instead of using a uniformly distributed displacement, one can also use a Gaussian displacement. Does this increase the efﬁciency of the simulation?

3.5 Questions and Exercises

61

Exercise 9 (Scaling as a Monte Carlo Move) Consider a system in which the energy is a function of one variable (Ü) only,

exp ¹¬Í ´Üµ

´Üµ ´½ ¹ Üµ

in which ´Üµ is the Heaviside step function: ´Ü ¼µ ¼ and ´Ü ¼µ ½. We wish to calculate the distribution of Ü in the canonical ensemble. We will consider two possible algorithms (we will use Æ ¼):
(i) Generate a random change in Ü between ¹ÆÆ . Accept or reject the new Ü according to its energy.
(ii) Generate a random number between ½½ · Æ . With a probability of ¼ , invert the value thus obtained. The new value of Ü is obtained by multiplying Ü with .

1. Derive the correct acceptance/rejection rules for both schemes.
2. Complete the computer code to calculate the probability density of Ü. The program writes this distribution to distri.dat.
3. What happens when the acceptance rule of method (i) is used in the algorithm of method (ii)?

This Page Intentionally Left Blank

Chapter 4
Molecular Dynamics Simulations

Molecular Dynamics simulation is a technique for computing the equilib-

rium and transport properties of a classical many-body system. In this con-

text, the word classical means that the nuclear motion of the constituent par-

ticles obeys the laws of classical mechanics. This is an excellent approxima-

tion for a wide range of materials. Only when we consider the translational

or rotational motion of light atoms or molecules (He, H¾, D¾) or vibrational

motion with a frequency such that

Ì should we worry about quan-

tum effects.

Of course, our discussion of this vast subject is necessarily incomplete.

Other aspects of the Molecular Dynamics techniques can be found in [19,

39–41].

4.1 Molecular Dynamics: The Idea
Molecular Dynamics simulations are in many respects very similar to real experiments. When we perform a real experiment, we proceed as follows. We prepare a sample of the material that we wish to study. We connect this sample to a measuring instrument (e.g., a thermometer, manometer, or viscosimeter), and we measure the property of interest during a certain time interval. If our measurements are subject to statistical noise (as most measurements are), then the longer we average, the more accurate our measurement becomes. In a Molecular Dynamics simulation, we follow exactly the same approach. First, we prepare a sample: we select a model system con-
sisting of Æ particles and we solve Newton’s equations of motion for this
system until the properties of the system no longer change with time (we

64

Chapter 4. Molecular Dynamics Simulations

equilibrate the system). After equilibration, we perform the actual measurement. In fact, some of the most common mistakes that can be made when performing a computer experiment are very similar to the mistakes that can be made in real experiments (e.g., the sample is not prepared correctly, the measurement is too short, the system undergoes an irreversible change during the experiment, or we do not measure what we think).
To measure an observable quantity in a Molecular Dynamics simulation, we must ﬁrst of all be able to express this observable as a function of the positions and momenta of the particles in the system. For instance, a convenient deﬁnition of the temperature in a (classical) many-body system makes use of the equipartition of energy over all degrees of freedom that enter quadratically in the Hamiltonian of the system. In particular for the average kinetic energy per degree of freedom, we have

½ ¾

ÑÚ¾«

½ ¾

Ì

(4.1.1)

In a simulation, we use this equation as an operational deﬁnition of the temperature. In practice, we would measure the total kinetic energy of the sys-
¹ tem and divide this by the number of degrees of freedom Æ (= ¿Æ ¿ for
a system of Æ particles with ﬁxed total momentum1). As the total kinetic energy of a system ﬂuctuates, so does the instantaneous temperature:

Ì´Øµ

Æ Ñ Ú¾´Øµ
½Æ

(4.1.2)
Ô

iTshteypreiclaatlilvyeoﬂnuthcteuoartdioenrsoifn½¼th¾e–½t¼e¿m, ptheerasttuartiestwicialll

be of order ½ ﬂuctuations in

Æ the

. As Æ temper-

ature are on the order of 5–10%. To get an accurate estimate of the tempera-

ture, one should average over many ﬂuctuations.

4.2 Molecular Dynamics: A Program
The best introduction to Molecular Dynamics simulations is to consider a simple program. The program we consider is kept as simple as possible to illustrate a number of important features of Molecular Dynamics simulations.
The program is constructed as follows:
1. We read in the parameters that specify the conditions of the run (e.g., initial temperature, number of particles, density, time step).
1Actually, if we deﬁne the temperature of a microcanonical ensemble through ´ Ìµ¹½ =
´ ln ª µ, then we ﬁnd that, for a -dimensional system of Æ atoms with ﬁxed total momentum, Ì is equal to ¾ ´ ´Æ ¹ ½µ ¹ ¾µ.

4.2 Molecular Dynamics: A Program

65

Algorithm 3 (A Simple Molecular Dynamics Program)

program md
call init t=0 do while (t.lt.tmax)
call force(f,en) call integrate(f,en) t=t+delt call sample enddo stop end

simple MD program
initialization
MD loop determine the forces integrate equations of motion
sample averages

Comment to this algorithm:
1. Subroutines init, force, integrate, and sample will be described in Algorithms 4, 5, and 6, respectively. Subroutine sample is used to calculate averages like pressure or temperature.

2. We initialize the system (i.e., we select initial positions and velocities).
3. We compute the forces on all particles.
4. We integrate Newton’s equations of motion. This step and the previous one make up the core of the simulation. They are repeated until we have computed the time evolution of the system for the desired length of time.
5. After completion of the central loop, we compute and print the averages of measured quantities, and stop.
Algorithm 3 is a short pseudo-algorithm that carries out a Molecular Dynamics simulation for a simple atomic system. We discuss the different operations in the program in more detail.
4.2.1 Initiali ation
To start the simulation, we should assign initial positions and velocities to all particles in the system. The particle positions should be chosen compatible with the structure that we are aiming to simulate. In any event, the particles should not be positioned at positions that result in an appreciable overlap of the atomic or molecular cores. Often this is achieved by initially placing

66

Chapter 4. Molecular Dynamics Simulations

Algorithm 4 (Initiali ation of a Molecular Dynamics Program)

subroutine init sumv=0 sumv2=0 do i=1,npart
x(i)=lattice pos(i) v(i)=(ranf()-0.5) sumv=sumv+v(i) sumv2=sumv2+v(i)**2 enddo sumv=sumv/npart sumv2=sumv2/npart fs=sqrt(3*temp/sumv2) do i=1,npart v(i)=(v(i)-sumv)*fs xm(i)=x(i)-v(i)*dt enddo return end

initialization of MD program
place the particles on a lattice give random velocities velocity center of mass kinetic energy
velocity center of mass mean-squared velocity scale factor of the velocities set desired kinetic energy and set velocity center of mass to zero position previous time step

Comments to this algorithm:
1. Function lattice pos gives the coordinates of lattice position and ranf() gives a uniformly distributed random number. We do not use a Maxwell-Boltzmann distribution for the velocities; on equilibration it will become a Maxwell-Boltzmann distribution.
2. In computing the number of degrees of freedom, we assume a three-dimensional system (in fact, we approximate Æ by ¿Æ).

the particles on a cubic lattice, as described in section 3.2.2 in the context of Monte Carlo simulations.

In the present case (Algorithm 4), we have chosen to start our run from

a simple cubic lattice. Assume that the values of the density and initial tem-

perature are chosen such that the simple cubic lattice is mechanically un-

stable and melts rapidly. First, we put each particle on its lattice site and

then we attribute to each velocity component of every particle a value that

is drawn from a uniform distribution in the interval ¹¼¼

. This initial

velocity distribution is Maxwellian neither in shape nor even in width. Sub-

sequently, we shift all velocities, such that the total momentum is zero and

we scale the resulting velocities to adjust the mean kinetic energy to the de-

4.2 Molecular Dynamics: A Program

67

sired value. We know that, in thermal equilibrium, the following relation

should hold:

ªÚ¾««

ÌÑ

(4.2.1)

where Ú« is the « component of the velocity of a given particle. We can use
this relation to deﬁne an instantaneous temperature at time Ø Ì´Øµ:

Ì ´Øµ

N ÑÚ¾« ´Øµ ½Æ

(4.2.2)

Clearly, we can adjust the instantaneous temperature Ì´Øµ to sired temperature Ì by scaling all velocities with a factor ´Ì

mÌ´aØtµcµh½

t¾h. eTdheis-

initial setting of the temperature is not particularly critical, as the tempera-

ture will change anyway during equilibration.

As will appear later, we do not really use the velocities themselves in

our algorithm to solve Newton’s equations of motion. Rather, we use the

positions of all particles at the present (x) and previous (xm) time steps,

combined with our knowledge of the force (f) acting on the particles, to

predict the positions at the next time step. When we start the simulation,

we must bootstrap this procedure by generating approximate previous po-

sitions. Without much consideration for any law of mechanics but the con-

servation of linear momentum, we approximate x for a particle in a direc-

tion by xm(i) = x(i) - v(i)*dt. Of course, we could make a better

estimate of the true previous position of each particle. But as we are only

bootstrapping the simulation, we do not worry about such subtleties.

4.2.2 The Force Calculation

What comes next is the most time-consuming part of almost all Molecular

Dynamics simulations: the calculation of the force acting on every particle.

If we consider a model system with pairwise additive interactions (as we

do in the present case), we have to consider the contribution to the force on

particle due to all its neighbors. If we consider only the interaction between

a particle and the nearest image of another particle, this implies that, for a
¢ system of Æ particles, we must evaluate Æ ´Æ ¹ ½µ ¾ pair distances.

the

TfohricseismspcalileesstahsatÆ, i¾f.wTeheurseeenxoisttriecfkﬁsc,itehnet

time needed for the evaluation of techniques to speed up the eval-

uation puting

otifmboetshcsahleosrta-sraÆng, eraatnhderlothnagn-rÆan¾g.eIfnorAcpespiennsduixchFa,

way that the we describe

comsome

of the more common techniques to speed up the simulations. Although the

examples in this Appendix apply to Monte Carlo simulations, the same tech-

niques can also be used in a Molecular Dynamics simulation. However, in

the present, simple example (see Algorithm 5) we will not attempt to make

68

Chapter 4. Molecular Dynamics Simulations

Algorithm 5 (Calculation of the Forces)

subroutine force(f,en) en=0 do i=1,npart
f(i)=0 enddo do i=1,npart-1
do j=i+1,npart xr=x(i)-x(j) xr=xr-box*nint(xr/box) r2=xr**2 if (r2.lt.rc2) then r2i=1/r2 r6i=r2i**3 ff=48*r2i*r6i*(r6i-0.5) f(i)=f(i)+ff*xr f(j)=f(j)-ff*xr en=en+4*r6i*(r6i-1)-ecut endif
enddo enddo return end

determine the force and energy set forces to zero
loop over all pairs periodic boundary conditions test cutoff
Lennard-Jones potential update force update energy

Comments to this algorithm:

1. For ef ciency reasons the factors 4 and 48 are usually taken out of the force loop and taken into account at the end of the calculation for the energy.

2. The term ecut is the value of the potential at Ö

potential, we have

ecut

¹ ½ ½
Ö½¾ Ö

Ö ; for the Lennard-Jones

the program particularly efﬁcient and we shall, in fact, consider all possible pairs of particles explicitly.
We ﬁrst compute the current distance in the Ü, Ý, and Þ directions between each pair of particles i and j. These distances are indicated by xr. As in the Monte Carlo case, we use periodic boundary conditions (see section 3.2.2). In the present example, we use a cutoff at a distance Ö in the explicit calculation of intermolecular interactions, where Ö is chosen to be less than half the diameter of the periodic box. In that case we can always limit the evaluation

4.2 Molecular Dynamics: A Program

69

of intermolecular interactions between i and j to the interaction between i and the nearest periodic image of j.
In the present case, the diameter of the periodic box is denoted by box. If we use simple cubic periodic boundary conditions, the distance in any direction between i and the nearest image of j should always be less (in absolute value) than box/2. A compact way to compute the distance between i and the nearest periodic image of j uses the nearest integer function (nint(x) in FORTRAN). The nint function simply rounds a real number to the nearest integer.2 Starting with the Ü-distance (say) between i and any periodic image of j, xr, we compute the Ü-distance between i and the nearest image of j as xr=xr-box*nint(xr/box). Having thus computed all Cartesian components of r , the vector distance between i and the nearest image of j, we compute Ö¾ (denoted by r2 in the program). Next we test if Ö¾ is
less than Ö¾, the square of the cutoff radius. If not, we immediately skip to the next value of j. It perhaps is worth emphasizing that we do not compute Ö itself, because this would be both unnecessary and expensive (as it would involve the evaluation of a square root).
If a given pair of particles is close enough to interact, we must compute the force between these particles, and the contribution to the potential energy. Suppose that we wish to compute the Ü-component of the force

Ü´Öµ

Ù´Öµ ¹Ü

Ü Ù´Öµ

¹Ö

Ö

For a Lennard-Jones system (in reduced units),

Ü´Öµ

Ü Ö¾

½ Ö½¾

¹

¼

½ Ö

4.2.3 Integrating the Equations of Motion

Now that we have computed all forces between the particles, we can integrate Newton’s equations of motion. Algorithms have been designed to do this. Some of these will be discussed in a bit more detail. In the program (Algorithm 6), we have used the so-called Verlet algorithm. This algorithm is not only one of the simplest, but also usually the best.
To derive it, we start with a Taylor expansion of the coordinate of a particle, around time Ø,

Ç Ö´Ø · ¡Øµ

Ö´Øµ · Ú´Øµ¡Ø

·

´Øµ ¾Ñ

¡Ø¾

·

¡Ø¿ ¿

Ö·

´¡Ø µ

2Unfortunately, many FORTRAN compilers yield very slow nint functions. It is often

cheaper to write your own code to replace the nint library routine.

70

Chapter 4. Molecular Dynamics Simulations

Algorithm 6 (Integrating the Equations of Motion)

subroutine integrate(f,en) sumv=0 sumv2=0 do i=1,npart
xx=2*x(i)-xm(i)+delt**2*f(i) vi=(xx-xm(i))/(2*delt) sumv=sumv+vi sumv2=sumv2+vi**2 xm(i)=x(i) x(i)=xx enddo temp=sumv2/(3*npart) etot=(en+0.5*sumv2)/npart return end

integrate equations of motion
MD loop Verlet algorithm (4.2.3) velocity (4.2.4) velocity center of mass total kinetic energy update positions previous time update positions current time
instantaneous temperature total energy per particle

Comments to this algorithm:
1. The total energy etot should remain approximately constant during the simulation. A drift of this quantity may signal programming errors. It therefore is important to monitor this quantity. Similarly, the velocity of the center of mass sumv should remain zero.
2. In this subroutine we use the Verlet algorithm (4.2.3) to integrate the equations of motion. The velocities are calculated using equation (4.2.4).

similarly,

Ö´Ø ¹ ¡Øµ

Ç Ö´Øµ ¹ Ú´Øµ¡Ø · ´Øµ ¡Ø¾ ¹ ¡Ø¿ Ö · ´¡Ø µ

¾Ñ

¿

Summing these two equations, we obtain

Ç Ö´Ø · ¡Øµ · Ö´Ø ¹ ¡Øµ ¾Ö´Øµ · ´Øµ ¡Ø¾ · ´¡Ø µ Ñ

or

Ö´Ø · ¡Øµ ¾Ö´Øµ ¹ Ö´Ø ¹ ¡Øµ · ´Øµ ¡Ø¾

(4.2.3)

Ñ

The estimate of the new position contains an error that is of order ¡Ø , where ¡Ø is the time step in our Molecular Dynamics scheme. Note that the

4.3 Equations of Motion

71

Verlet algorithm does not use the velocity to compute the new position. One, however, can derive the velocity from knowledge of the trajectory, using
Ç Ö´Ø · ¡Øµ ¹ Ö´Ø ¹ ¡Øµ ¾Ú´Øµ¡Ø · ´¡Ø¿µ

or

Ç Ú´Øµ

Ö´Ø · ¡Øµ ¹ Ö´Ø ¹ ¡Øµ ·

´¡Ø¾ µ

¾¡Ø

(4.2.4)

This expression for the velocity is only accurate to order ¡Ø¾. However, it

is possible to obtain more accurate estimates of the velocity (and thereby

of the kinetic energy) using a Verlet-like algorithm (i.e., an algorithm that

yields trajectories identical to that given by equation (4.2.3)). In our program,

we use the velocities only to compute the kinetic energy and, thereby, the

instantaneous temperature.

Now that we have computed the new positions, we may discard the po-

sitions at time Ø ¹ ¡Ø. The current positions become the old positions and the

new positions become the current positions.

After each time step, we compute the current temperature (temp), the

current potential energy (en) calculated in the force loop, and the total en-

ergy (etot). Note that the total energy should be conserved.

This completes the introduction to the Molecular Dynamics method. The

reader should now be able to write a basic Molecular Dynamics program for

liquids or solids consisting of spherical particles. In what follows, we shall

do two things. First of all, we discuss, in a bit more detail, the methods avail-

able to integrate the equations of motion. Next, we discuss measurements

in Molecular Dynamics simulations. Important extensions of the Molecular

Dynamics technique are discussed in Chapter 6.

4.3 Equations of Motion
It is obvious that a good Molecular Dynamics program requires a good algorithm to integrate Newton’s equations of motion. In this sense, the choice of algorithm is crucial. However, although it is easy to recognize a bad algorithm, it is not immediately obvious what criteria a good algorithm should satisfy. Let us look at the different points to consider.
Although, at ﬁrst sight, speed seems important, it is usually not very relevant because the fraction of time spent on integrating the equations of motion (as opposed to computing the interactions) is small, at least for atomic and simple molecular systems.
Accuracy for large time steps is more important, because the longer the time step that we can use, the fewer evaluations of the forces are needed per unit of simulation time. Hence, this would suggest that it is advantageous to use a sophisticated algorithm that allows use of a long time step.

72

Chapter 4. Molecular Dynamics Simulations

Algorithms that allow the use of a large time step achieve this by storing information on increasingly higher-order derivatives of the particle coordinates. As a consequence, they tend to require more memory storage. For a typical simulation, this usually is not a serious drawback because, unless one considers very large systems, the amount of memory needed to store these derivatives is small compared to the total amount available even on a normal workstation.
Energy conservation is an important criterion, but actually we should distinguish two kinds of energy conservation, namely, short time and long time. The sophisticated higher-order algorithms tend to have very good energy conservation for short times (i.e., during a few time steps). However, they often have the undesirable feature that the overall energy drifts for long times. In contrast, Verlet-style algorithms tend to have only moderate shortterm energy conservation but little long-term drift.
It would seem to be most important to have an algorithm that accurately predicts the trajectory of all particles for both short and long times. In fact, no such algorithm exists. For essentially all systems that we study by MD simulations, we are in the regime where the trajectory of the system through phase space (i.e., the Æ-dimensional space spanned by all particle coordinates and momenta) depends sensitively on the initial conditions. This means that two trajectories that are initially very close will diverge exponentially as time progresses. We can consider the integration error caused by the algorithm as the source for the initial small difference between the “true” trajectory of the system and the trajectory generated in our simulation. We should expect that any integration error, no matter how small, will always cause our simulated trajectory to diverge exponentially from the true trajectory compatible with the same initial conditions. This so-called Lyapunov instability (see section 4.3.4) would seem to be a devastating blow to the whole idea of Molecular Dynamics simulations but we have good reasons to assume that even this problem need not be serious.
Clearly, this statement requires some clariﬁcation. First of all, one should realize that the aim of an MD simulation is not to predict precisely what will happen to a system that has been prepared in a precisely known initial condition: we are always interested in statistical predictions. We wish to predict the average behavior of a system that was prepared in an initial state about which we know something (e.g., the total energy) but by no means everything. In this respect, MD simulations differ fundamentally from numerical schemes for predicting the trajectory of satellites through space: in the latter case, we really wish to predict the true trajectory. We cannot afford to launch an ensemble of satellites and make statistical predictions about their destination. However, in MD simulations, statistical predictions are good enough. Still, this would not justify the use of inaccurate trajectories unless the trajectories obtained numerically, in some sense, are close to true trajectories.
This latter statement is generally believed to be true, although, to our

4.3 Equations of Motion

73

knowledge, it has not been proven for any class of systems that is of interest for MD simulations. However, considerable numerical evidence (see, e.g., [66]) suggests that there exist so-called shadow orbits. A shadow orbit is a true trajectory of a many-body system that closely follows the numerical trajectory for a time that is long compared to the time it takes the Lyapunov instability to develop. Hence, the results of our simulation are representative of a true trajectory in phase space, even though we cannot tell a priori which. Surprisingly (and fortunately), it appears that shadow orbits are better behaved (i.e., track the numerical trajectories better) for systems in which small differences in the initial conditions lead to an exponential divergence of trajectories than for the, seemingly, simpler systems that show no such divergence [66]. Despite this reassuring evidence (see also section 4.3.5 and the article by Gillilan and Wilson [67]), it should be emphasized that it is just evidence and not proof. Hence, our trust in Molecular Dynamics simulation as a tool to study the time evolution of many-body systems is based largely on belief. To conclude this discussion, let us say that there is clearly still a corpse in the closet. We believe this corpse will not haunt us, and we quickly close the closet. For more details, the reader is referred to [27, 67, 68].
Newton’s equations of motion are time reversible, and so should be our algorithms. In fact, many algorithms (for instance the predictor-corrector schemes, see Appendix E, and many of the schemes used to deal with constraints) are not time reversible. That is, future and past phase space coordinates do not play a symmetric role in such algorithms. As a consequence, if one were to reverse the momenta of all particles at a given instant, the system would not trace back its trajectory in phase space, even if the simulation would be carried out with inﬁnite numerical precision. Only in the limit of an inﬁnitely short time step will such algorithms become reversible. However, what is more important, many seemingly reasonable algorithms differ in another crucial respect from Hamilton’s equation of motion: true Hamiltonian dynamics leaves the magnitude of any volume element in phase space unchanged, but many numerical schemes, in particular those that are not time reversible, do not reproduce this area-preserving property. This may sound like a very esoteric objection to an algorithm, but it is not. Again, without attempting to achieve a rigorous formulation of the problem, let us simply note that all trajectories that correspond to a partic-
ular energy are contained in a (hyper) volume ª in phase space. If we
let Hamilton’s equation of motion act on all points in this volume (i.e., we let the volume evolve in time), then we end up with exactly the same vol-
ume. However, a non-area-preserving algorithm will map the volume ª on another (usually larger) volume ª¼. After sufﬁciently long times, we ex-
pect that the non-area-preserving algorithm will have greatly expanded the volume of our system in phase space. This is not compatible with energy conservation. Hence, it is plausible that nonreversible algorithms will have serious long-term energy drift problems. Reversible, area-preserving algo-

74

Chapter 4. Molecular Dynamics Simulations

rithms will not change the magnitude of the volume in phase space. This property is not sufﬁcient to guarantee the absence of long-term energy drift, but it is at least compatible with it. It is possible to check whether an algorithm is area preserving by computing the Jacobian associated with the transformation of old to new phase space coordinates.
Finally, it should be noted that even when we integrate a time-reversible algorithm, we shall ﬁnd that the numerical implementation is hardly ever truly time reversible. This is so, because we work on a computer with ﬁnite machine precision using ﬂoating-point arithmetic that results in rounding errors (on the order of the machine precision).
In the remainder of this section, we shall discuss some of these points in more detail. Before we do so, let us ﬁrst consider how the Verlet algorithm scores on these points. First of all, the Verlet algorithm is fast. But we had argued that this is relatively unimportant. Second, it is not particularly accurate for long time steps. Hence, we should expect to compute the forces on all particles rather frequently. Third, it requires about as little memory as is at all possible. This is useful when we simulate very large systems, but in general it is not a crucial advantage. Fourth, its short-term energy conservation is fair (in particular in the versions that use a more accurate expression for the velocities) but, more important, it exhibits little long-term energy drift. This is related to the fact that the Verlet algorithm is time reversible and area preserving. In fact, although the Verlet algorithm does not conserve the total energy of this system exactly, strong evidence indicates that it does conserve a pseudo-Hamiltonian approaching the true Hamiltonian in the limit of inﬁnitely short time steps (see section 4.3.3). The accuracy of the trajectories generated with the Verlet algorithm is not impressive. But then, it would hardly help to use a better algorithm. Such an algorithm may postpone the unavoidable exponential growth of the error in the trajectory by a few hundred time steps (see section 4.3.4), but no algorithm is good enough that it will keep the trajectories close to the true trajectories for a time comparable to the duration of a typical Molecular Dynamics run.3

4.3.1 Other Algorithms
Let us now brieﬂy look at some alternatives to the Verlet algorithm. The most naive algorithm is based simply on a truncated Taylor expansion of the particle coordinates:

Ö´Ø · ¡Øµ

Ö´Øµ · Ú´Øµ¡Ø · ´Øµ ¡Ø¾ · ¡ ¡ ¡ ¾Ñ

3Error-free integration of the equations of motion is possible for certain discrete models, such as lattice-gas cellular automata. But these models do not follow Newton’s equation of motion.

4.3 Equations of Motion

75

If we truncate this expansion beyond the term in ¡Ø¾, we obtain the so-called

Euler algorithm. Although it looks similar to the Verlet algorithm, it is much

worse on virtually all counts. In particular, it is not reversible or area pre-

serving and suffers from a (catastrophic) energy drift. The Euler algorithm

therefore is not recommended.

Several algorithms are equivalent to the Verlet scheme. The simplest

among these is the so-called Leap Frog algorithm [24]. This algorithm evalu-

ates the velocities at half-integer time steps and uses these velocities to com-

pute the new positions. To derive the Leap Frog algorithm from the Verlet

scheme, we start by deﬁning the velocities at half-integer time steps as fol-

lows:

Ú´Ø ¹ ¡Ø ¾µ

Ö´Øµ ¹ Ö´Ø ¹ ¡Øµ ¡Ø

and

Ú´Ø · ¡Ø ¾µ

Ö´Ø · ¡Øµ ¹ Ö´Øµ ¡Ø

From the latter equation we immediately obtain an expression for the new positions, based on the old positions and velocities:

Ö´Ø · ¡Øµ Ö´Øµ · ¡ØÚ´Ø · ¡Ø ¾µ

(4.3.1)

From the Verlet algorithm, we get the following expression for the update of

the velocities:

´Øµ Ú´Ø · ¡Ø ¾µ Ú´Ø ¹ ¡Ø ¾µ · ¡Ø
Ñ

(4.3.2)

As the Leap Frog algorithm is derived from the Verlet algorithm, it gives rise to identical trajectories. Note, however, that the velocities are not deﬁned at the same time as the positions. As a consequence, kinetic and potential energy are also not deﬁned at the same time, and hence we cannot directly compute the total energy in the Leap Frog scheme.
It is, however, possible to cast the Verlet algorithm in a form that uses positions and velocities computed at equal times. This velocity Verlet algorithm [69] looks like a Taylor expansion for the coordinates:

Ö´Ø · ¡Øµ Ö´Øµ · Ú´Øµ¡Ø · ´Øµ ¡Ø¾ ¾Ñ

(4.3.3)

However, the update of the velocities is different from the Euler scheme:

´Ø · ¡Øµ · ´Øµ

Ú´Ø · ¡Øµ Ú´Øµ ·

¡Ø

¾Ñ

(4.3.4)

Note that, in this algorithm, we can compute the new velocities only after we have computed the new positions and, from these, the new forces. It is not

76

Chapter 4. Molecular Dynamics Simulations

immediately obvious that this scheme, indeed, is equivalent to the original Verlet algorithm. To show this, we note that

Ö´Ø · ¾¡Øµ Ö´Ø · ¡Øµ · Ú´Ø · ¡Øµ¡Ø · ´Ø · ¡Øµ ¡Ø¾ ¾Ñ
and equation (4.3.3) can be written as

Ö´Øµ
By addition we get

Ö´Ø · ¡Øµ ¹ Ú´Øµ¡Ø ¹ ´Øµ ¡Ø¾ ¾Ñ

Ö´Ø · ¾¡Øµ · Ö´Øµ ¾Ö´Ø · ¡Øµ · Ú´Ø · ¡Øµ ¹ Ú´Øµ ¡Ø · ´Ø · ¡Øµ ¹ ´Øµ ¡Ø¾ ¾Ñ
Substitution of equation (4.3.4) yields

Ö´Ø · ¾¡Øµ · Ö´Øµ ¾Ö´Ø · ¡Øµ · ´Ø · ¡Øµ ¡Ø¾ Ñ
which, indeed, is the coordinate version of the Verlet algorithm. Let us end the discussion of Verlet-like algorithms by mentioning two
schemes that yield the same trajectories as the Verlet algorithm, but provide better estimates of the velocity. The ﬁrst is the so-called Beeman algorithm. It looks quite different from the Verlet algorithm:

Ö´Ø · ¡Øµ Ö´Øµ · Ú´Øµ¡Ø · ´Øµ ¹ ´Ø ¹ ¡Øµ ¡Ø¾ Ñ

(4.3.5)

¾ ´Ø · ¡Øµ · ´Øµ ¹ ´Ø ¹ ¡Øµ

Ú´Ø · ¡Øµ Ú´Øµ ·

¡Ø

Ñ

(4.3.6)

However, by eliminating Ú´Øµ from equation (4.3.5), using equation (4.3.6),

it is easy to show that the positions satisfy the Verlet algorithm. However,

the velocities are more accurate than in the original Verlet algorithm. As a

consequence, the total energy conservation looks somewhat better. A dis-

advantage of the Beeman algorithm is that the expression for the velocities

does not have time-reversal symmetry. A very simple solution to this prob-

lem is to use the so-called velocity-corrected Verlet algorithm for which the
Ç error both in the positions and in the velocities is of order ´¡Ø µ.

The velocity-corrected Verlet algorithm is derived as follows. First write

down a Taylor expansion for Ö´Ø · ¾¡Øµ, Ö´Ø · ¡Øµ, Ö´Ø ¹ ¡Øµ and Ö´Ø ¹ ¾¡Øµ:

Ö´Ø · ¾¡Øµ Ö´Ø · ¡Øµ Ö´Ø ¹ ¡Øµ
Ö´Ø ¹ ¾¡Øµ

Ö´Øµ · ¾Ú´Øµ¡Ø · Ú˙ ´Øµ´¾¡Øµ¾ ¾ · Ú¨ ´¾¡Øµ¿ ¿ · ¡ ¡ ¡ Ö´Øµ · Ú´Øµ¡Ø · Ú˙ ´Øµ¡Ø¾ ¾ · Ú¨¡Ø¿ ¿ · ¡ ¡ ¡ Ö´Øµ ¹ Ú´Øµ¡Ø · Ú˙ ´Øµ¡Ø¾ ¾ ¹ Ú¨¡Ø¿ ¿ · ¡ ¡ ¡ Ö´Øµ ¹ ¾Ú´Øµ¡Ø · Ú˙ ´Øµ´¾¡Øµ¾ ¾ ¹ Ú¨ ´¾¡Øµ¿ ¿ · ¡ ¡ ¡

4.3 Equations of Motion

77

By combining these equations, we can write

½¾Ú´Øµ¡Ø

Ç Ö´Ø · ¡Øµ ¹ Ö´Ø ¹ ¡Øµ ¹ Ö´Ø · ¾¡Øµ ¹ Ö´Ø ¹ ¾¡Øµ · ´¡Ø µ

or, equivalently,

Ç Ú´Øµ

Ú´Ø · ¡Ø

¾µ · Ú´Ø ¹ ¡Ø ¾

¾µ ¡Ø · ½¾

Ú˙ ´Ø¹¡Øµ¹Ú˙ ´Ø·¡Øµ

·

´¡Ø µ

(4.3.7)

Note that this velocity can be computed only after the next time step (i.e., we must know the positions and forces at Ø · ¡Ø to compute Ú´Øµ).

4.3.2 Higher-Order Schemes
For most Molecular Dynamics applications, Verlet-like algorithms are perfectly adequate. However, sometimes it is convenient to employ a higherorder algorithm (i.e., an algorithm that employs information about higherorder derivatives of the particle coordinates). Such an algorithm makes it possible to use a longer time step without loss of (short-term) accuracy or, alternatively, to achieve higher accuracy for a given time step. But, as mentioned before, higher-order algorithms require more storage and are, more often than not, neither reversible nor area preserving. This is true in particular of the so-called predictor-corrector algorithms, the most popular class of higher-order algorithms used in Molecular Dynamics simulations. For the sake of completeness, the predictor-corrector scheme is described in Appendix E.1. We refer the reader who wishes to know more about the relative merits of algorithms for Molecular Dynamics simulations to the excellent review by Berendsen and van Gunsteren [70].

4.3.3 Liouville Formulation of Time-Reversible Algorithms
Thus far we have considered algorithms for integrating Newton’s equations of motion from the point of view of applied mathematics. However, recently Tuckerman et al. [71] have shown how to systematically derive timereversible, area-preserving MD algorithms from the Liouville formulation of classical mechanics. The same approach has been developed independently by Sexton and Weingarten [72] in the context of hybrid Monte Carlo simulations (see section 14.2). As the Liouville formulation provides considerable insight into what makes an algorithm a good algorithm, we brieﬂy review the Liouville approach.
Let us consider an arbitrary function that depends on all the coordinates and momenta of the Æ particles in a classical many-body system. The term ´pÆ´Øµ rÆ´Øµµ depends on the time Ø implicitly, that is, through the

