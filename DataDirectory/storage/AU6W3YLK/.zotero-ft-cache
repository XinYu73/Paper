Numerical methods for electronic structure calculations of materials ∗

Yousef Saad †

James R. Chelikowsky‡ March 21, 2006

Suzanne M. Shontz †

Abstract
The goal of this article is to give an overview of numerical problems encountered when determining the electronic structure of materials and the rich variety of techniques used to solve these problems. The paper is intended for a diverse scienti£c computing audience. For this reason, we assume the reader does not have an extensive background in the related physics. Our overview focuses on the nature of the numerical problems to be solved, their origin, and on the methods used to solve the resulting linear algebra or nonlinear optimization problems. It is common knowledge that the behavior of matter at the nanoscale is, in principle, entirely determined by the Schro¨dinger equation. In practice, this equation in its original form is not tractable. Successful, but approximate, versions of this equation, which allow one to study nontrivial systems, took about £ve or six decades to develop. In particular, the last two decades saw a ¤urry of activity in developing effective software. One of the main practical variants of the Schro¨dinger equation is based on what is referred to as Density Functional Theory (DFT). The combination of DFT with pseudopotentials allows one to obtain in an ef£cient way the ground state con£guration for many materials. This article will emphasize pseudopotentialdensity functional theory, but other techniques will be discussed as well.

∗This work was supported by NSF under grant DMR-05-51195, by DOE under Grants DE-FG02-03ER25585 and DE-FG02-03ER15491, and by the Minnesota Supercomputing Institute.
†Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN 55455. Email: {saad, shontz} @cs.umn.edu
‡Center for Computational Materials, Institute for Computational Engineering and Sciences, Departments of Physics and Chemical Engineering, University of Texas, Austin, Texas 78712. Email: jrc@ices.utexas.edu
1

Contents

1 Introduction

3

2 Quantum descriptions of matter

5

2.1 The Hartree approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

2.2 The Hartree-Fock approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

3 Density Functional Theory

11

3.1 Local density approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

3.2 The Kohn-Sham equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

3.3 Pseudopotentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

4 Discretization

16

4.1 Plane waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

4.2 Localized orbitals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

4.3 Finite differences in real space . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

5 Diagonalization

21

5.1 Historical perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

5.2 Lanczos, Davidson, and related approaches . . . . . . . . . . . . . . . . . . . . . 22

5.3 Diagonalization methods in current computational codes . . . . . . . . . . . . . . 25

6 The optimization path: Avoiding the eigenvalue problem

27

6.1 Optimization approaches without orthogonality . . . . . . . . . . . . . . . . . . . 27

6.2 Density matrix approaches in DFT . . . . . . . . . . . . . . . . . . . . . . . . . . 28

6.3 Density matrix approaches in Hartree-Fock . . . . . . . . . . . . . . . . . . . . . 29

6.4 The “Car-Parrinello” viewpoint . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

6.5 Use of orthogonal polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

7 Geometry optimization

35

7.1 The geometry optimization problem . . . . . . . . . . . . . . . . . . . . . . . . . 35

7.2 Minimization algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

7.2.1 The steepest descent method . . . . . . . . . . . . . . . . . . . . . . . . . 37

7.2.2 Newton’s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

7.2.3 Quasi-Newton methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

7.2.4 Truncated Newton methods . . . . . . . . . . . . . . . . . . . . . . . . . 40

7.2.5 Conjugate gradient methods . . . . . . . . . . . . . . . . . . . . . . . . . 40

7.2.6 Iterative subspace methods . . . . . . . . . . . . . . . . . . . . . . . . . . 42

7.3 Practical recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

8 Concluding remarks

44

2

1 Introduction
Some of the most time consuming jobs of any high-performance computing facility are likely to be those involving calculations related to high energy physics or quantum mechanics. These calculations are very demanding both in terms of memory and in terms of computational power. They entail computational methods that are characterized by a rich variety of techniques which blend ideas from physics and chemistry, with applied mathematics, numerical linear algebra, numerical optimization, and parallel computing. In recent years, the scienti£c community has dramatically increased its interest in these problems as government laboratories, industrial research labs, and academic institutions are putting an enormous emphasis on materials and everything related to nanotechnology. This trend can be attributed to two converging factors. The £rst is that the stakes are very high and the second is that a major practical breakthrough has never been closer because the synergistic forces at play today are making it possible to do calculations with accuracies never anticipated.
Nanotechnology may gradually take the forefront of scienti£c computing in the same way that computational ¤uid dynamics (CFD) was at the forefront of scienti£c computing for several decades. The past few decades of scienti£c computing has been dominated by ¤uid ¤ow computations, in part because of the needs in aeronautics and automobile industries (e.g., aerodynamics and turbines). Model test problems for numerical analysts developing new algorithms are often from CFD (such as the model “convection-diffusion equation” or the model “Laplacian”). Similarly, a big part of applied mathematics focuses on error analysis and discretization schemes (£nite elements) for ¤uid ¤ow problems. Today, the need to develop novel or improved methods for CFD is diminishing, though this does not mean in any way that CFD methods are no longer in need of improvements. Yet, a look at recent publications in scienti£c computing, reveals that there is a certain dichotomy between the current trend in nanotechnology and the interest of the scienti£c community.
The “mega”- trend in nanotechnology is only timidly re¤ected by published articles in scienti£c computing. Few papers on “algorithms” utilize data sets or examples from standard electronic structure problems, or address problems that are speci£c to this class of applications. For example, one would expect to see more articles on the problem of computing a very large number of eigenvectors or that of global optimization of very complex functionals.
Part of the dif£culty can be attributed to the fact that the problems encountered in quantum mechanics are enormously more complex than those addressed in other areas, i.e., classical mechanics. The approximations and methods used have taken several generations of innovations by a community that is much larger and broader than that of mechanical and aerospace engineers. Chemists, chemical engineers, materials scientists, solid state physicists, electrical engineers, and even geophysicists, and more recently, bioengineers, all explore materials at the atomic or molecular level, using quantum mechanical models. Therein lies a second dif£culty, which is that these different groups have their own notation, constraints, and their own preferred methods. Chemists have a certain preference for Hartree-Fock based methods which are more accurate for their needs, but which physicists £nd too costly, if not intractable. This preference re¤ects the historical interests of chemists on molecules and the interests of physicists on solids.
Our paper presents an overview of some of the most successful methods used today to study the electronic structures of materials. A large variety of techniques is available and we will emphasize those methods related to pseudopotentials and density functional theory.
3

One of the greatest scienti£c achievements of humankind is the discovery, in the early part of the twentieth century, of quantum mechanical laws describing the behavior of matter. These laws make it possible, at least in principle, to predict the electronic properties of matter from the nanoscale to the macroscale. The progress that lead to these discoveries is vividly narrated in the book “Thirty years that shook physics” by George Gamov [55]. A series of discoveries, starting with the notion of quantas originated by Max Planck at the end of 1900, and ending roughly in the mid-1920’s, with the emergence of the Schro¨dinger wave equation, set the stage for the new physics. Solutions of the Schro¨dinger wave equation resulted in essentially a complete understanding of the dynamics of matter at the atomic scale. Thus, in 1929, Dirac had this to say: “The underlying physical laws necessary for the mathematical theory of a large part of physics and the whole chemistry are thus completely known, and the dif£culty is only that the exact application of these laws leads to equations much too complicated to be soluble. It therefore becomes desirable that approximate practical methods of applying quantum mechanics should be developed, which can lead to the explanation of the main features of complex atomic systems without too much computations”.
One could understand atomic and molecular phenomena, formally at least, from these equations. However, even today, solving the equations in their original form is nearly impossible, save for systems with a very small number of electrons. In the seventy-six years that have passed since this statement by Dirac, one continues to strive for better explanation of the main features of complex atomic systems “without too much computations”. However, Dirac would certainly have been amazed at how much progress was achieved in sheer computing power. Interestingly, these gains have been brought about by a major discovery (the transistor), which can be attributed in big part to the new physics and a better understanding of condensed matter, especially semiconductors. The gains made in hardware, on the one hand, and methodology, on the other, multiply each other to yield huge speed-ups and improvement in computational capabilities.
When it comes to methodology and algorithms, the biggest steps forward were made in the sixties with the advent of two key new ideas. One of them is density functional theory, which enabled one to transform the initial problem into one which involves functions of only one space variables instead of N space variables, for N -particle systems in the original Schro¨dinger equation. Instead of dealing with functions in R3N , we only need to handle functions in R3. The second substantial improvement came with pseudopotentials. In short pseudopotentials allowed one to reduce the number of electrons to be considered by constructing special potentials, which would implicitly reproduce the effect of chemically inert core electrons and explicitly reproduce the properties of the chemically active valence electrons . With pseudopotentials, only valence electrons, those on the outer shells of the atom, need be considered, e.g., a Pb atom is no more complex than a C atom as both have s2p2 valence con£gurations. This leads to substantial gains both in memory and a reduction of computational complexity.
In the following we often use terminology that is employed by physicists and chemists. For example we will speak of “diagonalization” when we will in fact mean “computation of eigenvalues and eigenvectors”, i.e., partial diagonalization. We will use script letters for operators and bold letters for vectors.
4

R3 r3 r
1

r2 R2
R1

Figure 1: Atomic and electronic coordinates: Filled circles represent electrons, open circles represent nuclei.

2 Quantum descriptions of matter

Consider N nucleons of charge Zn at positions {Rn} for n = 1, · · · , N and M electrons at positions {ri} for i = 1, · · · , M . An illustration is shown in Figure 1. The non-relativistic, timeindependent Schro¨dinger equation for the electronic structure of the system can be written as:

HΨ=EΨ

(1)

where the many-body wave function Ψ is of the form

Ψ ≡ Ψ(R1, R2, R3, · · · ; r1, r2, r3, · · · )

(2)

and E is the total electronic energy. The Hamiltonian H in its simplest form can be written as

H(R1, R2, R3, · · · ; r1, r2, r3, · · · ) =

N
−

2∇2n + 1

N

ZnZn′ e2

n=1 2Mn

2 n,n′=1, |Rn − Rn′ |

n=n′

−

M i=1

2∇2i 2m

−

N M Zne2 n=1 i=1 |Rn − ri|

+

1M

e2

2 i,j=1 |ri − rj | .

(3)

i=j

Here, Mn is the mass of the nucleus, is Planck’s constant, h, divided by 2π, m is the mass of the electron, and e is the charge of the electron.
The above Hamiltonian includes the kinetic energies for the nucleus (£rst sum in H), and each electron (3rd sum), the inter-nuclei repulsion energies (2nd sum), the nuclei-electronic (Coulomb)
attraction energies (4th sum), and the electron-electron repulsion energies (5th sum). Each Lapla-
cian ∇2n involves differentiation with respect to the coordinates of the nth nucleus. Similarly the term ∇2i involves differentiation with respect to the coordinates of the ith electron.
In principle, the electronic structure of any system is completely determined by (1), or, to be
exact, by minimizing the energy < Ψ|H|Ψ > under the constraint of normalized wave functions Ψ. Recall that Ψ has a probabilistic interpretation: for the minimizing wave function Ψ,

|Ψ(R1, · · · , RN ; r1, · · · , rM )|2d3R1 · · · d3RN d3r1 · · · d3rM

5

represents the probability of £nding electron 1 in volume |R1 + d3R1|, electron 2 in volume |R2 + d3R2|, etc. From a computational point of view however, the problem is not tractable for systems which include more than just a few atoms and dozen electrons, or so. The main computational dif£culty stems from the nature of the wave function which depends on all coordinates of all particles (nuclei and electrons) simultaneously. For example, if we had just 10 particles, and discretized each coordinate using just 100 points for each of the x, y, z directions, we would have 106 points for each coordinate for a total of (106)10 = 1060 variables altogether.
Soon after the discovery of the Schro¨dinger equation, it was recognized that this equation provided the means of solving for the electronic and nuclear degrees of freedom. Using the variational principle, which states that an approximate (normalized) wave function will always have a less favorable energy than the true ground state wave function, one had an equation and a method to test the solution. One can estimate the energy from

E =< Ψ|H|Ψ >≡

Ψ∗HΨ d3R1 d3R2 d3R3 Ψ∗Ψ d3R1 d3R2 d3R3 ·

··· ·· .

. d3r1 d3r2 d3r3 d3r1 d3r2 d3r3 ·

··· ··

.

(4)

Recall that the wave function Ψ is normalized, since its modulus represents a probability distribution. The state wave function Ψ is an L2-integrable function in C3 × C3 × · · · × C3. The bra (for < |) and ket (for | >) notation is common in chemistry and physics. These resemble the notions of outer and inner products in linear algebra and are related to duality. (Duality is de£ned from a bilinear form a(x, y): The vectors x and y are dual to each other with respect to the bilinear form.)
When applying the Hamiltonian to a state function Ψ the result is |H|Ψ > which is another state function Φ. The inner product of this function with another function η is < η|Φ > which is a scalar, a complex one in the general setting.
A number of highly successful approximations have been made to compute both the ground state, i.e., the state corresponding to minimum energy E, and excited state energies, or energies corresponding to higher eigenvalues E in (1). The main goal of these approximations is to reduce the number of degrees of freedom in the system as much as possible.
A fundamental and basic approximation is the Born-Oppenheimer or adiabatic approximation which separates the nuclear and electronic degrees of freedom. Since the nuclei are considerably more massive than the electrons, it can be assumed that the electrons will respond “instantaneously” to the nuclear coordinates. This allows one to separate the nuclear coordinates from the electronic coordinates. Moreover, one can treat the nuclear coordinates as classical parameters. For most condensed matter systems, this assumption called the Born-Oppenheimer approximation or adiabatic approximation is highly accurate [69, 186]. Under this approximation, the £rst term in (3) vanishes and the second becomes a constant. We can then work with a new Hamiltonian:

H(r1, r2, r3, · · · ) =

M

− 2∇2i 2m

i=1

−

N M Zne2 n=1 i=1 |Rn − ri|

+

1M

e2 .

2 i,j=1 |ri − rj |

(5)

i=j

This simpli£cation essentially removes degrees of freedom associated with the nuclei, but it will not be suf£cient to reduce the complexity of the Schro¨dinger equation to an acceptable level.

6

2.1 The Hartree approximation

If we were able to write the Hamiltonian H as a sum of individual (non-interacting) Hamiltonians, one for each electron, then it is easy to see that the problem would become separable. In this case the wave function Ψ can be written as a product of individual orbitals, φk(rk) each of which is an eigenfunction of the non-interacting Hamiltonian. This is an important concept and it is often characterized as the “one-electron” picture of a many-electron system.
The eigenfunctions of such a Hamiltonian determine orbitals (eigenfunctions) and energy levels (eigenvalues). For many systems, there are an in£nite number of states, enumerated by quantum numbers. Each eigenvalue represents an “energy” level corresponding to the orbital of interest. For example, in an atom such as hydrogen, an in£nite number of bound states exist, each labeled by a set of three discrete integers. In general, the number of integers equal the spatial dimensionality of the system plus spin. In hydrogen, each state can be labeled by three indices (n, l, and m) and s for spin. In the case of a solid, there are essentially an in£nite number of atoms and the energy levels can be labeled by quantum numbers, which are no longer discrete, but quasi-continuous. In this case, the energy levels form an energy band.
The energy states are £lled by minimizing the total energy of the system. The N lowest orbitals account for 2N electrons, if one ignores spin, and are occupied states. States above N are unoccupied or virtual states. The state with lowest energy (smallest eigenvalue) is the ground state. The ground state energy corresponds to the lowest eigenvalue. The ground sate determines a number of properties, e.g., stable structures, mechanical deformations, phase transitions, and vibrational modes. The states above the ground state are known as excited states. They are often used to calculate response functions of the solid, e.g., the dielectric and the optical properties of materials.
In mathematical terms, H ≡ ⊕Hi, the circled sum being a direct sum meaning that Hi acts only on particle number i, leaving the others unchanged. This not being true in general, Hartree suggested to use this as an approximation technique whereby the basis resulting from this calculation will be substituted in < Ψ|H|Ψ > / < Ψ|Ψ >, to yield an upper bound for the energy.
In order to make the Hamiltonian (5) non-interactive, we must remove the last term in (5), i.e., we assume that the electrons do not interact with each other. Then the electronic part of the Hamiltonian becomes:

Hel = Hel(r1, r2, r3, · · · ) =

M

− 2∇2i 2m

i=1

−

N M Zne2 n=1 i=1 |Rn − ri|

(6)

which can be cast in the form

M
Hel =

− 2∇2i 2m

+

VN (ri)

≡

M

Hi

(7)

i=1

i=1

where

VN (ri) =

−

N n=1

Zne2 |Rn − ri|

.

(8)

This simpli£ed Hamiltonian is separable and admits eigenfunctions of the form

ψ(r1, r2, r3, · · · ) = φ1(r1)φ2(r2)φ3(r3) · · · ,

(9)

7

where the φi(r) orbitals can be determined from the “one-electron” Hamiltonian:

Hiφi(r) = Eiφi(r) .

(10)

The total energy of the system is the sum of the eigenvalues, Ei. This model is extremely simple, but clearly not realistic. Physically, using the statistical inter-
pretation mentioned above, writing Ψ as the product of φi’s, only means that the electrons have independent probabilities of being located in a certain position in space. This lack of correlation
between the particles causes the resulting energy to be overstated. In particular, the Pauli Principle
states that no two electrons can be at the same point in space and have the same quantum numbers.
The solutions Ψ computed in (9) is known as the Hartree wave function. The Hartree approximation consists of using the Hartree wave function as an approximation
to solve the Hamiltonian including the electron-electron interactions. This process starts with
the use of the original adiabatic Hamiltonian (5) and forces a wave function to be a product of
single-electron orbitals, as in (9). The next step is to minimize the energy < Ψ|H|Ψ > under the constraint < Ψ|Ψ >= 1 for Ψ in the form (9). This condition is identical to imposing the conditions that the integrals of each |ψk|2 be equal to one. If we impose the equations given by the standard approach of the Lagrange multipliers combined with £rst order necessary conditions for
optimality, we would get

d

<

ψk|H|ψk dψk

>

−

λψk

=

0.

Evaluating d < ψ|H|ψ > /dψ over functions φ of norm one is straightforward. The £rst and second terms are trivial to differentiate. For the simple case when k = 1 and M = 3, consider the third term, which we denote by < Ψ|Ve|Ψ >:

<

Ψ|Ve|Ψ

>≡

1 2

M

i,j=1

i=j

e2|φ21φ22φ23| |ri − rj|

d3r1d3r2

d3r3

.

(11)

Because φ1 is normalized, this is easily evaluated to be a constant independent of φ1 when both i and j are different from k = 1. We are left with the differential of the sum over i = 1, j = 1. Consider only the term i = 1, j = 2 (the coef£cient e2 is omitted):

|φ21φ22φ23| |r1 − r2|

d3r1

d3

r2d3

r3

=

d3r3|φ3(r3)|2 ×

|φ1|2

|φ22 |r1 −

| r2|

d3

r2

d3r1 .

=1

By introducing a variation δφ1 in the above relation, it is easy to see (at least in the case of real variables) that the differential of the above term with respect to φ1 is the functional associated with the integral of |φ2(r2)|2/|r2 − r1|. A similar result holds for the term i = 1, j = 3. In the end the individual orbitals, φi(r), are solutions of the eigenvalue problem





 

− 2∇2 2m

+

VN (r)

+

M

e2 |φj (r′ )|2 |r′ − r|

d3r′

 

φi(r)

=

Eiφi(r)

.

(12)

j=1

j=i

8

The subscripts i, j of the coordinates have been removed as there is no ambiguity. The Hamiltonian

related to each earlier and

particle

can

be

written

in

the

form

H

=

− 2∇2 2m

+ VN

+

WH ,

where

VN

was

de£ned

M
WH ≡

e2 φj (r)φj (r)∗ d3 r′ |r′ − r|

.

(13)

j=1

j=i

This “Hartree potential”, or “Couloumb potential”, can be interpreted as the potential seen from
each electron by averaging the distribution of the other electrons |φj(r)|2’s. It can be obtained from solving the Poisson equation with the charge density e|φj(r)|2 for each electron j. Note that both VN and WH depend on the electron i. Another important observation is that solving the eigenvalue problem (12), requires the knowledge of the other orbitals φj, i.e., those for j = i. Also, the electron density of the orbital in question should not be included in the construction of the Hartree
potential.
The solution of the problem requires a self-consistent £eld (SCF) iteration. One begins with
some set of orbitals, and computes iteratively new sets by solving (12), using the most current set
of φ′js for j = i. This iteration is continued until the set of φi’s is self-consistent. Once the orbitals, φ(r), which satisfy (12) are computed, the Hartree many-body wave function
can be constructed and the total energy determined from (4). The Hartree approximation is useful
as an illustrative tool, but it is not an accurate approximation.
As indicated earlier, a key weakness of the Hartree approximation is that it uses wave functions
that do not obey one of the major postulates of quantum mechanics. Namely, electrons or Fermions
must satisfy the Pauli exclusion principle [103]. Moreover, the Hartree equation is dif£cult to
solve. The Hamiltonian is orbitally dependent because the summation in (12) does not include the
ith orbital. This means that if there are M electrons, then M Hamiltonians must be considered and (12) solved for each orbital.

2.2 The Hartree-Fock approximation

So far, we have not included spin in the state functions Ψ. Spin can be viewed as yet another quantum coordinate associated with each electron. This coordinate can assume two values: spin up or spin down. The exclusion principle states that there can be only two electrons in the same orbit and they must be of opposite spin. Since the coordinates must now include spin, we de£ne

xi =

ri si

where si is the spin of the ith electron. A canonical way to enforce the exclusion

principle is to require that a wave function Ψ be an antisymmetric function of the coordinates

xi of the electrons in that by inter-changing any two of these its coordinates, the function must change its sign. In the Hartree-Fock approximation, many body wave functions with antisymmetric

properties are constructed, typically cast as Slater determinants, and used to approximately solve

the eigenvalue problem associated with the Hamiltonian (5).

Starting with one-electron orbitals, φi(x) ≡ φ(r)σ(s), the following functions meet the anti-

9

symmetry requirements:

φ1(x1) φ1(x2) · · · · · · φ1(xM )

Ψ(x1, x2, x3, · · · ) = (M !)−1/2

φ2(x1) ···

φ2(x2) · · · ··· ···

··· ···

··· ···

.

(14)

φM (x1) · · · · · · · · · φM (xM )

The term (M !)−1/2 is a normalizing factor. If two electrons occupy the same orbit, two rows of the determinant will be identical and Ψ will be zero. The determinant will also vanish if two electrons occupy the same point in generalized space (i.e., xi = xj) as two columns of the determinant will be identical. Exchanging positions of two particles will lead to a sign change in the determinant.
The Slater determinant is a convenient representation, but one should stress that it is an ansatz. It is
probably the simplest many-body wave function that incorporates the required symmetry properties
for fermions, or particles with non-integer spins.
If one uses a Slater determinant to evaluate the total electronic energy and maintains wave
function normalization, the orbitals can be obtained from the following Hartree-Fock equations:

Hiφi(r) =

− 2∇2 2m

+

VN (r)

+

M

j=1

e2 |φj(r |r − r

′)|2 ′|

d3r

′

φi(r)

M
−
j=1

|r

e2 −r

′|

φ∗j (r

′)φi(r

′)

d3r

′

δsi,sj φj(r) = Eiφi(r) .

(15)

It is customary to simplify this expression by de£ning an electronic charge density, ρ:

M

ρ(r) =

|φj(r )|2,

(16)

j=1

and an orbital dependent “exchange-charge density”, ρHi F for the ith orbital:

ρHi F (r, r ′) =

M j=1

φ∗j (r

′)

φi(r ′) φ∗i (r ) φ∗i (r ) φi(r )

φj (r

)

δsi,sj

.

(17)

This “density” involves a spin dependent factor which couples only states (i, j) with the same spin coordinates (si, sj).
With these charge densities de£ned, it is possible to de£ne corresponding potentials. The
Coulomb or Hartree potential, VH, is de£ned by

VH(r) =

ρ(r)

|r

e2 −r

′|

d3r′

.

(18)

and an exchange potential can be de£ned by

Vxi (r) = −

ρHi F

(r,

r

′)

|r

e2 −r

′|

d3r′

.

(19)

10

This combination results in the following Hartree-Fock equation:

− 2∇2 2m

+

VN (r)

+

VH (r)

+

Vxi (r)

φi(r) = Eiφi(r) .

(20)

Once the Hartree-Fock orbitals have been obtained, the total Hartree-Fock electronic energy of the system,EHF , can be obtained from

EHF =

M

Ei

−

1 2

ρ(r)VH (r)

d3r

−

1 2

M

φ∗i (r ) φi(r )Vxi (r) d3r .

(21)

i

i

EHF is not a sum of the Hartree-Fock orbital energies, Ei. The factor of one-half in the electronelectron terms arises because the electron-electron interactions have been double counted in the

Coulomb and exchange potentials. The Hartree-Fock Schro¨dinger equation is only slightly more

complex than the Hartree equation. Again, the equations are dif£cult to solve because the exchange potential is orbitally dependent.
There is one notable difference in the Hartree-Fock summations compared to the Hartree sum-

mation. The Hartree-Fock sums include the i = j terms in (15). This difference arises because the exchange term corresponding to i = j cancels an equivalent term in the Coulomb summation. The i = j term in both the Coulomb and exchange term is interpreted as a “self-screening” of the electron. Without a cancellation between Coulomb and exchange terms a “self-energy” contribution to

the total energy would occur. Approximate forms of the exchange potential often do not have this

property. The total energy then contains a self-energy contribution which one needs to remove to obtain a correct Hartree-Fock energy.
The Hartree-Fock equation is an approximate solution to the true ground-state, many-body

wave function. Terms not included in the Hartree-Fock energy are referred to as correlation con-
tributions. One de£nition for the correlation energy, Ecorr, is to write it as the difference between the exact total energy of the system, Eexact, and the Hartree-Fock energies: Ecorr = Eexact − EHF . Correlation energies may be included by considering Slater determinants composed of orbitals

which represent excited state contributions. This method of including unoccupied orbitals in the

many-body wave function is referred to as con£guration interactions or “CI” [70]. Applying Hartree-Fock wave functions to systems with many atoms is not routine. The result-
ing Hartree-Fock equations are often too complex to be solved for extended systems, except in

special cases. The number of electronic degrees of freedom grows rapidly with the number atoms

often prohibiting an accurate solution, or even one’s ability to store the resulting wave function. As such, it has been argued that a “wave function” approach to systems with many atoms does not offer a satisfactory approach to the electronic structure problem. An alternate approach is based on

density functional theory.

3 Density Functional Theory
In a number of classic papers, Hohenberg, Kohn, and Sham established a theoretical basis for justifying the replacement of the many-body wave function by one-electron orbitals [73, 85, 102]. Their results put the charge density at center stage. The charge density is a distribution of probability, i.e., ρ(r1)d3r1 represents,in a probabilistic sense, the number of electrons (all electrons) in the in£nitesimal volume d3r1.
11

Speci£cally, the Hohenberg-Kohn results were as follows. The £rst Hohenberg and Kohn the-
orem states that for any system of electrons in an external potential Vext, the Hamiltonian (specifically Vext up to a constant) is determined uniquely by the ground-state density alone. Solving the Schro¨dinger equation would result in a certain ground-state wave function Ψ, to which is associated a certain charge density,

ρ(r1) =

M |Ψ(x1, x2, · · · , xM )|dx2 · · · dxM .

(22)

s1=↑,↓

From each possible state function Ψ one can obtain a (unique) probability distribution ρ. This mapping from the solution of the full Schro¨dinger equation to ρ is trivial. What is less obvious is that the reverse is true: Given a charge density, ρ, it is possible in theory to obtain a unique Hamiltonian and associated ground-state wave function, Ψ. Hohenberg and Kohn’s £rst theorem states that this mapping is one-to-one, i.e., we could get the Hamiltonian (and the wave function) solely from ρ. Remarkably, this statement is easy to prove.
The second Hohenberg-Kohn theorem provides the means for obtaining this reverse mapping:
The ground-state density of a system in a particular external potential can be found by minimizing an associated energy functional. In principle, there is a certain energy functional, which is minimized by the unknown ground state charge density, ρ. This statement still remains at a formal level in the sense that no practical means was given for computing Ψ or a potential, V. From the magnitude of the simpli£cation, one can imagine that the energy functional will not be easy to construct. Indeed, this transformation changes the original problem with a total of 3N coordinates plus spin, to one with only 3 coordinates, albeit with N orbitals to be determined.
Later Kohn and Sham provided a workable computational method based on the following result: For each interacting electron system, with external potential V0, there is a local potential Vks, which results in a density ρ equal to that of the interacting system. Thus, the Kohn-Sham energy functional is formally written in the form

2

HKS = 2m ∇2 + Veff ,

(23)

where the effective potential is de£ned as for a one-electron potential, i.e., as in (7),

Veff = VN (ρ) + VH (ρ) + Vxc(ρ).

(24)

Note that in contrast with (7), Vxc is now without an index, as it is only for one electron. Also note the dependence of each potential term on the charge density ρ, which is implicitly de£ned from the
set of occupied eigenstates ψi, i = 1, · · · , N of (23) by Eq. (16). The energy term associated with the nuclei-electron interactions is < VN |ρ >, while that asso-
ciated with the electron-electron interactions is < VH|ρ >, where VH is the Hartree potential,

VH =

ρ(r′) |r − r′

|

dr′

.

12

3.1 Local density approximation

The Kohn-Sham energy functional is of the following form:

2N

E(ρ) = −2m

φ∗i (r)∇2φi(r)dr + ρ(r)Vion(r)dr

i=1

+

1 2

ρ(r)ρ(r′) |r − r′|

drdr′

+

Exc(ρ(r))

(25)

The effective energy, or Kohn-Sham energy, may not represent the true, or “experimental energy,” because the Hamiltonian has been approximated.
A key contribution of Kohn-Sham’s work is the local density approximation or LDA. Within LDA, the exchange energy is expressed as

Ex[ρ(r)] = ρ(r)Ex[ρ(r)] d3r,

(26)

where Ex[ρ] is the exchange energy per particle of a uniform gas at a density of ρ. Within this framework, the exchange potential in (20) is replaced by a potential determined from the functional
derivative of Ex[ρ]:

Vx[ρ]

=

δEx[ρ] δρ

.

(27)

One serious issue is the determination of the exchange energy per particle, Ex, or the corresponding exchange potential, Vx. The exact expression for either of these quantities is unknown, save for special cases. From Hartree-Fock theory one can show that the exchange energy is given by

EHF EF G = 2
k<kf

2k2 − e2kf 2m π
k<kf

1 + 1 − (k/kf )2 ln 2(k/kf )

k + kf k − kf

(28)

which is the Hartree-Fock expression for the exchange energy of a free electron gas. In this expression, k is the wave vector for a free electron; it can be related to the momentum by p = k. The highest occupied wave vector is given by kf , where the Fermi energy is given by Ef = 2kf2/2m. One can write:

Ex[ρ]

=

−

3e2 4π

(3π2)1/3

[ρ(r)]4/3 d3r,

(29)

and taking the functional derivative, one obtains:

Vx[ρ]

=

−

e2 π

(3π2ρ(r))1/3

.

(30)

In contemporary theories, correlation energies are explicitly included in the energy function-
als [102]. These energies have been determined by numerical studies performed on uniform elec-
tron gases resulting in local density expressions of the form: Vxc[ρ(r)] = Vx[ρ(r)]+Vc[ρ(r)], where Vc represents contributions to the total energy beyond the Hartree-Fock limit [21]. It is also possible to describe the role of spin explicitly by considering the charge density for up and down spins:
ρ = ρ↑ + ρ↓. This approximation is called the local spin density approximation (LSDA) [102].

13

3.2 The Kohn-Sham equation

The Kohn-Sham equation [85] for the electronic structure of matter is given by

− 2∇2 2m

+

VN (r)

+

VH (r)

+

Vxc[ρ(r)]

φi(r) = Eiφi(r) .

(31)

This equation is usually solved “self-consistently”. An approximate charge is assumed to estimate the exchange-correlation potential, and this charge is used to determine the Hartree potential from (18). These approximate potentials are inserted in the Kohn-Sham equation and the total charge density determined as in (16). The “output” charge density is used to construct new exchangecorrelation and Hartree potentials. The process is repeated until the input and output charge densities or potentials are identical to within some prescribed tolerance.
Once a solution of the Kohn-Sham equation is obtained, the total energy can be computed from

M

EKS = Ei − 1/2 ρ(r)VH(r) d3r + ρ(r) Exc[ρ(r)] − Vxc[ρ(r)] d3r,

(32)

i

where Exc is a generalization of (26), i.e., the correlation energy density is included. The electronic energy, as determined from EKS, must be added to the ion-ion interactions to obtain the structural energies. This is a straightforward calculation for con£ned systems. For extended systems such as crystals, the calculations can be done using Madelung summation techniques [187].
Owing to its ease of implementation and overall accuracy, the local density approximation is a popular choice for describing the electronic structure of matter. It is relatively easy to implement and surprisingly accurate. Recent developments have included so-called gradient corrections to the local density approximation. In this approach, the exchange-correlation energy depends on the local density and the gradient of the density. This approach is called the generalized gradient approximation (GGA) [130].
When £rst proposed, density functional theory was not widely accepted within the chemistry community. The theory is not “rigorous” in the sense that it is not clear how to improve the estimates for the ground state energies. For wave function based methods, one can include more Slater determinants, as in a con£guration interaction approach. As the accuracy of the wave functions improve, the energy is lowered via the variational theorem. The Kohn-Sham equation is also variational, but owing to the approximate Hamiltonian, the converged energy need not approach the true ground-state energy. This is not a problem provided that one is interested in relative energies, where any inherent density functional errors cancel in taking energy differences. For example, if the Kohn-Sham energy of an atom is 10% too high and the corresponding energy of the atom in a crystal is also 10% too high, the cohesive energies which involve the difference of the two energies can be better than the nominal 10% error of the absolute energies. An outstanding fundamental issue of using density functional theory is obtaining an a priori estimate of the cancellation errors.
In some sense, density functional theory is an a posteriori theory. Given the transference of the exchange-correlation energies from an electron gas, it is not surprising that errors would arise in its implementation to highly non-uniform electron gas systems as found in realistic systems. However, the degree of error cancellations is rarely known. Thus, the reliability of density functional theory has been established by numerous calculations for a wide variety of condensed matter systems. For example, the cohesive energies, compressibility, structural parameters and vibrational spectra

14

of elemental solids have been calculated within the density functional theory [26]. The accuracy of the method is best for systems in which the cancellation of errors is expected to be complete. Since cohesive energies involve the difference in energies between atoms in solids and atoms in free space, error cancellations are expected to be signi£cant. This is re¤ected in the fact that historically cohesive energies have presented greater challenges for density functional theory: the errors between theory and experiment are typically ∼ 10-20%, depending on the nature of the density functional and the material of interest. In contrast, vibrational frequencies which involve small structural changes within a given crystalline environment are often reproduced to within 1-2%.
3.3 Pseudopotentials
A major dif£culty in solving the eigenvalue problem arising from the Kohn-Sham equation is the length and energy scales involved. The inner (core) electrons are highly localized and tightly bound compared to the outer (valence electrons). A simple basis function approach is frequently ineffectual. For example, a plane wave basis (see next section) might require 106 waves to represent converged wave functions for a core electron, whereas only 102 waves are required for a valence electron [24]. The use of pseudopotentials overcomes this problem by removing the core states from the problem and replacing the all-electron potential by one that replicates only the chemically active, valence electron states [24]. It is well-known that the physical properties of solids depend essentially on the valence electrons rather than on the core electrons, e.g., the Periodic Table is based on this premise. By construction, the pseudopotential reproduces exactly the valence state properties such as the eigenvalue spectrum and the charge density outside the ion core. The pseudopotential model treats matter as a sea of valence electrons moving in a background of ion cores (Fig. 2).
The cores are composed of nuclei and inert inner electrons. Within this model many of the complexities of an all-electron calculation are avoided. A group IV solid such as C with 6 electrons is treated in a similar fashion to Pb with 82 electrons since both elements have 4 valence electrons.
The pseudopotential approximation takes advantage of this observation by removing the core electrons and introducing a weaker potential, which will make the (pseudo)wave functions behave like the all-electron wave function near the locations of the valence electrons, i.e., beyond a certain radius rc away from the core region. The valence wave functions often oscillate rapidly in the core region because of the orthogonality requirement of the valence states to the core states. This oscillatory or nodal structure of the wave functions corresponds to the high kinetic energy in this region. It is costly to represent these oscillatory functions accurately, no matter what discretization or expansion is used. (To some extent there is some resemblance between the pseudo-potential approximation and methods used in computer science related to principal component analysis: both methods reject components that are highly oscillating because their removal does not alter the entire perspective much.) Pseudopotential calculations center on the accuracy of the valence electron wave function in the spatial region away from the core, i.e., within the “chemically active” bonding region. The smoothly-varying pseudo wave function should be identical with the appropriate allelectron wave function in the bonding regions. A similar construction was introduced by Fermi in 1934 [45] to account for the shift in the wave functions of high-lying states of alkali atoms subject to perturbations from foreign atoms. In this remarkable paper, Fermi introduced the conceptual basis for both the pseudopotential and the scattering length. In Fermi’s analysis, he noted that it
15

was not necessary to know the details of the scattering potential. Any number of potentials which reproduced the phase shifts of interest would yield similar scattering events.

Figure 2: Standard pseudopotential model of a solid. The ion cores composed of the nuclei and tightly bound core electrons are treated as chemically inert. The pseudopotential model describes only the outer, chemically-active, valence electrons.

A variety of methods exist to construct pseudopotentials [105]. Almost all these methods are
based on “inverting” the Kohn-Sham equation. As a simple example, suppose we consider an
atom, where we know the valence wave function, ψv and the valence energy, Ev. Let us replace the true valence wave function by an approximate pseudo-wave function, φpv. Then the ion core pseudopotential is given by

Vipon =

2φpv 2m

−

VH

−

Vxc

+

Ev

.

(33)

The charge density in this case is ρ = |φpv|2 from which VH and Vxc can be calculated. The key aspect of this inversion is choosing φpv to meet several criteria, e.g., φpv=ψv outside the core radius, rc. Unlike the all-electron potential, pseudopotentials are not simple functions of position. For example, the pseudopotential is state dependent, or angular momentum dependent, i.e., in principle
one has a different potential for s, p, d, · · · states. Details can be found in the literature [24, 105].

4 Discretization
The Kohn-Sham equation must be ‘discretized’ before it can be numerically solved. The term ‘discretization’ is used here in the most inclusive manner, and in agreement with common terminology of numerical computing, to mean any method which reduces a continuous problem to one with a £nite number of unknowns.

16

Figure 3: A simple cubic lattice.

There have been three predominant ways of discretizing the Schro¨dinger equation. The £rst uses plane wave bases, the second uses specialized functions such as exponential or Gaussian orbitals, and the third does not use an explicit basis but discretizes the equations in real space.

4.1 Plane waves
Owing to the use of pseudopotentials, simple basis sets such as a plane wave basis can be quite effective, especially for crystalline matter. For example, in the case of crystalline silicon only 50-100 plane waves need to be used for a well-converged solution. The resulting matrix representation of the Schro¨dinger operator is dense in Fourier (plane-wave) space, but it is not formed explicitly. Instead, matrix-vector product operations are performed with the help of fast Fourier transforms. A plane wave approach is akin to spectral techniques used in solving certain types of partial differential equations [51]. The plane-wave basis used is of the following form:

ψk(r) = α(k, G) exp (i(k + G) · r)

(34)

G

where k is the wave vector, G is a reciprocal lattice vector, and α(k, G) represent the coef£cients of the basis. Thus, each plane wave is labelled by a wave vector, which is a quantum number composed of a triplet of three integers, i.e., k = (k1, k2, k3), and in principle spin. The vector parameter G translates the periodicity of the wave function with respect to a lattice, which along with an atomic basis de£nes a crystalline structure.
It is interesting to consider the origin of the use of plane waves. As might be guessed, plane wave bases are closely tied to periodic systems. The well-known Bloch theorem characterizes the spectrum of the Schro¨dinger operator ∇2 + V when the potential V is periodic. It states that eigenfunctions must be of the form ψjk(r)e−ik.r, where k is a vector in the ‘Brillion’ zone. For a given lattice, periodicity takes place in three spatial directions, see Figure 3. The Hamiltonian is invariant under translation in each of these directions.
Bloch’s theorem states that for a periodic potential V, the spectrum of the Schro¨dinger operator ∇2 + V consists of a countable set of intervals (called energy bands). The eigenvalues are labelled as {ǫj,k}, where k belongs to an interval, and j = 1, 2, . . . .

17

When expressed (i.e., projected) in a plane-wave basis, the Hamiltonian is actually a dense matrix. Speci£cally, the Laplacian term of the Hamiltonian is represented by a diagonal matrix, but the potential term Vtpot gives rise to a dense matrix.
For periodic systems, where k is a good quantum number, the plane-wave basis coupled to pseudopotentials is quite effective. However, for non-periodic systems, such as clusters, liquids or glasses, the plane-wave basis is often combined with a supercell method [24]. The supercell repeats the localized con£guration to impose periodicity to the system. This preserves the “arti£cial” validity of k and Bloch’s theorem which (34) obeys.
There is a parallel to be made with spectral methods, which are quite effective for simple periodic geometries, but lose their superiority when more generality is required. In addition to these dif£culties, the two fast Fourier transforms performed at each iteration can be costly, requiring n log n operations, where n is the number of plane waves, versus O(N ) for real space methods where N is the number of grid points. Usually, the matrix size N × N is larger than n × n but only within a constant factor. This is exacerbated in high performance environments where fast Fourier transforms require an excessive amount of communication and are particularly dif£cult to implement ef£ciently.
4.2 Localized orbitals
A popular approach to studying the electronic structure of materials uses a basis set of orbitals localized on atomic sites. This is the approach for example taken in the SIESTA code [169] where, with each atom a, is associated a basis set of functions, which combine radial functions around a with spherical harmonics:
φalmn(r) = φaln(ra)Ylm(ˆra)
where ra = r − Ra. The radial functions can be exponentials, Gaussians, or any localized function. Gaussian bases
have a special advantage of yielding analytical matrix elements provided the potentials are also expanded in Gaussians [16, 25, 75, 76]. However, the implementation of a Gaussian basis is not as straightforward as with plane waves. For example, numerous indices must be used to label the state, the atomic site, and the Gaussian orbitals employed. This increases “bookkeeping” operations tremendously. Also, the convergence is not controlled by a single parameter as it is with plane waves, e.g., if atoms in a solid are moved, the basis should be re-optimized for each new geometry. Moreover, it is not always obvious what basis functions are needed and much testing has to be done to insure that the basis is complete. On the positive side, a Gaussian basis yields much smaller matrices and requires less memory than plane-wave methods. For this reason, Gaussians are especially useful for describing transition metal systems, where large number of plane waves are needed.
4.3 Finite differences in real space
An appealing alternative is to avoid explicit bases altogether and work instead in real space, using £nite difference discretizations. This approach has become popular in recent years, and has seen a number of implementations [11, 15, 27–29, 43, 51, 64, 83, 94, 122, 182, 188].
18

The real-space approach overcomes many of the complications involved with non-periodic

systems, and although the resulting matrices can be larger than with plane waves, they are quite

sparse, and the methods are easy to implement on sequential and parallel computers. Even on

sequential machines, real-space methods can be an order of magnitude faster than methods based

on traditional approaches.

The simplest real-space method utilizes £nite difference discretization on a cubic grid. There

have also been implementations of the method with £nite elements [127, 177], and even meshless

methods [79]. Finite element discretization methods may be successful in reducing the total num-

ber of variables involved, but they are far more dif£cult to implement. A key aspect to the success

of the £nite difference method is the availability of higher-order £nite difference expansions for

the kinetic energy operator, i.e., expansions of the Laplacian [52]. Higher-order £nite difference

methods signi£cantly improve convergence of the eigenvalue problem when compared with stan-

dard, low-order £nite difference methods. If one imposes a simple, uniform grid on our system

where

the

points

are

described

in

a

£nite

domain

by

(xi, yj, zk),

we

approximate

∂2ψ ∂x2

at

(xi, yj, zk)

by

∂2ψ ∂x2

=

M

Cnψ(xi + nh, yj, zk) + O(h2M+2),

(35)

n=−M

where h is the grid spacing and M is a positive integer. This approximation is accurate to O(h2M+2)
upon the assumption that ψ can be approximated accurately by a power series in h. Algorithms are
available to compute the coef£cients Cn for arbitrary order in h [52]. These are shown for the £rst few orders in Table 1

ord 2

1 -2 1

ord 4

−1 4 −5 4 −1

12 3

2

3 12

ord 6

1 90

−

3 20

3 2

−

49 18

3 2

−

3 20

1 90

ord 8

−

1 560

8 315

−

1 5

8 5

−

205 72

8 5

−

1 5

8 315

−

1 560

Table 1: Finite Difference coef£cients (Fornberg-Sloan formulas) for ∂ 2/∂x2 for orders 2 to 8.
With the kinetic energy operator expanded as in (35), one can set up a one-electron Schro¨dinger equation over a grid. One may assume a uniform grid, but this is not a necessary requirement. ψ(xi, yj, zk) is computed on the grid by solving the eigenvalue problem:

2

M

M

− 2m

Cn1ψn(xi + n1h, yj, zk) +

Cn2ψn(xi, yj + n2h, zk)

n1=−M

n2=−M

M

+

Cn3ψn(xi, yj, zk + n3h) + [ Vion(xi, yj, zk) + VH (xi, yj, zk)

n3=−M

+Vxc(xi, yj, zk) ] ψn(xi, yj, zk) = En ψn(xi, yj, zk)

(36)

If we have L grid points, the size of the full matrix resulting from the above problem is L × L. 19

Figure 4: A uniform grid illustrating a typical con£guration for examining the electronic structure of a localized system. The dark gray sphere represents the actual computational domain, i.e., the area where wave functions are allowed to be nonzero. The light spheres within the domain are atoms.
20

A grid based on points uniformly-spaced in a three-dimensional cube as shown in Fig. 4 is typically used. Many points in the cube are far from any atoms in the system, and the wave function on these points may be replaced by zero. Special data structures may be used to discard these points and keep only those having a nonzero value for the wave function. The size of the Hamiltonian matrix is usually reduced by a factor of two to three with this strategy, which is quite important considering the large number of eigenvectors which must be saved. Further, since the Laplacian can be represented by a simple stencil, and since all local potentials sum up to a simple diagonal matrix, the Hamiltonian need not be stored explicitly as a sparse matrix. Handling the ion core pseudopotential is complex, as it consists of a local and a nonlocal term In the discrete form, the nonlocal term becomes a sum over all atoms, a, and quantum numbers, (l, m), of rank-one updates:

Vion =

Vloc,a +

ca,l,mUa,l,mUTa,l,m

a

a,l,m

(37)

where Ua,l,m are sparse vectors which are only non-zero in a localized region around each atom, and ca,l,m are normalization coef£cients.

5 Diagonalization
There are a number of dif£culties which emerge when solving the (discretized) eigenproblems, besides the sheer size of the matrices. The £rst, and biggest, challenge is that the number of required eigenvectors is proportional to the atoms in the system, and can grow up to thousands, if not more. In addition to storage, maintaining the orthogonality of these vectors can be very demanding. Usually, the most computationally expensive part of diagonalization codes is orthogonalization. Second, the relative separation of the eigenvalues decreases as the matrix size increases, and this has an adverse effect on the rate of convergence of the eigenvalue solvers. Preconditioning techniques attempt to alleviate this problem. Real-space codes bene£t from savings brought about by not needing to store the Hamiltonian matrix, although this may be balanced by the need to store large vector bases.

5.1 Historical perspective
Large computations on the electronic structure of materials started in the 1970’s after the seminal work of Kohn, Hohenberg, and Sham in developing DFT and because of the invention of ab initio pseudopotentials [24]. It is interesting to note that “large” in the 1970’s implied matrices of sizes a few hundreds to a few thousands. One must wait the mid- to late-1980’s to see references to calculations with matrices of size around 7,000. For example, the abstract of a paper by Martins and Cohen [104] states: “Results of calculations for molecular hydrogen with matrix sizes as large as 7,200 are presented as an example”. Similarly, the well-known Car and Parrinello paper [19], which uses an approach based on simulated annealing, features an example with 16 × 437 = 6, 992 unknowns. This gives a rough idea of the typical problem sizes about 20 years ago. The paper by Car and Parrinello [19] is often viewed as a de£ning moment in the development of computational codes. It illustrated how to effectively combine several ingredients: plane waves, pseudopotentials, the use of FFT’s, and especially how to apply pseudopotential methods to molecular dynamics.
21

From the inception of realistic computations for the electronic structure of materials, the basis of choice has been plane waves. In the early days this contributed to the limitation of the capability because the matrices were treated as dense. The paper [104], see also [105], showed how to avoid storing a whole dense matrix by a judicious use of FFT in plane-wave codes and by working essentially in Fourier space. A code called Ritzit, initially published in Algol, was available [147], and this constituted an ideally-suited technique for diagonalization. The method was “preconditioned” by a Jacobi iteration or by DIIS.

5.2 Lanczos, Davidson, and related approaches
The Lanczos algorithm [90] is one of the best-known techniques [148] for diagonalizing a large sparse matrix A. In theory, the Lanczos algorithm generates an orthonormal basis v1, v2, . . . , vm, via an inexpensive 3-term recurrence of the form :

βj+1vj+1 = Avj − αjvj − βjvj−1 .

In the above sequence, αj = vjH Avj, and βj+1 = Avj − αjvj − βjvj−1 2. So the jth step of the algorithm starts by computing αj, then proceeds to form the vector vˆj+1 = Avj − αjvj − βjvj−1, and then vj+1 = vˆj+1/βj+1. Note that for j = 1, the formula for vˆ2 changes to vˆ2 = Av2 − α2v2.
Suppose that m steps of the recurrence are carried out, and consider the tridiagonal matrix,





α1 β2

Tm

=

β2 



α2 ...

β3 ...



.

.

.

 


.

βm αm

Further, denote by Vm the n × m matrix Vm = [v1, . . . , vm] and by em the mth column of if the m × m identity matrix. After m steps of the algorithm, the following relation holds:

AVm = VmTm + βm+1vm+1eTm .
In the ideal situation, where βm+1 = 0 for a certain m, AVm = VmTm, and so the subspace spanned by the vi’s is invariant under A, and the eigenvalues of Tm become exact eigenvalues of A. This is the situation when m = n, and it may also happen for m ≪ n, though this situation, called lucky (or happy) breakdown ( [123]) is highly unlikely in practice. In the generic situation, some of the eigenvalues of the tridiagonal matrix Hm will start approximating corresponding eigenvalues of A when m becomes large enough. An eigenvalue λ˜ of Hm is called a Ritz value, and if y is an associated eigenvector, then the vector Vmy is, by de£nition, the Ritz vector, i.e., the approximate eigenvector of A associated with λ˜. If m is large enough, the process may yield good approximations to the desired eigenvalues λ1, . . . , λs of H, corresponding to the occupied states, i.e., all occupied eigenstates.
There are several practical implementations of this basis scheme. All that was said above is what happens in theory. In practice, orthogonality of the Lanczos vectors, which is guaranteed in theory, is lost as soon as one of the eigenvectors starts to converge [123]. As such, a number of schemes have been developed to enforce the orthogonality of the Lanczos vectors; see [91, 92, 166, 167, 183]. The most-common method consists of building a scalar recurrence, which parallels

22

the three-term recurrence of the Lanczos vectors and models the loss of orthogonality. As soon as

loss of orthogonality is detected, a reorthogonalization step is taken. This is the approach taken

in the computational codes PROPACK [91] and PLAN [183]. In these codes, semi-orthogonality

is enforced, i.e., the inner product threshold, which is of the order of

√ofεtwwohebraesεisisvethcetomrsaicshoinnelyepgsuialorann[t6e2ed].

not

to

exceed

a

certain

Since the eigenvectors are not individually needed, one can think of not computing them but

rather to just use a Lanczos basis Vm = [v1, . . . , vm] directly. This does not provide a good basis in general. However, a full Lanczos algorithm without partial reorthogonalization can work quite

well when combined with a good stopping criterion.

A simple scheme used in [12] is to monitor the eigenvalues of the tridiagonal matrices Ti, i = 1, . . . , m. The cost for computing only the eigenvalues of Ti is O(i2). If we were to apply the test at every single step of the procedure, the total cost for all m Lanczos steps would be O(m3), which

can be quite high. This cost can be reduced drastically, to the point of becoming negligible relative

to the overall cost, by employing a number of simple strategies. For example, one can monitor the

eigenvalues of the tridiagonal matrix Ti at £xed intervals, i.e., when M OD(i, s) = 0, where s is a certain £xed stride. Of course, large values of s will induce infrequent convergence tests, thus

reducing the cost from O(m3) O(s) additional Lanczos steps

tboefOor(em3sc3o).nvOenrgtehnecoethiserdehtaencdte,da.

large

stride

may

in¤ict

unnecessary

Though not implemented in [12], a better strategy is to use the bisection algorithm (see [62]

Sec. 8.5) to track the latest eigenvalue that has converged, exploiting the important property that

the Lanczos procedure is a variational technique in the sense that when an eigenvalue converges,

later steps can only improve it. In addition, convergence tends to occur from left to right in the

spectrum, meaning that typically the smallest eigenvalue converges £rst followed by the second

smallest, etc. This suggests many simple procedures based on the bisection algorithm. When

convergence has been detected (say at step l ≤ m) then the charge densities are approximated as

the squares of the norms of the associated eigenvectors. See [12] for details.

Another popular algorithm for extracting the eigenpairs is the Davidson [115] method, which

can be viewed as a preconditioned version of the Lanczos algorithm, in which the preconditioner

is the diagonal of A. We refer to the generalized Davidson algorithm as a Davidson approach in

which the preconditioner is not restricted to being a diagonal matrix. (A detailed description can

be found in [150].)

The Davidson algorithm differs from the Lanczos method in the way in which it de£nes new

vectors to add to the projection subspace. Instead of adding just Avj, it preconditions a given residual vector ri = (A − µiI)ui and adds it to the subspace (after orthogonalizing it against current basis vectors). The algorithm consists of an “eigenvalue loop,” which computes the desired

eigenvalues one by one (or a few at a time), and a “basis” loop which gradually computes the

subspace on which to perform the projection. Consider the eigenvalue loop which computes the

ith eigenvalue and eigenvector of A. If M is the current preconditioner, and V = [v1, · · · , vk] is the current basis, the main steps of the main loop are as follows:

1. Compute the ith eigenpair (µk, yk) of Ck = VkT AVk. 2. Compute the residual vector rk = (A − µkI)Vkyk. 3. Precondition rk, i.e., compute tk = M−1rk.

23

4. Orthonormalize tk against v1, · · · , vk and call vk+1 the resulting vector, so Vk+1 = [Vk, vk+1].

5. Compute the last column-row of Ck+1 = VkT+1AVk+1.
At this point, one needs to decide on the choice of a preconditioner. The original Davidson approach used the diagonal of the matrix as a preconditioner, but this works only for limited cases. For a plane-wave basis, it is possible to construct fairly effective preconditioners by exploiting the lower-order bases. By this, we mean that if Hk is the matrix representation obtained by using k plane waves, we can construct a good approximation to Hk from Hm, with m ≪ k, by completing it with a diagonal matrix representing the larger (undesirable) modes. Note that these matrices are not explicitly computed as they are dense. This possibility of building lower-dimensional approximations to the Hamiltonian, which can be used to precondition the original matrix, constitutes an advantage of plane wave-based methods.
Preconditioning techniques in this approach are typically based on £ltering ideas and the fact that the Laplacian is an elliptic operator [175]. The eigenvectors corresponding to the few lowest eigenvalues of ∇2 are smooth functions, and so are the corresponding wave functions. When an approximate eigenvector is known at the points of the grid, a smoother eigenvector can be obtained by averaging the value at every point with the values of its neighboring points. Assuming a cartesian (x, y, z) coordinate system, the low frequency £lter acting on the value of the wave function at the point (i, j, k), which represents one element of the eigenvector, is described by:

ψi−1,j,k + ψi,j−1,k + ψi,j,k−1 + ψi+1,j,k + ψi,j+1,k + ψi,j,k+1

12

+

ψi,j,k 2

→ (ψi,j,k)F iltered .

(38)

Other preconditioners that have been tried resulted in mixed success. The use of shift-andinvert [124] involves solving linear systems with A − σI, where A is the original matrix, and the shift σ is close to the desired eigenvalue (s). These methods would be prohibitively expensive in most situations, given the size of the matrix and the number of times that A − σI must be factored. Alternatives based on an approximate factorization such as ILUT [149] are ineffective beyond the £rst few eigenvalues. Methods based on approximate inverse techniques have been somewhat more successful, performing better than £ltering at additional preprocessing and storage cost. Preconditioning ‘interior’ eigenvalues, i.e., eigenvalues located well inside the interval containing the spectrum, is still a very hard problem. Current solutions only attempt to dampen the effect of eigenvalues which are far away from the ones being computed. This is in effect what is achieved by £ltering and sparse approximate inverse preconditioners. These techniques do not reduce the number of steps required for convergence in the same way that shift-and-invert techniques do. However, £ltering techniques are inexpensive to apply and result in non-negligible savings in iterations.
In real space, it is trivial to operate with the potential term which is represented by a diagonal matrix, and in Fourier space it is trivial to operate with the Laplacian term, which is also represented by a diagonal matrix. The use of plane-wave bases also leads to natural preconditioning techniques which are obtained by simply employing a matrix obtained from a smaller plane-wave basis, neglecting the effect of high frequency terms on the potential.
Real-space algorithms avoid the use of fast Fourier transforms by performing all calculations in real physical space instead of Fourier space. Fast Fourier transforms require global commu-

24

nication; as such, they tend to be harder to implement on message-passing distributed memory multi-processor systems. The only global operation remaining in real-space approaches is that of the inner products. These inner products are required when forming the orthogonal basis used in the generalized Davidson procedure. Inner products will scale well as long as the vector sizes in each processor remain relatively large.
5.3 Diagonalization methods in current computational codes
Table 2 shows a number of computational codes currently available or in development. This list is by no means exhaustive. What is rather remarkable is the time-frame in which these codes have been developed and the speed with which they have been adapted to new computing environments. Most of them have been coded in Fortran-90/95 and most offer parallel versions with either MPI or OPEN-MP. (An interesting account of the impact of new software engineering methods in electronic structure codes can be found in [162].) The middle column of the table shows the type of discretization (basis) used, where PW stands for plane waves, RS for real space, LCAO for Linear combination of atomic orbitals, APW for Augmented plane waves, Gauss for Gaussian orbitals, and OTH for other. As can be noted, most codes use plane-wave bases. The augmented, planewave basis essentially combines a radial function times a spherical function near the atom, and a plane-wave expansion in the interstitial region.
A few of the codes have not been updated in a few years; we only listed those for which the website is still maintained. A star next to the code name indicates that the code has restricted distribution (e.g. DoD PW), or that it is still in a development phase. We separated the codes which use the GPL license and the codes which can be downloaded directly. These are the £rst 5 listed in the table, and they are separated by a horizontal line from the others. All others require either a fee (e.g., VASP, Wien2K, phi98PP, and CASTEP) or a licensing agreement (without a fee).
Not all these codes resort to some form of diagonalization. For example, the CPMD code [1], uses the Car-Parrinello approach which relies entirely on a statistical approach and molecular dynamics to minimize the energy. (See Section 6.4.) Similarly the CONQUEST code is a linearscaling method which uses a density matrix approach (see next section). In addition, the codes using an LCAO basis obtain dense matrices and resort to standard dense matrix diagonalization.
The earliest electronic structures codes used variants of the subspace iteration algorithm [105]. There should therefore be no surprise that many existing codes propose improved versions of this scheme. For example, ABINIT [63] uses a form of subspace iteration, in which an initial subspace is selected, and then an iterative scheme is invoked to ‘improve’ the basis vectors individually by some form of preconditioned conjugate gradient algorithm. In this approach, orthogonality is enforced as a Rayleigh Ritz procedure and is used once each eigenvector is modi£ed. ABINIT offers a block version of the same algorithm (with parallelism across the different vectors in the block) and an alternative which minimizes residual norms.
The Vienna Ab-initio Simulation Package (VASP), [88, 89] uses three main diagonalization schemes. The £rst, similar to ABINIT, is a form of subspace iteration in which the wave functions are individually re£ned by either the Conjugate Gradient (CG) Algorithm or by a method called the Direct Inversion in the Iterative Subspace (DIIS) iteration. The CG method is adapted from a method suggested by Teter et al. [171]. It consists of a preconditioned CG algorithm for minimizing the Rayleigh quotient. The preconditioner is an astutely adjusted diagonal matrix in plane-wave space where the kinetic part of the Hamiltonian is diagonal. A few steps of this scheme are applied
25

Code
PWscf PEtot ABINIT Soccoro PARSEC
fhi98PP VASP PARATEC SeqQuest* Wien2K ACRES SIESTA AIMPRO FLEUR CPMD CONQUEST* CASTEP DoD PW * CRYSTAL Octopus MIKA

Discr.
PW PW PW PW+RS RS
PW PW PW LCAO APW RS LCAO Gauss. APW PW RS/OTH PW PW Gauss RS RS

URL
http://www.pwscf.org/ http://crd.lbl.gov/˜linwang/PEtot/PEtot.html http://www.abinit.org/ http://dft.sandia.gov/Socorro/mainpage.html http://www.ices.utexas.edu/˜mtiago/parsec/software/index.html
http://www.fhi-berlin.mpg.de/th/fhi98md/fhi98PP/ http://cms.mpi.univie.ac.at/vasp/vasp/vasp.html http://www.nersc.gov/projects/paratec/ http://dft.sandia.gov/Quest/ http://www.wien2k.at/ http://cst-www.nrl.navy.mil/˜singh/acres/info.html http://www.uam.es/departamentos/ciencias/£smateriac/siesta/ http://aimpro.ncl.ac.uk/ http://www.¤apw.de/ http://www.cpmd.org/ http://www.cmmp.ucl.ac.uk/˜conquest/ http://www.tcm.phy.cam.ac.uk/castep/ http://cst-www.nrl.navy.mil/people/singh/plane wave/v3.0/ http://www.cse.clrc.ac.uk/cmg/CRYSTAL/ http://www.tddft.org/programs/octopus/ http://www.csc.£/physics/mika/index.html

Table 2: A few available DFT computational codes for electronic structures.

to each vector of the basis and once this is done the new basis is orthogonalized in preparation for a Rayleigh-Ritz projection. The process is repeated until self-consistence. Note that ABINIT uses a variant of this scheme as well. A major drawback of this scheme is the requirement to always orthogonalize the current (preconditioner) residual vectors in CG against all other eigenvectors. This is necessary because the method essentially consists of minimizing the Rayleigh quotient in the space orthogonal to all other eigenvectors. Without it, the method would only compute one eigenvalue, namely the smallest one.
The second method in VASP avoids this problem by minimizing (A − µ(u)I)u 2 instead of the Rayleigh quotient. This represents the norm of the residual vector, hence the name Residual Minimization Method (RMM). The minimization itself is done with the Direct Inversion in the Iterative Subspace (DIIS) method, which is a form of Krylov subspace method due to Pulay [140] in the early 1980’s. 1 In the second scheme employed by VASP, an initial subspace is selected and then an iterative scheme is invoked to ‘improve’ the basis vectors individually by minimizing the residual norms. In this approach, there is no need to orthogonalize each vector against all others after each update to the basis vectors. Finally, the third alternative method proposed in VASP is the Davidson approach with a form of block preconditioning. This is recommended as a more robust alternative in the documentation, though it is also mentioned as being more costly in some cases.
1It is remarkable to note here, that this work parallels the work of many others in scienti£c computing working on solving (nonsymmetric) linear systems of equations, who were not aware of this development.

26

This approach will be revisited in the next section. The PWscf code (part of Espresso) [6], uses similar general methods to VASP. The default
diagonalization algorithm is the Davidson method. There are also subspace-type methods offered with CG-like band-by-band diagonalization, or DIIS-like diagonalization.
The Octopus code focuses on time-dependent density functional theory and can handle excited states. Recall that DFT is only applicable to the ground state. Octopus can also obtain static properties generally computable from DFT codes, such as static polarizabilities and ground-state geometries, but the authors warn that the code is not optimized for this purpose.
MIKA stands for Multigrid Instead of the K-spAce and is a relatively recent addition to the list of computational codes available [71, 176]. MIKA works in real space and uses a Multigrid approach for diagonalization. The methods in MIKA are once more inspired from subspace iteration; the main difference being that CG or DIIS, is replaced by a multigrid approach. As the levels are crossed, there is no orthogonalization at each level.
Quite a few papers in the early to mid-1990’s were devoted to using the standard conjugate gradient algorithm by a more elaborate scheme which does not impose the constraint of orthogonality, see, e.g., [2, 40, 53, 107, 108, 131, 169–171] for a few references. Since these methods are more akin to optimization we discuss them in the next section. A number of codes, e.g., SIESTA, adopted variants of these schemes.
It was observed by many that the Davidson approach is in fact more robust than methods based on local optimization. For example, the authors of [82], comment that “For relatively small submatrix sizes the Davidson method still gives correct results whereas the latter two frequently fail to do it.” The other two methods to which the paper refers are a form of subspace iteration (modi£cation of RITZIT code) with DIIS preconditioning and a form of conjugate gradient minimization. The observation that Davidson is a more robust approach is not a unanimous viewpoint. For example developers of PWscf and VASP seem to recommend direct minimization, in spite of a less favorable speed. Clearly, implementation is a key factor. We believe that with proper implementation, a Davidson or Krylov-based approach should be vastly superior to direct minimization.
6 The optimization path: Avoiding the eigenvalue problem
From one viewpoint, there is no need to refer to eigenvalues in order to minimize the total energy, and this provided a number of alternative methods used in electronic structures. Indeed, the stationary states of (5) are eigenfunctions of the Hamiltonian, but one can also just consider (4) as an optimization problem by itself.
6.1 Optimization approaches without orthogonality
In reading about computational schemes that have been proposed in the literature on DFT, one cannot miss to observe that the most commonly-mentioned challenge or burden is that associated with the need to orthogonalize a given basis which approximates the desired eigenbasis. It is therefore only natural that major efforts have been devoted to designing algorithms which do not require orthogonalization, or that attempt to reduce its cost. A number of these algorithms have been exploited in the context of Order-N, (O(N )) methods as well is in standard methods. The end of the previous section alluded to this approach, which seeks to compute a subspace as represented
27

by a basis. If the basis, call it V = [v1, · · · , vm], is orthogonal, then the problem of minimizing the energy is equivalent to that of minimizing the trace of V∗AV. Thus, it is possible to formulate the problem to that of computing a set of vectors such that tr(V∗AV) is minimized subject to the constraint V∗V = I. Note that an algorithm for explicitly minimizing the trace has been developed by Sameh [151] as far back as in 1982, motivated in part by parallelism, but this does not seem to have been noticed by researchers on the applied side.
Many authors have considered a related approach in which the orthogonality constraint is not enforced. In this situation, the problem is equivalent to minimizing S−1A or S−1/2AS−1/2 where S is the “overlap” matrix, i.e., the matrix S = V∗V; the “overlap” matrix, is only approximately inverted. For example, the simple Neumann-series expansion
k
S−1 ≈ Si
i=0
can be used [2, 53, 107, 108, 170]. The paper [40] examines in detail algorithms that minimize energy on Grassman and Stiefel
manifolds, i.e., manifolds of matrices that satisfy orthogonality constraints. In these algorithms, the iterates evolve by following geodesic paths on the manifolds (ideal case). The cost of the ideal case algorithm requires the Singular Value Decomposition (SVD, see [62]) of matrices of size n × p (the same size as that of the basis of the desired subspace), and so the authors of [40] show that quadratic convergence can be achieved if the directions used by the algorithms are only approximate. Other approaches taken consist of making use of the McWeeny [110] projection which will be discussed shortly.

6.2 Density matrix approaches in DFT
As was previously discussed, one can bypass the eigenvalue problem and focus instead on the whole subspace spanned by the occupied eigenvalues as an alternative to computing individual eigenfunctions. We also mentioned this viewpoint in the standard context of eigenvector-based methods when we discussed adapting the Lanczos algorithm for DFT. However, the methods that rely on the density matrix approach go much further by not even referring to eigenspaces. Instead they attempt to compute directly the eigenprojector associated with these eigenvalues.
Note that after discretization, the function ψ becomes a vector of length N whose ith component is the approximation of ψ at the mesh-point ri. If we call V the matrix whose column vectors are the (normalized) eigenvectors ψi, i = 1, . . . , s, for the s occupied states, then

P = VV∗

(39)

is a projector, and the charge density at a given point ri in space is the ith diagonal element of P. A number of techniques have been developed based on using this observation [7, 96, 163]. Here,
we will only sketch the main ideas.
Density matrix methods are prevalent mainly in the O(N ) methods. These methods are based on constructing an approximation to the projector P in (39) without knowledge of eigenvectors. Denote by pij the coef£cients of the matrix P. A number of properties are exploited for this

28

purpose. First, are the following two relations:

tr[P] = tr[PH] =

pii = particle number
i
pijHji = system energy.
i,j

The £rst relation is a consequence of the fact that each pii represents the charge density at point ri as was mentioned above. The second is a consequence of the fact that PH = PHP is the restriction of H to the invariant subspace associated with eigenvalues corresponding to the occupied states. The trace of PH is the sum of these eigenvalues, which is the total energy of the system assuming a “Hartree” approximation, i.e., assuming the total energy is the sum of the occupied eigenvalues.
Another important property that is exploited is the physical fact that entries of P decay away from the main diagonal. Hence the idea is to try to £nd a P whose trace is £xed and which minimizes the trace of PH. The trace constraint can be avoided by shifting H

tr[P(H − µI)] = tr[PH] − µNe
The optimization problem is not yet well-posed, since without constraints on P, the minimum can be arbitrarily negative or small. The missing constraint is to force P to be a projector. This can be achieved by forcing its eigenvalues to be between zero and one [96]. The minimization will yield a matrix P, which has eigenvalues equal to either one or zero, and satisfying the desired idempotent constraint automatically.
One strategy that has been used in [96] for this purpose is to seek P in the form
P = 3S2 − 2S3
If the eigenvalues of S are in the range [−0.5, 1.5] this transformation will map them into [0,1]. The procedure then is to seek a banded S that minimizes
tr[(3S2 − 2S3)(H − µI)]
using descent-type algorithms. The gradient of the above function is computable. This is referred to as McWeeny puri£cation [110].
The drawback of this approach is its lack of accuracy. It is also not clear if a minimum exists, because of the various constraints added, and if there is a (unique) minimum. In addition, the band required for S may not be so small for certain systems. Nevertheless, it is interesting to note that explicit eigenvalue calculations are avoided. Some global approximation of the invariant subspace associated with these eigenvalues is extracted, meaning that a global approximation to the set of eigenfunctions is computed via the approximation to the projector P.

6.3 Density matrix approaches in Hartree-Fock
Charge-density methods have played a major role since the early days of the quantum theory. Such methods were used in particular by Thomas [174] and Fermi [44] as far back as in 1927. These were among the £rst realistic attempts at yielding the atomic structure of atoms. They

29

gave qualitatively correct information on the electronic structure of atoms, but were fundamentally ¤awed as they did not describe the observed shell structure of the atom.
Modern density methods are based on the “density matrix.” The density matrix can be viewed as a function of a pair of coordinates: ρ(r, r′). It is de£ned by

N

ρ(r, r′) = ψi(r)∗ψi(r′) .

(40)

i=1

It has been known for quite some time that the computation of eigenfunctions can be avoided and replaced by computations involving the density matrix. For example, one of the implementations of the Hatree-Fock method, known as the Roothan method, involves a self-consistent (£xed point) iteration in which the unknown is the density matrix P. Speci£cally, each wave function is expressed in a basis {χk}, k = 1, · · · , K,

K

φk = cjkχj .

(41)

j=1

Formally, we would like to minimize the Hartree-Fock energy (15) with the constraint that the
orbital be of the above form. The χj’s are convenient and well-selected spatial basis orbitals associated with the atoms (Atomic Orbitals). For the purpose of simplifying notation we de£ne
H to be the Fock operator in expression (15) for a particular i, say i = 1. Then a Galerkin-type approach would be to write Hφk = εkφk in the space spanned by the φj’s:

K
χi | H | ckjχj

K
= εk χi| cjkχj

for i = 1, · · · , K →

j=1

j=1

K

K

χi|H|χj cjk = εk

χi|χj cjk for k = 1, · · · , K

(42)

j=1

j=1

If we denote by F the Fock matrix whose entries are Fij =< χi|H|χj > and by S the matrix with entries Sij =< χi|χj > then, it is clear that (42) is a generalized eigenvalue problem of size K. An eigenvector is a column of the K × K matrix C with entries cjk on the kth column and jth row. In matrix form the problem can be written as

FC = CSW

where W = diag(εk) is the diagonal matrix with entries εk. This problem can be solved with standard techniques for dense problems. The matrix C is such that CT SC = I. It is also of interest to look at the charge density in this context. The procedure can be written in terms of the density
matrix which is de£ned as P = CC T . Eq. (41) can be rewritten in the form

 φ1(r)

 χ1(r)

  


φ2...(r)

  


=

CT

  


χ2...(r)

  


φN (r)

χN (r)

30

Since the charge density at a location r is the 2-norm of the above vector, then clearly

 χ1(r)

ρ(r)

=

[χ1(r)∗,

χ2(r)∗,

·

·

·

χN (r)∗]CCT

  


χ2...(r)

  


.

χN (r)

Note that the matrix S depends only on the basis {ξk} selected, while F depends on the solution itself. So the problem is again a nonlinear one which needs to be solved by a self-consistent iteration of the form
1. Given {χk}k=1,..,K construct S. Get an initial set {φj}j=1,...K. 2. Compute the new matrix F. 3. Solve the eigenvalue problem F SC = SCW. Compute P = CCT . 4. If P has not converged then return to 2.
Details on the computation of F are complex, but it is useful to mention that this matrix consists of two parts, which arise by splitting, i.e., we can split H in two parts:

H = Hcore + F ,

where Hcore corresponds to the £rst 2 terms in (20) and does not involve the φj’s, and F contains the part which depends on the φi’s. If we write the general matrix term gij = φi|H|φj , then we obtain

gij = φi|Hcore|φj + φi|F |φj .
So, the matrix G is the sum of two matrices, the £rst of which is constant with respect to the φi’s, and the second, call it Fφ, is itself a function of the φis. As can be seen from expression of Fφ, this will involve double-electron integrals of the form

drdr′

χ∗i (r)χj(r)χ∗k(r′)χl(r′) |r − r′|

.

The cost of the procedure is dominated by the fact that there are many such integrals to evaluate.

6.4 The “Car-Parrinello” viewpoint
Car and Parrinello [19] took an approach which combined molecular dynamics with pseudopotentials and DFT by proposing a scheme that exploits heuristic optimization procedures to achieve the minimum energy. Speci£cally, they used simulated annealing [84] to minimize an energy functional, which they write in the form:

E[{ψi}, {RI}, {αv}] =

d3ri ψi∗[−( 2/2m)∇2]ψi(ri) + V[ρ(ri), {RI }, {αv}],

iΩ

where {αv} are the external constraints. V contains the internuclear Coulomb repulsion and the effective electronic potential energy, which includes the external nuclear, Hartree, and exchange
and correlation terms.

31

They then use a Lagrangian formula to generate trajectories for ionic and electronic degrees
of freedom via a coupled set of equations of motion. The idea is to propagate the electronic wave
functions, i.e., the Kohn-Sham orbitals along the motions of the atoms. To this end, they consider
the parameters {ψi}, {RI}, {αv} in the energy functional to be time-dependent and introduce the Lagrangian, L, which is the difference in the kinetic and potential energies of the system:

L=

i

1 2

µ

d3ri |ψ˙i|2 +
Ω

I

1 2

MI

R˙ 2I

+

v

1 2

µvα˙ v2

−

E

[{ψi},

{RI

},

{αv }],

where MI are the physical ionic masses, µ and µv are arbitrary parameters having the appropriate units, and the ψi are subject to an orthonormality constraint.
The Lagrangian generates dynamics for the parameters through the following equations of
motion:

µψ¨i(ri, t) = −δE/δψi∗(ri, t) + Λikψk(ri, t)

(43)

k

MI R¨ I = −∇RIE

(44)

µvα¨v = −(∂E/∂αv),

(45)

where the Lagrange multipliers, Λik, have been introduced to satisfy the orthonormality constraint. Only the ion dynamics have real physical meaning; the rest are £ctitious and are employed by the simulated annealing algorithm.
The Lagrangian formula de£nes both potential and kinetic energies for the system. The equilibrium value of the kinetic energy can be calculated as the temporal average over the trajectories generated by the equations of motion. By varying the velocities, the temperature of the system can be reduced; as T → 0, the equilibrium state of the DFT energy functional is reached. At equilibrium, ψ¨i = 0, and (43) corresponds to the Kohn-Sham equation through a unitary transformation. At this temperature, the eigenvalues of Λ agree with the occupied Kohn-Sham eigenvalues, and the Lagrangian describes a real physical system whose representative point lies on the BornOppenheimer surface.
The main advantage of this approach is that diagonalization, self-consistency, ionic relaxation, and volume and strain relaxation are achieved simultaneously rather than separately.
Pastore et al. investigated the theoretical basis of the Car-Parrinello method in [128]. There they showed how the classical dynamics generated by the Car-Parrinello Lagrangian approximated ef£ciently the quantum adiabatic evolution of a system, and they discuss the role played by the spectrum of the eigenvalues of the Kohn-Sham Hamiltonian matrix.
The Car-Parrinello method is one of several ab initio molecular dynamics (AIMD) methods. A discussion of AIMD methods is beyond the scope of this paper. However, the interested reader is referred to [54, 125, 129, 178] for descriptions of AIMD methods and their diverse applications which include the melting of silicon and the study of molecular crystals and liquids.
The Car-Parrinello method has been used extensively in materials science, physics, chemistry, and biology since its invention. The Car-Parrinello Molecular Dynamics Consortium website [172] lists numerous papers that have been published on this method since 1994; however, this list is not exhaustive. To give an idea of the wide range of applications studied by this method, we list several papers in materials science [32, 78, 106, 109, 185], physics [14, 20, 46, 143, 154], chemistry [23,

32

57, 87, 137, 184], and biology [56, 80, 113, 132, 146] that employ this method. For information on recent advances in chemistry and materials science with Car-Parinello molecular dynamics methods, see [1]. For a nice review of the £rst 15 years of the use of the Car-Parrinello methods in physics, chemistry, and biology, the reader is referred to [145].

6.5 Use of orthogonal polynomials
Approximation theory, and especially the theory of orthogonal polynomials, has been extensively used in density functional theory. A typical approach is to write the projector (39) as
P = h(H)
where h(λ) is the shifted Heaviside step function which has value one for λ ≤ EF and zero elsewhere. The Heaviside function can now be approximately expanded into orthogonal polynomials
n
h(λ) ≈ αipi(λ) .
i=1
The most common orthogonal polynomials that are used in this context are the Chebyshev polynomials of the £rst class. If a good basis is used, such as Gaussian orbitals or plane waves, then it is known that the density matrix has a £nite decay range, i.e., it will be represented by a sparse matrix. It is therefore possible to construct a good approximation to P ef£ciently in this case; see, e.g., [77, 98].
Another approach which has been used focuses not on P directly, but rather on the basis V; see, e.g., [74]. Here a number of trial vectors ξ1, . . . , ξm are selected and then a basis for the desired space span(V) is built by computing approximations to the vectors
wi = h(H)ξi
with the help of the polynomial expansion. Note that each of these vectors is a member of the desired subspace and collectively the set of wi’s will constitute a basis of the space under generic conditions. This set is then orthonormalized to get V.
The above approaches attempt to extract the charge density ρ(r, r), which is represented by the diagonal of the operator P. One can easily imagine that while the techniques should scale well for large systems, the prefactor in the cost function can be very high. This was observed in [77]. There are, however, situations where the use of orthogonal polynomials can be very cost-effective. In [144] the density of states (DOS) is computed using this strategy. One starts with a density of eigenvalues in Rn which in the form of a sum of Dirac functions:
N
η(λ) = δ(λ − λi) .
i=1
This is a distribution (in a mathematical sense) that is sought. The classical moment problem is to £nd this distribution from a knowledge of its classical moments µk = λkη(λ)dλ.
A numerically viable alternative is to use the modi£ed or generalized moments:

µk = tk(λ)η(λ)dλ

(46)

33

where {tk} is a sequence of orthogonal polynomials on the interval containing the eigenvalues of H. Typically, the problem is shifted and scaled so that the action takes place in the interval
[−1, 1]. In addition, the polynomials tk are just the Chebyshev polynomials, though other polynomials can be used as well. Assuming that the moments have been computed for the Chebyshev
polynomials, then expression of the distribution in the interval (−1, 1) is given by

η(λ)

=

√2 π 1−

λ2

∞ k=0

µk 1 + δk0

tk(λ)

.

(47)

Here δk0 is the Kronecker symbol. Of course, the sum is truncated at M terms resulting in a certain
function ηM (λ). Another notable approximation lies in the computation of the moments µk. The sequence µk is not readily available and can be only approximated [144,181]. The various methods proposed in the literature consist of using probabilistic arguments for this purpose. Speci£cally, µk can be written as

1

N

µk = tk(λ)η(λ)dλ = tk(λi) = tr[tk(H)]

(48)

−1

i=1

so one needs only compute the traces of the sequence of operators tk(H). These traces are typically computed with a Monte-Carlo type technique. A sequence of nr random vectors rj is generated and µk is approximated by

µk

≈

1 nr

nr j=1

< rj|tk(H)|rj

>.

The 3-term recurrence of the Chebyshev polynomials is exploited to reduce the memory and computational cost of the calculations.
This type of calculation for the DOS can only be of interest in cases where the geometry is £xed and the Hamiltonian can be well-approximated without a self-consistent iteration. A related, but more complex, technique allows to obtain optical-absorption spectra (OAS) [24, 181]. The calculation can be viewed as an extension of the problem discussed above to two variables. What is sought is the 2-variable distribution:

η(λ1, λ2) = |σi,j|2δ(λ1 − λi)δ(λ2 − λi) with σi,j =< φi|i ∇|φj >

(49)

i,j

from which the OAS can be obtained by computing a double integral [181]. Physically, |σi,j|2 represents the transition probability between states i and j. To compute the function (49) requires
to generalize the 1-variable moments de£ned by (48) to two 2-variable moments:

1

1

µk,l =

tk(λj)tk(λi)tl(λj)|σi,j|2 = dλ1 dλ2 tk(λ1)tl(λ2)η(λ1, λ2) .

i,j

−1

−1

A probabilistic technique that is similar to the one for the density of states is used to recover an
approximation to the function (49). In this case, approximating µk,l consists of averaging terms of the form < r|i ∇tk(H).i ∇tk(H)|r > where r is a random vector as before [181].

34

7 Geometry optimization
The composition and structure of a material (including its geometry) determine many of its physical and chemical properties. For example, the reactivity, polarity, phase, color, and magnetism of a material are determined, in large part, by the material’s geometry [111]. Thus, it is important to determine the geometry of the material in a stable state.

7.1 The geometry optimization problem
The geometry optimization problem (referred to as the structural relaxation problem by materials scientists and computational physicists) is to £nd a low-energy con£guration of the material. This is done by repositioning the atoms of the material and subsequently evaluating its energy at various places on the potential energy surface. The potential energy surface is a hypersurface and is a plot of the potential energy, E, vs. the atomic coordinates, r. Here r is a vector of length 3K containing the atomic coordinates for the K atoms, i.e., r = (x1, y1, z1, . . . , xK, yK, zK)T . There have been many reviews, see, e.g., [141, 155–158] written on this topic in recent years.
There are some applications which require the lowest-energy conformation of the system of interest, i.e., a global minimum of the potential energy surface [158]. Finding a global minimum is a very dif£cult optimization problem; often it is impossible or impractical to £nd the global minimum. However, there are many applications where it is enough to £nd a local minimum. For example, local minima can be used as starting points for global minimization algorithms [8, 9, 22, 86, 95, 97, 133, 134, 152, 153, 179].
Even though £nding a local minimum is an easier problem than £nding a global minimum, it can be quite dif£cult. One problem that may occur is that the optimization algorithm may become trapped at a saddle point, for example. Because the gradient is zero at all critical points, the Hessian must be used to determine whether or not the critical point is a local minimum. At a local minimum, the Hessian matrix is positive de£nite. A second problem is that the optimization algorithm may not converge from all starting points, especially not from those points corresponding to very high potential energies. The methods we review in this paper focus on £nding a local minimum (as opposed to the global minimum).
The geometry optimization problem is a nonlinear, unconstrained optimization problem. In optimization terms, the objective function and gradient are given by the potential energy and forces, respectively. There are four important qualities which serve to characterize the optimization problem. First, the objective function is highly nonlinear. For example, a simple model that is often given for the problem is the Lennard-Jones potential which describes the potential energy of two inert gas atoms in locations r1 and r2:

E(r1, r2) = 4ǫ

σ |r1 − r2|

12
−

σ |r1 − r2|

6

.

Here ǫ is the well-depth, and σ is a hard sphere radius. Second, it is very expensive to evaluate the energy and forces of the material for a particular
geometry. For the PARSEC package [126], on which the authors work, the self-consistent £eld iteration, corresponding to the solution of the Kohn-Sham equations, must be executed each time the energy and forces are evaluated. This corresponds to solving the nonlinear eigenvalue problem

35

Code
PWscf PEtot ABINIT Soccoro fhi98PP VASP PARATEC SeqQuest Wien2K ACRES SIESTA AIMPRO FLEUR CPMD CONQUEST CASTEP DoD PW CRYSTAL Octopus MIKA

Minimization algorithm
damped dynamics BFGS molecular dynamics (Numerov, Verlet) steepest descent, conjugate gradient, quenched minimization ionic relaxation with damped Verlet conjugate gradient, RMM-DIIS direct minimization of total energy modi£ed Broyden, damped dynamics, steepest descent, accelerated steepest descent geometry optimization details not given constrained dynamics conjugate gradient, molecular dynamics algorithms (including standard Verlet) conjugate gradient geometry optimization details not given GDIIS, L-BFGS, P-RFO, RFO, BFGS, steepest descent geometry optimization details not given BFGS, damped molecular dynamics, delocalized internal coordinates damped dynamics Berny (modi£ed conjugate gradient) algorithm steepest descent BFGS

Table 3: Minimization algorithms for a few electronic structures codes.

in (31). For this reason, it is impractical to compute the Hessian via £nite-differencing, and it is not possible to compute the Hessian via automatic differentiation due to the structure of the nonlinear eigenvalue problem. Third, the energy and force £elds often contain inaccuracies, as it is dif£cult to know the potential energy surface exactly. Finally, there can be many local minima; any of them will be considered acceptable solutions to the geometry optimization problem.

7.2 Minimization algorithms
Many different minimization algorithms are employed by electronic structures packages to solve the geometry optimization problem. Table 3 gives the type of minimization algorithm used by each DFT electronic structure package listed in Table 2. These minimization algorithms fall into six main categories: the steepest descent method, quasi-Newton methods, truncated Newton methods, conjugate gradient methods, iterative subspace methods, and molecular dynamics methods. In this section, we describe the £rst £ve classes of methods and review algorithms from each category. The sixth category, molecular dynamics methods, are beyond the scope of this paper, as they are really simulation methods rather than unconstrained optimization algorithms.

36

7.2.1 The steepest descent method
Steepest descent methods exploit the direction, d, that yields the fastest decrease in E from r. Mathematically, this direction results from the solution of the optimization problem

min ∇E(r)T d subject to d = 1 .

(50)

d∈R3K

The solution to the above minimization problem is given by d = −∇E(r)/ ∇E(r) 2 in the l2norm. This is the steepest-descent direction.
Iteration k of Cauchy’s classical steepest descent algorithm [38] is as follows:

1. Find the lowest point of E in the direction −∇E(rk) from rk, i.e., £nd λk that solves minλk>0 E (rk − λk∇E (rk)).
2. Update rk+1 = rk − λk∇E(rk).
Clearly, this is only a theoretical algorithm in that the £rst step requires the solution of a onedimensional minimization problem. In order to implement the algorithm, an inexact minimization must be performed. Goldstein [61] showed that under mild conditions, the steepest descent algorithm converges to a local minimum or a saddle point of E. However, the convergence is only linear.
Several electronic structure codes (e.g., Socorro, SeqQuest, CPMD, and Octopus) employ the steepest descent algorithm for the geometry optimization problem because it is easy to implement. The ease of implementation comes at the cost of slow convergence. More sophisticated minimization algorithms usually yield a better convergence rate and are more cost-effective.

7.2.2 Newton’s method

An example of an optimization algorithm with a higher rate of convergence is Newton’s method.

It enjoys quadratic convergence in the best case. The goal of Newton’s method is to £nd a point

rk+1 such that ∇E(rk+1) = 0. For such a point, rk+1 satis£es a necessary condition for being a

minimizer of E. In order to do this, a quadratic model, mk, of the function is created. This model

is

given

by

mk(rk

+

d)

=

E (rk )

+

∇E(rk)T d

+

1 2

dT

∇2E (rk )

d.

Then

the

point

rk+1

=

rk

+ dNk

is determined so that ∇mk(rk+1) = 0, making rk+1 a critical point of mk. The vector dNk is called

the Newton step.

Formally, iteration k of Newton’s method is written as the following two step procedure:

1. Solve ∇2E(rk) dNk = −∇E(rk).
2. Update rk+1 = rk + dNk .
There are many dif£culties associated with this simple version of Newton’s method. First, the Newton direction might be a direction of negative curvature, i.e., it might not be a descent direction. Second, if the Hessian matrix is ill-conditioned, the rate of convergence might be decreased. Third, Newton’s method is not globally convergent. Another major dif£culty associated with using Newton’s method for electronic structure calculations is that, in most cases, an analytic Hessian is not available. Newton’s method (as written) is not a viable option for electronic structure codes. As such, we turn to quasi-Newton methods, which employ approximate Hessians.

37

7.2.3 Quasi-Newton methods

Quasi-Newton methods are among the most successful approaches for the geometry optimization

problem [155, 156, 158, 180]. The PEtot, SeqQuest, CPMD, CASTEP, and MIKA electronic struc-

ture packages all employ quasi-Newton methods, which are modi£ed Newton methods in which

the actual Hessian is replaced by an approximation to it. Typically, the approximation is obtained

by updating an initial Hessian which may be a positive multiple of the identity, or it may come

from experimental results, or from optimizing the geometry at a higher level of theory. For ex-

ample, if one is interested in performing a geometry optimization for the DFT level of theory, it

may be possible to initialize the Hessian using the Hessian from a geometry optimization with a

semi-empirical force £eld. Another way of initializing the Hessian may be to use the Hessian from

the geometry optimization of a related model problem.

The generic quasi-Newton method is the same as Newton’s method except that ∇2E(rk) is replaced by Bk ≈ ∇2E(rk) in the computation of the Newton step. One way in which the many quasiNewton methods differ is in their techniques for updating the Hessian. One major class of Hessian

update formulas are the secant updates which enforce the quasi-Newton condition Bk+1sk = yk, where sk = rk+1 − rk and yk = gk+1 − gk, where gk = ∇E(rk). This condition is used to create low-rank approximations to the Hessian.

One of the most successful updates from this class has been the BFGS [18, 47, 60, 164] update

which was discovered independently by Broyden, Fletcher, Goldfarb, and Shanno in 1970. It is

given by

Bk+1

=

Bk

+

yk ykT ykT sk

−

Bk sk sTk Bk sTk Bk sk

.

This is a rank-two, symmetric secant update. In addition, Bk+1 is positive de£nite if ykT sk > 0 and Bk is positive de£nite. (The former condition is the only one to be concerned with, as ∇2E0 is usually a positive multiple of the identity. The update is normally skipped whenever ykT sk ≤ 0.) This is a desirable feature of a Hessian update since the Hessian matrix is positive de£nite
at a minimum. Thus, we seek positive de£nite Hessian update methods for minimizations of a

quadratic model. One dif£culty with the BFGS update is that, within the linesearch framework (to

be discussed below), it only converges to the true Hessian if accurate linesearches are used [38].

Owing to inaccuracies in the potential energies and forces, this is often not the case in geometry

optimization problems.

A competitive update which converges to the true Hessian on a quadratic surface without an

exact linesearch is the symmetric rank-one update by Murtagh and Sargent [116]. This update is

given by

Bk+1

=

Bk

+

(yk

− Bksk)(yk − Bksk)T (yk − Bksk)T sk

.

Unfortunately, this is not a positive-de£nite update, and sometimes the denominator becomes van-

ishingly small. However, the diagonal of the approximate Hessian may be perturbed to make the

approximate Hessian positive de£nite at a given step.

A third update which has been useful for geometry optimization problems is to take a speci£c

convex combination of the SR1 update shown above and the BFGS update. Speci£cally, this update

38

is given by

Bk+1

= Bk + φ

(yk − Bk sk)(yk − Bk (yk − Bk sk)T sk

sk )T

+ (1 − φ)

−

Bk sk sTk Bk sTk Bk sk

+

yk ykT ykT sk

,

where φ is given by t1/t2t3, where t1 = ((yk − Bk sk)T sk)2, t2 = (yk − Bk sk)T (yk − Bk sk), and t3 = sTk sk. This update is due to Farkas and Schlegel [41] and is based on an idea of Bo£ll’s [13] for locating transition-state structures. This update strives to take advantage of the

positive-de£niteness of the BFGS update and the greater accuracy of the SR1 update. Other Hes-

sian updates are also possible; see [38] for a description of several others.

The second way in which quasi-Newton methods differ is their techniques for controlling the

Newton step. Linesearch methods [38, 48, 58, 81, 121] attempt to choose a steplength, λk, such that the quasi-Newton step given by rk+1 = rk + λkdk satis£es suf£cient decrease and curvature conditions. One of the most successful linesearch codes is the limited-memory BFGS code, L-

BFGS, which was implemented by Liu and Nocedal [100, 120]. It is intended for large-scale

optimization problems. In the L-BFGS code, quasi-Newton update vectors, rather than the full

Hessian matrix, are stored. When the available storage has been depleted, the oldest correction

vector is removed to make room for a new one. The step length in this code is determined by the

sophisticated More´-Thuente linesearch [114].

In contrast with linesearch methods, trust-region methods choose the direction and step length

by minimizing the quadratic model subject to an elliptical constraint. The constrained minimiza-

tion

problem

they

solve

is

given

by:

min mk(rk

+ dk)

=

E(rk) + ∇E(rk)T dk

+

1 2

dTk

Bk

dk,

subject to dk 2 ≤ δk, where δk is the trust-region radius. The resulting step is of the form

dk = −(Bk + λkI)−1∇E(rk). The trust-region radius is adjusted based on how well the quadratic

model approximates the function. See [30, 38, 48, 58, 81, 121] for more details on trust-region

methods.

The rational function optimization (RFO) method [5, 168] is related to trust-region methods in

that it seeks to compute a step in a direction that will improve the convergence of the method. In

this method, the quadratic model found in Newton’s method is replaced with a rational function

approximation. In particular,

1 2

1 dTk

∆E = E(rk + dk) − E(rk) ≈

(1 dTk )

0 gkT gk Bk
1 0T 0 Sk

1 dk , 1 dk

where Sk is a symmetric matrix that is normally taken to be the identity. Observe that the numerator in the above formula is the quadratic model used in the quasi-Newton method. The displacement
vector, dk, is computed so as to minimize ∆E. For further details on solving this optimization problem, see [138].
Recent years have seen the development of hybrid methods for optimization based upon quasi-
Newton methods. One such example is the hybrid method by Morales and Nocedal [112] that
interlaces iterations of L-BFGS with a Hessian-free Newton method. The performance of this
method is compared with the L-BFGS method and a truncated Newton method at the end of the
next section.

39

7.2.4 Truncated Newton methods
If the exact Hessian is available, then it is possible to use a truncated Newton method. Truncated Newton methods are a subclass of Newton methods which are used in the context of large-scale optimization. Here an iterative method is used to compute the search direction, dk, using a linesearch or trust-region method. They are based on the idea that when far away from the solution, it does not make sense to compute an exact solution to the Newton equation, as this may be very computationally intensive and a descent direction may suf£ce. However, near a minimum, a more exact solution is desired. At each outer iteration, it is required that the residual, rk, satisfy rk = ∇2Ek dk + gk ≤ ηk gk , where ηk is the forcing sequence. The methods are called truncated Newton methods, as they are stopped (or truncated) when the above convergence criterion is met. For appropriately chosen ηk, asymptotic quadratic convergence of the method is achieved as ∇Ek → 0 [37]. One appropriate choice of ηk given in [158] is
ηk = min {cr/k, ∇Ek }, 0 < cr ≤ 1.
See [118] for an excellent survey of truncated Newton methods. Schlick and Overton developed the idea for a truncated Newton method which was used for
potential energy minimization in [161]. The resulting Fortran package, TNPACK [159, 160], written by Schlick and Fogelson, was later incorporated into the CHARMM [17] molecular mechanics package [39]. The user of TNPACK is required to implement a sparse preconditioner which alters the clustering of the eigenvalues and enhances convergence. Automatic preconditioning is included in an implementation by Nash [117] which makes it readily portable.
Das et al. [35] tested the performance of the Morales-Nocedal hybrid method (which was discussed in the quasi-Newton section), the Liu-Nocedal L-BFGS method, and the truncated Newton method with preconditioner of Nash on the protein bovine pancreatic trypsin inhibitor (BPTT) and a loop of protein ribonuclease A. Their results showed that the hybrid approach is usually two times more ef£cient in terms of CPU time and function/gradient evaluations than the other two methods [35].
7.2.5 Conjugate gradient methods
Nonlinear conjugate gradient (NLCG) algorithms [66, 165] form another important class of methods used in electronic structure packages (e.g., Socorro, VASP, SIESTA, AIMPRO, and CRYSTAL) for solving the geometry optimization problem. For an excellent survey paper on nonlinear conjugate gradient methods, see [66].
In the 1960’s, Fletcher and Reeves generalized the conjugate gradient algorithm to nonlinear problems [50] by building upon earlier work by Davidon [36] and Fletcher and Powell [49]. The nonlinear conjugate gradient algorithms were developed by combining the linear conjugate gradient algorithm with a linesearch. The nonlinear and linear conjugate gradient algorithms are related in the following way: if the objective function is convex and quadratic and an exact linesearch is used, then the nonlinear algorithm reduces to the linear one. This reduction is important since the linear conjugate gradient algorithm requires at most 3K steps in exact arithmetic. This is because the search vectors span the entire 3K-dimensional space after 3K steps.
NLCG algorithms are of the form:
rk+1 = rk + αkdk,
40

dk = −gk + βkdk−1, d0 = −g0,

where αk is obtained via a linesearch, and βk is a scalar that determines how much the previous direction is used in the computation of the current one. NLCG methods differ in their choice of βk; many different formulas have been proposed. Seven of the methods are: the Hestenes and Stiefel

method (HS) [72], the Fletcher-Reeves method (FR) [50], Daniel’s method (D) [34], the method by

Polak, Ribiere, and Polyak (PRP) [135,136], the Conjugate Descent method by Fletcher (CD) [48],

the Liu and Storey method (LS) [101], and the method by Dai and Yuan (DY) [33]; their formulas

for computing βk are as follows:

βkHS

=

gkT yk−1 dTk−1yk−1

,

βkF R =

gk gk−1

2

2

,

βkD

=

gkT+1 ∇2E (rk) dk dTk ∇2E (rk) dk

,

βkP RP

=

gkT yk−1 gk−1 2

,

βkCD

=

gk −gkT−1

2
dk−1

,

βkLS

=

gkT yk−1 −gkT−1dk−1

,

and

βkDY

=

gk 2 dTk−1yk−1

,

where yk−1 = gk − gk−1 and . is the l2-norm. The most popular formulas from the above list are FR, PRP, and HS. The FR method converges

if the starting point is suf£ciently near the desired minimum. On the other hand, PRP cycles in-

£nitely in rare cases; this undesirable behavior can be remedied by restarting the method whenever

βkP RP < 0. It is often the case that PRP converges more quickly than the FR method and is the one most often implemented in codes.

Recently Hager and Zhang [65, 67] developed a new nonlinear conjugate gradient method,

CG DESCENT, with guaranteed descent and an inexact linesearch. Their formula for computing

βk is given by

βkHZ =

yk

−

2dk

yk 2 dTk yk

gk−1 dTk yk

.

Numerical comparisons in [67] showed that CG DESCENT outperformed L-BFGS and several

other nonlinear conjugate gradient methods on a set of 113 problems from the CUTEr (The Con-

strained and Unconstrained Testing Environment, revisited) test set [173] with dimensions ranging

from 50 to 10,000. Thus, CG DESCENT should be seriously considered for the geometry opti-

mization problem.

Some of the best-performing nonlinear conjugate gradient methods today are hybrid meth-

ods [66]. These methods dynamically adjust the formula for βk as the iterations evolve. In [65],

41

several numerical experiments were performed which compared hybrid NLCG methods with CG DESCENT and L-BFGS. The top performers relative to CPU time were CG DESCENT, a code based upon a hybrid DY/HS scheme, and Liu’s and Nocedal’s L-BFGS code. Thus, the hybrid DY/HS scheme also has excellent potential for use on the geometry optimization problem.
Baysal et al. [10] studied the performance of several particular minimization algorithms as applied to models of peptides and proteins. In particular, they compared the performance of Liu’s and Nocedal’s L-BFGS code with the performances of the truncated Newton (TN) method with automatic preconditioner of Nash and the nonlinear conjugate gradient algorithm (CG) of Shanno and Phua. Their results [10] show that for one potential energy formulation, the truncated Newton method outperformed L-BFGS and CG by a factor of 1.2 to 2. With another potential energy formulation, L-BFGS outperformed TN by a factor of 1.5 to 2.5 and CG by a larger factor.
7.2.6 Iterative subspace methods
The £nal class of optimization methods we review are those that employ iterative subspace optimization. Electronic structure packages which employ iterative subspace methods include VASP and CPMD. One algorithm in this class is the Direct Inversion in the Iterative Subspace (DIIS) method [139, 140] which is also referred to as Residual Minimization Method-Direct Inversion in the Iterative Subspace (RMM-DIIS). DIIS is the same as a Krylov subspace method in the case of solving a linear system without preconditioning. The relationship between the methods in the nonlinear case is more complicated and is described in [68].
DIIS was £rst used to accelerate self-consistent £eld calculations before it was extended to the geometry optimization problem and to charge-mixing. The name of the method that has been speci£cally tailored for the geometry optimization problem is: Geometry Optimization in the Iterative Subspace (GDIIS) [31].
GDIIS is different from quasi-Newton methods in that it assumes a linear connection between the coordinate and gradient changes; this is similar to using a quadratic approximation to the potential energy surface. However, in the quasi-Newton case, the linear connection was between the Hessian matrix and the gradient.
We now give the derivation for the GDIIS method from [42]. The development of the GDIIS method is based on a linear interpolation (and extrapolation) of previous molecular geometries, ri, that minimizes the length of an error vector. The formula for the interpolation/extrapolation is given by:
r∗ = ci ri, where ci = 1.
An error vector, ei, is created for each molecular geometry using a quadratic model of the potential energy surface. First, a simple relaxation step, r∗i , is computed using a Newton step, i.e.,
r∗i = ri − ∇2E −1gi.
Then, the corresponding error vector, ei, is taken to be the displacement from the atomic structure:
ei = r∗i − ri = −∇2E −1gi.
The error (or residual) vector for r∗ is the linear combination of the individual error vectors and is given by:
z = ci ei = e∗.
42

Next, the coef£cients are obtaining by solving the least-squares problem which corresponds to minimizing z 2 subject to the constraint that ck = 1. Finally, the next atomic geometry in the optimization procedure is given by
rk+1 = r∗ + z = ci r∗i .
According to [42], this version of the GDIIS method is quite ef£cient in the quadratic vicinity of a minimum. However, farther away from the minimum, the method is not as reliable and can fail in three major ways: convergence to a nearby critical point of higher order, oscillation around an in¤ection point on the potential energy surface, and numerical instability problems in determining the GDIIS coef£cients. In [42], Farkas and Schlegel give an improved GDIIS method which overcomes these issues and performs as well as a quasi-Newton RFO method on a test set of small molecules. On a system with a large number of atoms, their improved GDIIS algorithm outperformed the quasi-Newton RFO method.
7.3 Practical recommendations
We conclude this section on geometry optimization with some practical recommendations. First, it is important to consider different starting points. A given optimization algorithm might not be globally convergent. It also might converge to another type of critical point such as a local maximum or a saddle point. The user can distinguish the type of critical point by calculating the eigenvalues at the solution. One example of a system in the literature where global convergence was not achieved with an optimization algorithm is the biphenyl molecule. When started from anything other than a ¤at geometry, the geometry optimization produced an acceptable result. However, when starting with the ¤at geometry, it produced a ¤at ring geometry which corresponds to a maximum [99].
Second, the user can try different algorithmic parameters, approximate initial Hessian matrices (in the case of quasi-Newton methods), and termination criterion, for example, as these can dramatically affect the algorithms’ convergence, as well. It can also be helpful to try using various optimization algorithms on one’s geometry optimization problem. Different optimization algorithms perform better on some problems and not as well on other problems as was demonstrated in this paper. Which algorithm will produce the best results for a given problem depends on several characteristics of the problem such as: deviation of the objective function from quadratic, condition number of the Hessian (or approximate Hessian) matrices, convexity, and eigenvalue structure. See [119] for a numerical study which compares the performances of the L-BFGS method, a truncated Newton method, and the Polak-Ribiere conjugate gradient method on a set of test problems and analyzes the results in terms of these quantities.
Finally, it may be worth to consider using a different coordinate system. In [3], Baker studied the use of Cartesian and natural internal coordinates (based upon the bonds and angles in the material) for geometry optimization; he concluded that for good starting geometries and initial Hessian approximations, geometry optimizations performed in Cartesian coordinates are as ef£cient as those using natural internal coordinates. Thus, the standard Cartesian coordinates are recommended for £nding local minima. However, for the case of no initial Hessian information, natural internal coordinates were more effective. Thus, natural internal coordinates are recommended for £nding a global minimum. See [4, 142] for alternative coordinate systems.
43

8 Concluding remarks
Though signi£cant progress has been made in recent years in developing effective practical methods for studying electronic structure of materials, there are from an algorithmic point of view many challenges remaining. Developing effective techniques for solving large eigenvalue problems in the case of a very large number of eigenvectors still remains an important issue. Interestingly, the large and dense eigenvalue problem will gain importance as systems become larger. This is because most methods solve a dense eigenvalue problem which arises from projecting the Hamiltonian into some subspace. As the number of states increases, this dense problem can reach sizes in the tens of thousands. Because of the cubic scaling of standard eigenvalue methods for dense matrices, these calculations may become a bottleneck.
In the same vein, as systems become larger, eigenfunction-free methods may start playing a major role. Although there has been much work done in this area (see, e.g., the survey [59], and [96]), linear scaling methods in existence today have limited applicability and it becomes important to explore their generalizations. There are also many questions to explore from a more theoretical viewpoint; see, e.g., [93] for an overview. Work needs to be done, for example, in gaining a better understanding of the relation between the choice of the exchange correlation functional and the nature of the resulting nonlinear eigenvalue problem. Thus, the self-consistent iteration is slow to converge in many cases (e.g., metallic compounds). It is known that such problems are intrinsically harder due to several factors, including the small gap between the eigenvalues of the occupied states and the others. In situations like these, it is intuitive that the solution will be more sensitive to small changes in the equations than in other cases. In particular, the solutions may depend more critically on the functional used for the exchange correlation energy.
References
[1] W. ANDREONI AND A. CURIONI, New advances in chemistry and material science with CPMD and parallel computing, Parallel Comput., 26 (2000), pp. 819–842.
[2] T. A. ARIAS, M. C. PAYNE, AND J. D. JOANNOPOULOS, Ab initio molecular dynamics: Analytically continued energy functionals and insights into iterative solutions, Phys. Rev. Lett., 69 (1992), pp. 1077–1080.
[3] J. BAKER, Techniques for geometry optimization: A comparison of Cartesian and natural internal coordinates, J. Comput. Chem., 14 (1993), pp. 1085–1100.
[4] J. BAKER AND P. PULAY, Geometry optimization of atomic microclusters using inversepower distance coordinates, J. Chem. Phys., 105 (1996), pp. 11100–11107.
[5] A. BANERJEE, N. ADAMS, J. SIMONS, AND R. SHEPARD, Search for stationary points on surfaces, J. Phys. Chem., 89 (1985), pp. 52–57.
[6] S. BARONI, S. DAL CORSO, S. DE GIRONCOLI, AND P. GIANNOZZI, PWSCF and PHONON: Plane-wave pseudo-potential codes. Available from http://www.pwscf.org.
[7] S. BARONI AND P. GIANNOZZI, Towards very large scale electronic structures calculations, Europhysics Letters, 17 (1992), pp. 547–552.
44

[8] C. BAYSAL AND H. MEIROVITCH, Ef£ciency of the local torsional deformations method for identifying the stable structures of cyclic molecules, J. Phys. Chem. A, 101 (1997), pp. 2185–2191.
[9] , Determination of the stable microstates of a peptide from NOE distance constraints and optimization of atomic solvation parameters, J. Am. Chem. Soc., 120 (1998), pp. 800– 812.
[10] C. BAYSAL, H. MEIROVITCH, AND I. M. NAVON, Performance of ef£cient minimization algorithms as applied to models of peptides and proteins, J. Comput. Chem., 20 (1999), pp. 354–364.
[11] T. L. BECK, Real-space mesh techniques in density functional theory, Rev. Mod. Phys., 74 (2000), pp. 1041–1080.
[12] C. BEKAS, Y. SAAD, M. L. TIAGO, AND J. R. CHELIKOWSKY, Computing charge densities with partially reorthogonalized lanczos, Tech. Rep. umsi-2005-029, Minnesota Supercomputer Institute, University of Minnesota, Minneapolis, MN, 2005.
[13] J. M. BOFILL, Updated Hessian matrix and the restricted step method for locating transition structures, J. Comput. Chem., 15 (1994), pp. 1–11.
[14] B. BRENA, D. NORDLUND, M. ODEIUS, H. OGASAWARA, AND L. G. M. PETTERSSON, Ultrafast molecular dissociation of water in ice, Phys. Rev. Lett., 93 (2004), p. 148302.
[15] E. L. BRIGGS, D. J. SULLIVAN, AND J. BERNHOLC, Large-scale electronic-structure calculations with multigrid acceleration, Phys. Rev. B, 52 (1995), pp. R5471–R5474.
[16] A. BRILEY, M. R. PEDERSON, K. A. JACKSON, D. C. PATTON, AND D. V. POREZAG, Vibrational frequencies and intensities of small molecules: All-electron, pseudopotential, and mixed-potential methodologies, Phys. Rev. B, 58 (1997), pp. 1786–1793.
[17] B. R. BROOKS, R. E. BRUCCOLERI, B. D. OLAFSON, D. J. STATES, S. SWAMINATHAN, AND M. KARPLUS, A program for macromolecular energy, minimization, and dynamics calculations, J. Comput. Chem., 4 (1983), pp. 187–217.
[18] C. G. BROYDEN, The convergence of a class of double-rank minimization algorithms. 2. The new algorithm., J. Inst. Math. Appl., 6 (1970), pp. 222–231.
[19] R. CAR AND M. PARRINELLO, Uni£ed approach for molecular dynamics and density functional theory, Phys. Rev. Lett., 55 (1985), pp. 2471–2474.
[20] M. CAVALLERI, M. ODELIUS, A. NILSSON, AND L. G. M. PETTERSSON, X-ray absorption spectra of water within a plane-wave Car-Parrinello molecular dynamics framework, J. Chem. Phys., 12 (2004), pp. 10065–10075.
[21] D. M. CEPERLEY AND B. J. ALDER, Ground state of the electron gas by a stochastic method, Phys. Rev. Lett., 45 (1980), pp. 566–569.
45

[22] G. CHANG, W. C. GUIDA, AND W. C. STILL, An internal-coordinate Monte Carlo method for searching conformational space, J. Am. Chem. Soc., 111 (1989), pp. 4379–4386.
[23] T. CHARPENTIER, S. ISPAS, M. PROFETA, F. MAURI, AND C. J. PICKARD, Firstprinciples calculation of 17O, Si-29, and Na-23 NMR spectra of sodium silicate crystals and glasses, J. Phys. Chem. B, 108 (2004), pp. 4147–4161.
[24] J. R. CHELIKOWSKY AND M. L. COHEN, Pseudopotentials for semiconductors, in Handbook of Semiconductors, T. S. Moss and P. T. Landsberg, eds., Elsevier, Amsterdam, 2nd edition, 1992.
[25] J. R. CHELIKOWSKY AND S. G. LOUIE, First-principles linear combination of atomic orbitals method for the cohesive and structural properties of solids: Application to diamond, Phys. Rev. B, 29 (1984), pp. 3470–3481.
[26] , eds., Quantum Theory of Materials, Kluwer, 1996.
[27] J. R. CHELIKOWSKY, N. TROULLIER, X. JING, D. DEAN, N. BIGGELI, K. WU, AND Y. SAAD, Algorithms for the structural properties of clusters, Comp. Phys. Comm., 85 (1995), pp. 325–335.
[28] J. R. CHELIKOWSKY, N. TROULLIER, AND Y. SAAD, The £nite-differencepseudopotential method: Electronic structure calculations without a basis, Phys. Rev. Lett., 72 (1994), pp. 1240–1243.
[29] J. R. CHELIKOWSKY, N. TROULLIER, K. WU, AND Y. SAAD, Higher order £nite difference pseudopotential method: An application to diatomic molecules, Phys. Rev. B, 50 (1994), pp. 11355–11364.
[30] A. R. CONN, N. I. M. GOULD, AND P. L. TOINT, Trust-Region Methods, MPS-SIAM, 2000.
[31] P. CSA´ SZA´ R AND P. PULAY, Geometry optimization by direct inversion in the iterative subspace, THEOCHEM, 114 (1984), pp. 31–34.
[32] A. CURIONI AND W. ANDREONI, Computer simulations for organic light-emitting diodes, IBM J. Res. Dev., 45 (2001), pp. 101–113.
[33] Y. H. DAI AND Y. YUAN, A nonlinear conjugate gradient method with a strong global convergence property, SIAM J. Optim., 10 (1999), pp. 177–182.
[34] J. W. DANIEL, The conjugate gradient method for linear and nonlinear operator equations, SIAM J. Numer. Anal., (1967), pp. 10–26.
[35] B. DAS, H. MEIROVITCH, AND I. M. NAVON, Performance of hybrid methods for largescale unconstrained optimization as applied to models of proteins, J. Comput. Chem., 24 (2003), pp. 1222–1231.
[36] W. C. DAVIDON, Variable metric method for minimization, Technical Report ANL-5990, Argonne National Laboratory, Argonne, IL, 1959.
46

[37] R. S. DEMBO AND T. STEIHAUG, Truncated Newton algorithms for large-scale unconstrained optimization, Math. Programming, 26 (1983), pp. 190–212.
[38] J. DENNIS, JR AND R. B. SCHNABEL, Numerical Methods for Unconstrained Optimization and Nonlinear Equations, SIAM, 1996.
[39] P. DERREUMAUX, G. ZHANG, B. BROOKS, AND T. SCHLICK, A truncated-Newton minimizer adapted for CHARMM and biomolecular applications, J. Comput. Chem., 15 (1994), pp. 532–552.
[40] A. EDELMAN, T. A. ARIAS, AND S. T. SMITH, The geometry of algorithms with orthogonality constraints, SIAM J. Matrix Anal. Appl., 20 (1999), pp. 303–353.
[41] O. FARKAS AND H. B. SCHLEGEL, Methods for optimizing large molecules. II. Quadratic search, J. Chem. Phys., 111 (1999), pp. 10806–10814.
[42] O. FARKAS AND H. B. SCHLEGEL, Methods for optimizing large molecules. III. an improved algorithm for geometry optimization using direct inversion in the iterative subspace (GDIIS), Phys. Chem. Chem. Phys., 4 (2002), pp. 11–15.
[43] J.-L. FATTEBERT AND J. BERNHOLC, Towards grid-based O(N) density-functional theory methods: Optimized nonorthogonal orbitals and multigrid acceleration, Phys. Rev. B, 62 (2000), pp. 1713–1722.
[44] E. FERMI, Application of statistical gas methods to electronic systems, Atti Accad. Nazl. Lincei, 6 (1927), pp. 602–607.
[45] , Tentativo di una teoria dei raggi beta, Nuovo Cimento, II (1934), pp. 1–19.
[46] D. FISCHER, A. CURIONI, S. R. BILLETER, AND W. ANDREONI, Effects of nitridation on the characteristics of silicon dioxide: Dielectric and structural properties from ab initio calculations, Phys. Rev. Lett., 92 (2004), p. 236405.
[47] R. FLETCHER, A new approach to variable metric algorithms, Comput. J., 13 (1970), pp. 317–322.
[48] , Practical Methods of Optimization, Vol. 1: Unconstrained optimization, John Wiley & Sons, 1987.
[49] R. FLETCHER AND M. J. D. POWELL, A rapidly convergent descent method for minimization, Comput. J., 6 (1963), pp. 163–168.
[50] R. FLETCHER AND C. M. REEVES, Function minimization by conjugate gradients, Comput. J., 7 (1964), pp. 149–154.
[51] C. Y. FONG, Topics in Computational Materials Science, World Scienti£c, 1998.
[52] B. FORNBERG AND D. M. SLOAN, A review of pseudospectral methods for solving partial differential equations, Acta Numer., 94 (1994), pp. 203–268.
47

[53] G. GALLI AND M. PARRINELLO, Large scale electronic structure calculation, Phys. Rev. Lett., 69 (1992), pp. 3547–3550.
[54] G. GALLI AND A. PASQUARELLO, First-principles molecular dynamics, in Computer Simulation in Chemical Physics, M. P. Allen and D. J. Tildesley, eds., Kluwer, 1993, pp. 261– 313.
[55] G. GAMOV, Thirty Years That Shook Physics: The Story of Quantum Theory, Dover, 1966.
[56] F. L. GERVASIO, R. CHELLI, P. PROCACCI, AND V. SCHETTINO, The nature of intermolecular interactions between aromatic amino acid residues, Proteins, 48 (2002), pp. 117– 125.
[57] F. L. GERVASIO, A. LAIO, M. IANNUZZI, AND M. PARRINELLO, In¤uence of the DNA structure on the reactivity of the guanine radical cation, Chemistry-A European Journal, 10 (2004), pp. 4846–4852.
[58] P. E. GILL, W. MURRAY, AND M. H. WRIGHT, Practical Optimization, Academic Press, 1981.
[59] S. GOEDECKER, Linear scaling electronic structure methods, Reviews of Modern Physics, 71 (1999), pp. 1085–1123.
[60] D. GOLDFARB, A family of variable metric methods derived by variational means, SIAM J. Appl. Math., 17 (1970), pp. 739–764.
[61] A. A. GOLDSTEIN, Constructive Real Analysis, Harper and Row, 1967.
[62] G. H. GOLUB AND C. VAN LOAN, Matrix Computations, 3rd edn, The John Hopkins University Press, Baltimore, 1996.
[63] X. GONZE, J.-M. BEUKEN, R. CARACAS, F. DETRAUX, M. FUCHS, G.-M. RIGNANESE, L. SINDIC, M. VERSTRAETE, G. ZERAH, F. JOLLET, M. TORRENT, A. ROY, M. MIKAMI, P. GHOSEZ, J.-Y. RATY, AND D. ALLAN, First-principles computation of material properties: The ABINIT software project, Computational Materials Science, 25 (2002), pp. 478–492.
[64] F. GYGI AND G. GALLI, Real-space adaptive-coordinate electronic-structure calculations, Phys. Rev. B, 52 (1995), pp. R2229–R2232.
[65] W. W. HAGER AND H. ZHANG, CG DESCENT, a conjugate gradient method with guaranteed descent, ACM Trans. Math. Software. To appear.
[66] , A survey of nonlinear conjugate gradient methods, Pac. J. Optim. To appear.
[67] , A new conjugate gradient method with guaranteed descent and an ef£cient linesearch, SIAM J. Optim., 16 (2005), pp. 170–192.
[68] R. J. HARRISON, Krylov subspace accelerated inexact Newton method for linear and nonlinear equations, J. Comput. Chem., 25 (2004), pp. 328–334.
48

[69] A. HAUG, Theoretical Solid State Physics, Pergamon Press, 1972.
[70] M. HEAD-GORDON, Quantum chemistry and molecular processes, J. Phys. Chem., 100 (1996), p. 13213.
[71] M. HEIKANEN, T. TORSTI, M. J. PUSKA, AND R. M. NIEMINEN, Multigrid method for electronic structure calculations, Phys. Rev. B, 63 (2001), pp. 245106–245113.
[72] M. R. HESTENES AND E. L. STIEFEL, Methods of conjugate gradients for solving linear systems, J. Res. Nat. Bur. Standards, Sect. 5, 49 (1952), pp. 409–436.
[73] P. HOHENBERG AND W. KOHN, Inhomogeneous electron gas, Phys. Rev., 136 (1964), pp. B864–B871.
[74] Y. HUANG, D. J. KOURI, AND D. K. HOFFMAN, Direct approach to density functional theory: iterative treatment using a polymomial representation of the heaviside step function operator, Chem. Phys. Let., 243 (1995), pp. 367–377.
[75] K. A. JACKSON, M. R. PEDERSON, D. V. POREZAG, Z. HAJNAL, AND T. FRAUNHEIM, Density-functional-based predictions of Raman and IR spectra for small Si clusters, Phys. Rev. B, 55 (1997), pp. 2549–2555.
[76] R. W. JANSEN AND O. F. SANKEY, Ab initio linear combination of pseudo-atomic-orbital scheme for the electronic properties of semiconductors: Results for ten materials, Phys. Rev. B, 36 (1987), pp. 6520–6531.
[77] L. O. JAY, H. KIM, Y. SAAD, AND J. R. CHELIKOWSKY, Electronic structure calculations using plane wave codes without diagonlization, Comput. Phys. Comm., 118 (1999), pp. 21– 30.
[78] R. O. JONES AND P. BALLONE, Density functional calculations for polymers and clusters - progress and limitations, Comp. Mater. Sci., 22 (2001), pp. 1–6.
[79] S. JUN, Meshfree implementation for the real-space electronic-structure calculation of crystalline solids, Internat. J. Numer. Methods Engrg., 59 (2004), pp. 1909 – 1923.
[80] R. KASCHNER AND D. HOHL, Density functional theory and biomolecules: A study of glycine, alanine, and their oligopeptides, J. Phys. Chem. A, 102 (1998), pp. 5111–5116.
[81] C. T. KELLEY, Iterative Methods for Optimization, SIAM, 1999.
[82] H. KIM AND J. IHM, Ab initio pseudopotential plane-wave calculations of the electronic structure of Y Ba2Cu3O7, Phys. Rev. B, 51 (1995), pp. 3886–3892.
[83] Y. H. KIM, I. H. LEE, AND R. M. MARTIN, Object-oriented construction of a multigrid electronic structure code with Fortran, Comp. Phys. Comm., 131 (2000), pp. 10–25.
[84] S. KIRKPATRICK, C. GELATT, JR., AND M. P. VECCHI, Optimization by simulated annealing, Science, 220 (1983), pp. 671–680.
49

[85] W. KOHN AND L. J. SHAM, Self-consistent equations including exchange and correlation effects, Phys. Rev., 140 (1965), pp. A1133–A1138.
[86] I. KOLOSSVA´ RY AND G. M. KESERU¨ , Hessian-free low-mode conformational search for large-scale protein loop optimization: Application to c-jun N-terminal kinase JNK3, J. Comput. Chem., 22 (2001), pp. 21–30.
[87] M. KONOPKA, R. ROUSSEAU, I. STICH, AND D. MARX, Detaching thiolates from copper and gold clusters: Which bonds to break?, J. Amer. Chem. Soc., 126 (2004), pp. 12103– 12111.
[88] G. KRESSE AND J. J. FURTHM ULLER, Ef£cient iterative schemes for ab initio total-energy calculations using a plane-wave basis set, Phys. Rev. B, 54 (1996), pp. 11169–11186.
[89] G. KRESSE AND J. HAFNER, Ab initio molecular dynamics for liquid metals, Phys. Rev. B, 47 (1993), pp. 558–561.
[90] C. LANCZOS, An iteration method for the solution of the eigenvalue problem of linear differential and integral operators, Journal of Research of the National Bureau of Standards, 45 (1950), pp. 255–282.
[91] R. M. LARSEN, PROPACK: A software package for the symmetric eigenvalue problem and singular value problems on Lanczos and Lanczos bidiagonalization with partial reorthogonalization, SCCM, Stanford University URL: http://sun.stanford.edu/∼rmunk/PROPACK/.
[92] , Ef£cient Algorithms for Helioseismic Inversion, PhD thesis, Dept. Computer Science, University of Aarhus, DK-8000 Aarhus C, Denmark, October 1998.
[93] C. LE BRIS AND P. L. LIONS, Froms atoms to crystals: A mathematical journey, Bull. of the A. M. S., 42 (2005), pp. 291–363.
[94] I. H. LEE, Y. H. KIM, AND R. M. MARTIN, One-way multigrid method in electronicstructure calculations, Phys. Rev. B, 61 (2000), p. 4397.
[95] J. LEE, H. A. SCHERAGA, AND S. RACKOVSKY, New optimization method for conformational energy calculations on polypeptides: Conformational space annealing, J. Comput. Chem., 18 (1997), pp. 1222–1232.
[96] X.-P. LI, R. W. NUNES, AND D. VANDERBILT, Density-matrix electronic-structure method with linear system-size scaling, Phys. Rev. B, 47 (1993), p. 10891.
[97] Z. LI AND H. A. SCHERAGA, Monte Carlo-minimization approach to the multiple-minima problem in protein folding, Proc. Natl. Acad. Sci., 84 (1987), pp. 6611–6615.
[98] W. Z. LIANG, C. SARAVANAN, Y. SHAO, A. BELL, AND H. HEAD-GRDON, Improved Fermi operator expansion methods for fast electronic structure calculations, J. Chem. Phys, 119 (2003), pp. 4117–4125.
50

[99] K. B. LIPKOWITZ, Abuses of molecular mechanics: Pitfalls to avoid, J. Chem. Educ., 72 (1995), pp. 1070–1075.
[100] D. C. LIU AND J. NOCEDAL, On the limited memory BFGS method for large-scale optimization, Math. Programming, 45 (1989), pp. 503–528.
[101] Y. LIU AND C. STOREY, Ef£cient generalized conjugate gradient algorithms, Part I: Theory, J. Optim. Theory Appl., 69 (1991), pp. 129–137.
[102] S. LUNDQVIST AND N. H. MARCH, eds., Theory of the Inhomogeneous Electron Gas, Plenum, 1983.
[103] O. MADELUNG, Introduction to Solid State Theory, Springer-Verlag, 1996.
[104] J. L. MARTINS AND M. COHEN, Diagonalization of large matrices in pseudopotential band-structure calculations: Dual-space formalism, Phys. Rev. B, 37 (1988), pp. 6134– 6138.
[105] J. L. MARTINS, N. TROULLIER, AND S.-H. WEI, Pseudopotential plane-wave calculations for ZnS, Phys. Rev. B, 43 (1991), pp. 2213–2217.
[106] R. MARTONAK, C. MOLTENI, AND M. PARRINELLO, A new constant-pressure ab initio/classical molecular dynamics method: Simulation of pressure-induced amorphization in a Si35H36 cluster, Comp. Mater. Sci., 20 (2001), pp. 293–299.
[107] F. MAURI AND G. GALLI, Electronic-structure calculations and molecular-dynamics simulation with linear system size scaling, Phys. Rev. B, 50 (1994), pp. 4316–4326.
[108] F. MAURI, G. GALLI, AND R. CAR, Orbital formulation for electronic-structure calculations with with linear system size scaling, Phys. Rev. B, 47 (1993), pp. 9973–9976.
[109] D. G. MCCULLOCH, A. R. MERCHANT, N. A. MARKS, N. C. COOPER, P. FIRZHENRY, M. M. M. BILEK, AND D. R. MCKENZIE, Wannier function analysis of tetrahedral amorphous networks, Diamond and Related Materials, 12 (2003), pp. 2026–2031.
[110] R. MCWEENY, Some recent advances in density matrix theory, Rev. Mod. Phys., 32 (1960), pp. 335–369.
[111] Molecular geometry. http://en.wikipedia.org/wiki/Molecular form, 2005.
[112] J. L. MORALES AND J. NOCEDAL, Enriched methods for large-scale unconstrained optimization, Comput. Optim. Appl., 21 (2002), pp. 143–154.
[113] T. MORDASINI, A. CURIONI, R. BURSI, AND W. ANDREONI, The binding mode of progesterone to its receptor deduced from molecular dynamics simulations, ChemBioChem, 4 (2003), pp. 155–161.
[114] J. J. MORE´ AND D. J. THUENTE, Line search algorithms with guaranteed suf£cient decrease, ACM Trans. Math. Software, 20 (1994), pp. 268–307.
51

[115] R. B. MORGAN AND D. S. SCOTT, Generalizations of Davidson’s method for computing eigenvalues of sparse symmetric matrices, SIAM J. Sci. Comput., 7 (1986), pp. 817–825.
[116] B. MURTAGH AND R. W. H. SARGENT, Computational experience with quadratically convergent minimisation methods, Comput. J., 13 (1970), pp. 185–194.
[117] S. G. NASH, User’s guide for TN/TNBC: FORTRAN routines for nonlinear optimization, Tech. Rep. No. 397, Mathematical Sciences Department, Johns Hopkins University, Baltimore, MD, 1984.
[118] S. G. NASH, A survey of truncated-Newton methods, J. Comput. Appl. Math., 124 (2000), pp. 45–59.
[119] S. G. NASH AND J. NOCEDAL, A numerical study of the limited memory BFGS method and the truncated-Newton method for large scale optimization, SIAM J. Optim., 1 (1991), pp. 358–372.
[120] J. NOCEDAL, Updating quasi-Newton matrices with limited storage, J. Mathematical of Comp., 35 (1980), pp. 773–782.
[121] J. NOCEDAL AND S. J. WRIGHT, Numerical Optimization, Springer, 1997.
[122] T. ONO AND K. HIROSE, Timesaving double-grid method for real-space electronic structure calculations, Phys. Rev. Lett., 82 (1999), pp. 5016–5019.
[123] B. N. PARLETT, The Symmetric Eigenvalue Problem, Prentice Hall, Englewood Cliffs, 1980.
[124] , The Symmetric Eigenvalue Problem, Prentice Hall, Englewood Cliffs, 1980.
[125] M. PARRINELLO, From silicon to RNA: The coming of age of ab initio molecular dynamics, Solid State Commun., 102 (1997), pp. 107–120.
[126] PARSEC website. http://www.ices.utexas.edu/parsec/index.html, 2005.
[127] J. E. PASK, B. M. KLEIN, C. Y. FONG, AND P. A. STERNE, Real-space local polynomial basis for solid-state electronic-structure calculations: A £nite-element approach, Phys. Rev. B, 59 (1999), pp. 12352–12358.
[128] G. PASTORE, E. SMARGIASSI, AND F. BUDA, Theory of ab initio molecular-dynamics calculations, Phys. Rev. A, 44 (1991), pp. 6334–6347.
[129] M. C. PAYNE, M. P. TETER, D. C. ALLAN, T. A. ARIAS, AND J. D. JOANNOPOULOS, Iterative minimization techniques for ab-initio total energy calculations: Molecular dynamics and conjugate gradients, Rev. Mod. Phys., 64 (1992), pp. 1045–1097.
[130] J. P. PERDEW, K. BURKE, AND Y. WANG, Generalized gradient approximation for the exchange-correlation hole of a many-electron system, Phys. Rev. B, 54 (1996), pp. 16533– 16539.
52

[131] B. G. PFROMMER, J. DEMMEL, AND H. SIMON, Unconstrained energy functionals for electronic structure calculations, J. Comput. Phys., 150 (1999), pp. 287–298.
[132] S. PIANA, P. CARLONI, AND M. PARRINELLO, Role of conformational ¤uctuations in the enzymatic reaction of HIV-1 protease, J. Mol. Biol., 319 (2002), pp. 567–583.
[133] J. PILLARDY, Y. A. ARNAUTOVA, C. CZAPLEWSKI, K. D. GIBSON, AND H. A. SCHERAGA, Conformation-family Monte Carlo: A new method for crystal structure prediction, Proc. Natl. Acad. Sci., 98 (2001), pp. 12351–12356.
[134] J. PILLARDY, C. CZAPLEWSKI, A. LIWO, J. LEE, D. R.RIPOLL, R. KAZMIERKIEWICZ, S. OLDZIEJ, W. J. WEDEMEYER, K. D. GIBSON, Y. A. ARNAUTOVA, J. SAUNDERS, Y. J. YE, AND H. A. SCHERAGA, Recent improvements in prediction of protein structure by global optimization of a potential energy function, Proc. Natl. Acad. Sci., 98 (2001), pp. 2329–2333.
[135] E. POLAK AND G. RIBIEÁRE, Note sur la convergence de me´thodes de directions conjuge´es, Revue Franc¸aise d’Informatique et de Recherche Ope´rationelle, 3e Annee 16 (1969), pp. 35–43.
[136] B. T. POLYAK, The conjugate gradient method in extreme problems, USSR Comp. Math. and Math. Phys., 9 (1969), pp. 94–112.
[137] V. POPHRISTIC, M. L. KLEIN, AND M. N. HOLERCA, Modeling small aluminum chlorohydrate polymers, J. Phys. Chem. A, 108 (2004), pp. 113–120.
[138] X. PRAT-RESINA, Study of the reaction mechanism in Mandelate Racemase enzyme: Reaction path and dynamical sampling approaches, PhD thesis, Universitat Autono´ma de Barcelona, 2004.
[139] P. PULAY, Convergence acceleration of iterative sequences. The case of SCF iteration, Chem. Phys. Lett., 73 (1980), pp. 393–398.
[140] , Improved SCF convergence acceleration, J. Comput. Chem., 3 (1982), pp. 556–560.
[141] P. PULAY, Analytical derivative methods in quantum chemistry, Adv. Chem. Phys., 69 (1987), pp. 241–286.
[142] P. PULAY AND G. FOGARASI, Geometry optimization in redundant internal coordinates, J. Chem. Phys., 96 (1992), pp. 2856–2860.
[143] L. M. RAMANIAH, M. BOERO, AND M. LAGHATE, Tantalum-fullerene clusters: A £rst principles study of static properties and dynamical behaviour, Phys. Rev. B, 70 (2004), pp. 35411–35424.
[144] H. RODER, R. SILVER, D. DRABOLD, AND J. DONG, The kernel polynomial method for non-orthogonal electronic structure calculation of amorphous diamond, Phys. Rev. B, 55 (1997), pp. 10582–10586.
53

[145] U. ROTHLISBERGER, 15 years of Car-Parrinello simulations in physics, chemistry and biology, in Computational Chemistry: Reviews of Current Trends, J. Leszczynski, ed., vol. 6, World Scienti£c, 2001.
[146] C. ROVIRA, B. SCHULZA, AND M. E. ET AL., In¤uence of the heme pocket conformation on the structure and vibrations of the Fe-CO bond in myoglobin: A QM/MM density functional study, Biophys. J., 81 (2001), pp. 435–445.
[147] H. RUTISHAUSER, Simultaneous iteration for symmetric matrices, in Handbook for automatic computations (linear algebra), J. Wilkinson and C. Reinsch, eds., New York, 1971, Springer Verlag, pp. 202–211.
[148] Y. SAAD, Numerical Methods for Large Eigenvalue Problems, Halstead Press, New York, 1992.
[149] , Iterative Methods for Sparse Linear Systems, PWS Publishing, 1996.
[150] Y. SAAD, A. STATHOPOULOS, J. R. CHELIKOWSKY, K. WU, AND S. OGUT, Solution of large eigenvalue problems in electronic structure calculations, BIT, 36 (1996), pp. 563–578.
[151] A. H. SAMEH AND J. A. WISNIEWSKI, A trace minimization algorithm for the generalized eigenvalue problem, SIAM Journal on Numerical Analysis, 19 (1982), pp. 1243–1259.
[152] M. SAUNDERS, Searching for conformers of nine- to twelve-ring hydrocarbons on the MM2 and MM3 energy surfaces: Stochastic search for interconversion pathways, J. Comput. Chem., 12 (1991), pp. 645–663.
[153] M. SAUNDERS, K. N. HOUK, Y.-D. WU, W. C. STILL, M. LIPTON, G. CHANG, AND W. C. GUIDA, Conformations of cycloheptadecane. A comparison of methods for conformational searching, J. Am. Chem. Soc., 112 (1990), pp. 1419–1427.
[154] F. SCHAUTZ, F. BUDA, AND C. FILIPPI, Excitations in photoactive molecules from quantum Monte Carlo, J. Chem. Phys., 121 (2004), pp. 5836–5844.
[155] H. B. SCHLEGEL, Geometry optimization on potential energy surfaces, in Modern Electronic Structure Theory, D. R. Yarkony, ed., World Scienti£c Publishing, 1995, pp. 459–500.
[156] , Geometry optimization 1, in Encyclopedia of Computational Chemistry, P. v. R. Schleyer, N. L. Allinger, P. A. Kollman, T. Clark, H. Schaefer, III, J. Gasteiger, and P. R. Schreiner, eds., vol. 2, Wiley, 1998, pp. 1136–1142.
[157] , Exploring potential energy surfaces for chemical reactions: An overview of practical methods, J. Comput. Chem., 24 (2003), pp. 1514–1527.
[158] T. SCHLICK, Geometry optimization 2, in Encyclopedia of Computational Chemistry, P. v. R. Schleyer, N. L. Allinger, P. A. Kollman, T. Clark, H. Schaefer, III, J. Gasteiger, and P. R. Schreiner, eds., vol. 2, Wiley, 1998, p. 1142.
54

[159] T. SCHLICK AND A. FOGELSON, TNPACK - A truncated Newton minimization package for large scale problems: I. Algorithm and usage, ACM Trans. Math. Software, 18 (1992), pp. 46–70.
[160] , TNPACK - A truncated Newton minimization package for large scale problems: II. Implementation examples, ACM Trans. Math. Software, 18 (1992), pp. 71–111.
[161] T. SCHLICK AND M. OVERTON, A powerful truncated Newton method for potential energy minimization, J. Comp. Chem., 8 (1987), pp. 1025–1039.
[162] M. D. SEGALL, P. J. D. LINDAN, M. J. PROBERT, C. J. PICKARD, P. J. HASNIP, S. J. CLARK, AND M. C. PAYNE, First-principles simulation: Ideas, illustrations and the CASTEP code, J. Phys.: Condens. Matter, 14 (2002), pp. 2717–2744.
[163] A. P. SEITSONEN, M. J. PUSKA, AND R. M. NIEMINEN, Real-space electronic-structure calculations: Combination of the £nite-difference and conjugate-gradient methods, Phys. Rev. B, 51 (1995), pp. 14057–14061.
[164] D. F. SHANNO, Conditioning of quasi-Newton methods for function minimization, Math. Comput., 24 (1970), pp. 647–656.
[165] J. R. SHEWCHUK, An introduction to the conjugate gradient method without the agonizing pain, School of Computer Science Report CMU-CS-94-125, Carnegie Mellon University, Pittsburgh, PA, 1994.
[166] H. D. SIMON, Analysis of the symmetric lanczos algorithm with reorthogonalization methods, Linear Algebra Appl., 61 (1984), pp. 101–132.
[167] , The Lanczos algorithm with partial reorthogonalization, Math. Comp., 42 (1984), pp. 115–142.
[168] J. SIMONS, P. JORGENSEN, H. TAYLOR, AND J. OZMENT, Walking on potential energy surfaces, J. Phys. Chem., 87 (1983), pp. 2745–2753.
[169] J. M. SOLER, E. ARTACHO, J. D. GALE, A. GARCIA, J. JUNQUERA, P. ORDEJO´ N, AND D. SA´ NCHEZ-PORTAL, The SIESTA method for ab-initio order-N materials simulation, J. Phys.: Condens. Matter, 14 (2002), pp. 2745–2779.
[170] I. STICH, R. CAR, M. PARRINELLO, AND S. BARONI, Conjugate gradient minimization of the energy functional: A new method for electronic structure calculation, Phys. Rev. B, 39 (1989), pp. 4997–5004.
[171] M. P. TETER, M. C. PAYNE, AND D. C. ALLEN, Solution of Schro¨dinger’s equation for large systems, Phys. Rev. B, 40 (1989), pp. 12255–12263.
[172] The CPMD Consortium publication website. http://www.cpmd.org/cpmd publications.html, 2005.
[173] The CUTEr test problem set. http://cuter.rl.ac.uk/cuter-www/Problems/mastsif.html, 2005.
55

[174] L. H. THOMAS, The calculation of atomic £elds, Proc. of the Cambridge Phil. Society, 23 (1927), p. 542.
[175] C. H. TONG, T. F. CHAN, AND C. C. J. KUO, Multilevel £ltering preconditioners: Extensions to more general elliptic problems, SIAM J. Sci. Stat. Comput., 13 (1992), pp. 227–242.
[176] T. TORSTI, M. HEISKANEN, M. PUSKA, AND R. NIEMINEN, MIKA: A multigrid-based program package for electronic structure calculations, Int. J. Quantum Chem., 91 (2003), pp. 171–176.
[177] E. TSUCHIDA AND M. TSUKADA, Electronic-structure calculations based on the £niteelement method, Phys. Rev. B, 52 (1995), pp. 5573–5578.
[178] M. E. TUCKERMAN, P. J. UNGAR, T. VON ROSENVINGE, , AND M. L. KLEIN, Ab initio molecular dynamics simulations, J. Phys. Chem., 100 (1996), pp. 12878–12887.
[179] M. VA´ SQUEZ AND H. A. SCHERAGA, Use of buildup and energy-minimization procedures to compute low-energy structures of the backbone enkephalin, Biopolymers, 24 (1985), pp. 1437–1447.
[180] D. J. WALES, Energy Landscapes, Cambridge University Press, 2003.
[181] L. W. WANG, Calculating the density of sates and optical-absorption spectra of large quantum systems by the plane-wave moments method, Phys. Rev. B, 49 (1994), pp. 10154–10158.
[182] S. R. WHITE, J. W. WILKINS, AND M. P. TETER, Finite-element method for electronic structure, Phys. Rev. B, 39 (1989), pp. 5819–5833.
[183] K. WU AND H. SIMON, A parallel Lanczos method for symmetric generalized eigenvalue problems, Tech. Rep. 41284, Lawrence Berkeley National Laboratory, 1997. Available on line at http://www.nersc.gov/research/SIMON/planso.html.
[184] S. YOO, J. J. ZHAO, J. L. WANG, AND X. C. ZENG, Endohedral silicon fullerenes Si-N (27 <= N <= 39), J. Amer. Chem. Soc., 126 (2004), pp. 13845–13849.
[185] X. ZHAO, Y. LIU, S. INOUE, T. SUZUKI, R. O. JONES, AND Y. ANDO, Smallest carbon nanotube is 3Aª in diameter, Phys. Rev. Lett., 92 (2004), p. 125502.
[186] J. M. ZIMAN, Electrons and Phonons, Oxford University Press, 1960.
[187] , Principles of the Theory of Solids, Cambridge University Press, 2nd ed., 1986.
[188] G. ZUMBACH, N. A. MODINE, AND E. KAXIRAS, Adaptive coordinate, real-space electronic structure calculations on parallel computers, Solid State Commun., 99 (1996), pp. 57–61.
56

