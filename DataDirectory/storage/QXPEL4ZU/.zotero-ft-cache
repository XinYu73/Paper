MPI: A Message-Passing Interface Standard
Version 3.0
Message Passing Interface Forum September 21, 2012

1

This document describes the Message-Passing Interface (MPI) standard, version 3.0.

2 The MPI standard includes point-to-point message-passing, collective communications, group

3 and communicator concepts, process topologies, environmental management, process cre-

4 ation and management, one-sided communications, extended collective operations, external

5 interfaces, I/O, some miscellaneous topics, and a proﬁling interface. Language bindings for

6 C and Fortran are deﬁned.

7

Historically, the evolution of the standards is from MPI-1.0 (June 1994) to MPI-1.1

8 (June 12, 1995) to MPI-1.2 (July 18, 1997), with several clariﬁcations and additions and

9 published as part of the MPI-2 document, to MPI-2.0 (July 18, 1997), with new functionality,

10 to MPI-1.3 (May 30, 2008), combining for historical reasons the documents 1.1 and 1.2

11 and some errata documents to one combined document, and to MPI-2.1 (June 23, 2008),

12 combining the previous documents. Version MPI-2.2 (September 2009) added additional

13 clariﬁcations and seven new routines. This version, MPI-3.0, is an extension of MPI-2.2.

14

15 Comments. Please send comments on MPI to the MPI Forum as follows:

16

17

1. Subscribe to http://lists.mpi-forum.org/mailman/listinfo.cgi/mpi-comments

18
2. Send your comment to: mpi-comments@mpi-forum.org, together with the URL of
19
the version of the MPI standard and the page and line numbers on which you are
20
commenting. Only use the oﬃcial versions.
21

22

Your comment will be forwarded to MPI Forum committee members for consideration.

23

Messages sent from an unsubscribed e-mail address will not be considered.

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

c 1993, 1994, 1995, 1996, 1997, 2008, 2009, 2012 University of Tennessee, Knoxville,

46 Tennessee. Permission to copy without fee all or part of this material is granted, provided

47 the University of Tennessee copyright notice and the title of this document appear, and

48 notice is given that copying is by permission of the University of Tennessee.

ii

Version 3.0: September 21, 2012. Coincident with the development of MPI-2.2, the MPI 1

Forum began discussions of a major extension to MPI. This document contains the MPI-3 2

Standard. This draft version of the MPI-3 standard contains signiﬁcant extensions to MPI 3

functionality, including nonblocking collectives, new one-sided communication operations, 4

and Fortran 2008 bindings. Unlike MPI-2.2, this standard is considered a major update to 5

the MPI standard. As with previous versions, new features have been adopted only when 6

there were compelling needs for the users. Some features, however, may have more than a 7

minor impact on existing MPI implementations.

8

9

Version 2.2: September 4, 2009. This document contains mostly corrections and clariﬁ- 10

cations to the MPI-2.1 document. A few extensions have been added; however all correct 11

MPI-2.1 programs are correct MPI-2.2 programs. New features were adopted only when 12

there were compelling needs for users, open source implementations, and minor impact on 13

existing MPI implementations.

14

15

Version 2.1: June 23, 2008. This document combines the previous documents MPI-1.3 (May 16

30, 2008) and MPI-2.0 (July 18, 1997). Certain parts of MPI-2.0, such as some sections of 17

Chapter 4, Miscellany, and Chapter 7, Extended Collective Operations, have been merged 18

into the Chapters of MPI-1.3. Additional errata and clariﬁcations collected by the MPI 19

Forum are also included in this document.

20

21

22
Version 1.3: May 30, 2008. This document combines the previous documents MPI-1.1 (June 23
12, 1995) and the MPI-1.2 Chapter in MPI-2 (July 18, 1997). Additional errata collected 24
by the MPI Forum referring to MPI-1.1 and MPI-1.2 are also included in this document. 25

26
Version 2.0: July 18, 1997. Beginning after the release of MPI-1.1, the MPI Forum began 27
meeting to consider corrections and extensions. MPI-2 has been focused on process creation 28
and management, one-sided communications, extended collective communications, external 29
interfaces and parallel I/O. A miscellany chapter discusses items that do not ﬁt elsewhere, 30
in particular language interoperability. 31

32

Version 1.2: July 18, 1997. The MPI-2 Forum introduced MPI-1.2 as Chapter 3 in the 33

standard “MPI-2: Extensions to the Message-Passing Interface”, July 18, 1997. This section 34

contains clariﬁcations and minor corrections to Version 1.1 of the MPI Standard. The only 35

new function in MPI-1.2 is one for identifying to which version of the MPI Standard the 36

implementation conforms. There are small diﬀerences between MPI-1 and MPI-1.1. There 37

are very few diﬀerences between MPI-1.1 and MPI-1.2, but large diﬀerences between MPI-1.2 38

and MPI-2.

39

40

Version 1.1: June, 1995. Beginning in March, 1995, the Message-Passing Interface Forum 41

reconvened to correct errors and make clariﬁcations in the MPI document of May 5, 1994, 42

referred to below as Version 1.0. These discussions resulted in Version 1.1. The changes from 43

Version 1.0 are minor. A version of this document with all changes marked is available.

44

45

Version 1.0: May, 1994. The Message-Passing Interface Forum (MPIF), with participation 46 from over 40 organizations, has been meeting since January 1993 to discuss and deﬁne a set 47

48

iii

1 of library interface standards for message passing. MPIF is not sanctioned or supported by

2 any oﬃcial standards organization.

3

The goal of the Message-Passing Interface, simply stated, is to develop a widely used

4 standard for writing message-passing programs. As such the interface should establish a

5 practical, portable, eﬃcient, and ﬂexible standard for message-passing.

6

This is the ﬁnal report, Version 1.0, of the Message-Passing Interface Forum. This

7 document contains all the technical features proposed for the interface. This copy of the

8 draft was processed by LATEX on May 5, 1994.
9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

iv

Contents

Acknowledgments

ix

1 Introduction to MPI

1

1.1 Overview and Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1

1.2 Background of MPI-1.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2

1.3 Background of MPI-1.1, MPI-1.2, and MPI-2.0 . . . . . . . . . . . . . . . . . 2

1.4 Background of MPI-1.3 and MPI-2.1 . . . . . . . . . . . . . . . . . . . . . . 3

1.5 Background of MPI-2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.6 Background of MPI-3.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

1.7 Who Should Use This Standard? . . . . . . . . . . . . . . . . . . . . . . . . 4

1.8 What Platforms Are Targets For Implementation? . . . . . . . . . . . . . . 5

1.9 What Is Included In The Standard? . . . . . . . . . . . . . . . . . . . . . . 5

1.10 What Is Not Included In The Standard? . . . . . . . . . . . . . . . . . . . . 6

1.11 Organization of this Document . . . . . . . . . . . . . . . . . . . . . . . . . 6

2 MPI Terms and Conventions

9

2.1 Document Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.2 Naming Conventions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.3 Procedure Speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

2.4 Semantic Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11

2.5 Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.5.1 Opaque Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.5.2 Array Arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

2.5.3 State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

2.5.4 Named Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

2.5.5 Choice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.5.6 Addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.5.7 File Oﬀsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

2.5.8 Counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

2.6 Language Binding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

2.6.1 Deprecated and Removed Names and Functions . . . . . . . . . . . . 17

2.6.2 Fortran Binding Issues . . . . . . . . . . . . . . . . . . . . . . . . . . 18

2.6.3 C Binding Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

2.6.4 Functions and Macros . . . . . . . . . . . . . . . . . . . . . . . . . . 19

2.7 Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

2.8 Error Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

2.9 Implementation Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

v

2.9.1 Independence of Basic Runtime Routines . . . . . . . . . . . . . . . 21 2.9.2 Interaction with Signals . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.10 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

3 Point-to-Point Communication

23

3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

3.2 Blocking Send and Receive Operations . . . . . . . . . . . . . . . . . . . . . 24

3.2.1 Blocking Send . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

3.2.2 Message Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

3.2.3 Message Envelope . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

3.2.4 Blocking Receive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

3.2.5 Return Status . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

3.2.6 Passing MPI_STATUS_IGNORE for Status . . . . . . . . . . . . . . . . 32

3.3 Data Type Matching and Data Conversion . . . . . . . . . . . . . . . . . . 33

3.3.1 Type Matching Rules . . . . . . . . . . . . . . . . . . . . . . . . . . 33

Type MPI_CHARACTER . . . . . . . . . . . . . . . . . . . . . . . . . . 34

3.3.2 Data Conversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

3.4 Communication Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

3.5 Semantics of Point-to-Point Communication . . . . . . . . . . . . . . . . . . 40

3.6 Buﬀer Allocation and Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

3.6.1 Model Implementation of Buﬀered Mode . . . . . . . . . . . . . . . . 46

3.7 Nonblocking Communication . . . . . . . . . . . . . . . . . . . . . . . . . . 47

3.7.1 Communication Request Objects . . . . . . . . . . . . . . . . . . . . 48

3.7.2 Communication Initiation . . . . . . . . . . . . . . . . . . . . . . . . 48

3.7.3 Communication Completion . . . . . . . . . . . . . . . . . . . . . . . 52

3.7.4 Semantics of Nonblocking Communications . . . . . . . . . . . . . . 56

3.7.5 Multiple Completions . . . . . . . . . . . . . . . . . . . . . . . . . . 57

3.7.6 Non-destructive Test of status . . . . . . . . . . . . . . . . . . . . . . 63

3.8 Probe and Cancel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64

3.8.1 Probe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64

3.8.2 Matching Probe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67

3.8.3 Matched Receives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69

3.8.4 Cancel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71

3.9 Persistent Communication Requests . . . . . . . . . . . . . . . . . . . . . . 73

3.10 Send-Receive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78

3.11 Null Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81

4 Datatypes

83

4.1 Derived Datatypes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83

4.1.1 Type Constructors with Explicit Addresses . . . . . . . . . . . . . . 85

4.1.2 Datatype Constructors . . . . . . . . . . . . . . . . . . . . . . . . . . 85

4.1.3 Subarray Datatype Constructor . . . . . . . . . . . . . . . . . . . . . 95

4.1.4 Distributed Array Datatype Constructor . . . . . . . . . . . . . . . . 97

4.1.5 Address and Size Functions . . . . . . . . . . . . . . . . . . . . . . . 102

4.1.6 Lower-Bound and Upper-Bound Markers . . . . . . . . . . . . . . . 104

4.1.7 Extent and Bounds of Datatypes . . . . . . . . . . . . . . . . . . . . 107

4.1.8 True Extent of Datatypes . . . . . . . . . . . . . . . . . . . . . . . . 108

4.1.9 Commit and Free . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110

vi

4.1.10 Duplicating a Datatype . . . . . . . . . . . . . . . . . . . . . . . . . 111 4.1.11 Use of General Datatypes in Communication . . . . . . . . . . . . . 112 4.1.12 Correct Use of Addresses . . . . . . . . . . . . . . . . . . . . . . . . 115 4.1.13 Decoding a Datatype . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 4.1.14 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 4.2 Pack and Unpack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131 4.3 Canonical MPI_PACK and MPI_UNPACK . . . . . . . . . . . . . . . . . . . 138

5 Collective Communication

141

5.1 Introduction and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . 141

5.2 Communicator Argument . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144

5.2.1 Speciﬁcs for Intracommunicator Collective Operations . . . . . . . . 144

5.2.2 Applying Collective Operations to Intercommunicators . . . . . . . . 145

5.2.3 Speciﬁcs for Intercommunicator Collective Operations . . . . . . . . 146

5.3 Barrier Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147

5.4 Broadcast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148

5.4.1 Example using MPI_BCAST . . . . . . . . . . . . . . . . . . . . . . . 149

5.5 Gather . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149

5.5.1 Examples using MPI_GATHER, MPI_GATHERV . . . . . . . . . . . . 152

5.6 Scatter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159

5.6.1 Examples using MPI_SCATTER, MPI_SCATTERV . . . . . . . . . . 162

5.7 Gather-to-all . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165

5.7.1 Example using MPI_ALLGATHER . . . . . . . . . . . . . . . . . . . . 167

5.8 All-to-All Scatter/Gather . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168

5.9 Global Reduction Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 173

5.9.1 Reduce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174

5.9.2 Predeﬁned Reduction Operations . . . . . . . . . . . . . . . . . . . . 176

5.9.3 Signed Characters and Reductions . . . . . . . . . . . . . . . . . . . 178

5.9.4 MINLOC and MAXLOC . . . . . . . . . . . . . . . . . . . . . . . . 179

5.9.5 User-Deﬁned Reduction Operations . . . . . . . . . . . . . . . . . . 183

Example of User-deﬁned Reduce . . . . . . . . . . . . . . . . . . . . 186

5.9.6 All-Reduce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187

5.9.7 Process-Local Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 189

5.10 Reduce-Scatter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190

5.10.1 MPI_REDUCE_SCATTER_BLOCK . . . . . . . . . . . . . . . . . . . 190

5.10.2 MPI_REDUCE_SCATTER . . . . . . . . . . . . . . . . . . . . . . . . 191

5.11 Scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193

5.11.1 Inclusive Scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193

5.11.2 Exclusive Scan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194

5.11.3 Example using MPI_SCAN . . . . . . . . . . . . . . . . . . . . . . . . 195

5.12 Nonblocking Collective Operations . . . . . . . . . . . . . . . . . . . . . . . 196

5.12.1 Nonblocking Barrier Synchronization . . . . . . . . . . . . . . . . . . 198

5.12.2 Nonblocking Broadcast . . . . . . . . . . . . . . . . . . . . . . . . . 199

Example using MPI_IBCAST . . . . . . . . . . . . . . . . . . . . . . 200

5.12.3 Nonblocking Gather . . . . . . . . . . . . . . . . . . . . . . . . . . . 200

5.12.4 Nonblocking Scatter . . . . . . . . . . . . . . . . . . . . . . . . . . . 202

5.12.5 Nonblocking Gather-to-all . . . . . . . . . . . . . . . . . . . . . . . . 204

5.12.6 Nonblocking All-to-All Scatter/Gather . . . . . . . . . . . . . . . . . 206

vii

5.12.7 Nonblocking Reduce . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 5.12.8 Nonblocking All-Reduce . . . . . . . . . . . . . . . . . . . . . . . . . 210 5.12.9 Nonblocking Reduce-Scatter with Equal Blocks . . . . . . . . . . . . 211 5.12.10 Nonblocking Reduce-Scatter . . . . . . . . . . . . . . . . . . . . . . . 212 5.12.11 Nonblocking Inclusive Scan . . . . . . . . . . . . . . . . . . . . . . . 213 5.12.12 Nonblocking Exclusive Scan . . . . . . . . . . . . . . . . . . . . . . . 214 5.13 Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214

6 Groups, Contexts, Communicators, and Caching

223

6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223

6.1.1 Features Needed to Support Libraries . . . . . . . . . . . . . . . . . 223

6.1.2 MPI’s Support for Libraries . . . . . . . . . . . . . . . . . . . . . . . 224

6.2 Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

6.2.1 Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

6.2.2 Contexts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

6.2.3 Intra-Communicators . . . . . . . . . . . . . . . . . . . . . . . . . . 227

6.2.4 Predeﬁned Intra-Communicators . . . . . . . . . . . . . . . . . . . . 227

6.3 Group Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

6.3.1 Group Accessors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

6.3.2 Group Constructors . . . . . . . . . . . . . . . . . . . . . . . . . . . 230

6.3.3 Group Destructors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235

6.4 Communicator Management . . . . . . . . . . . . . . . . . . . . . . . . . . . 235

6.4.1 Communicator Accessors . . . . . . . . . . . . . . . . . . . . . . . . 235

6.4.2 Communicator Constructors . . . . . . . . . . . . . . . . . . . . . . . 237

6.4.3 Communicator Destructors . . . . . . . . . . . . . . . . . . . . . . . 248

6.4.4 Communicator Info . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248

6.5 Motivating Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250

6.5.1 Current Practice #1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 250

6.5.2 Current Practice #2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 251

6.5.3 (Approximate) Current Practice #3 . . . . . . . . . . . . . . . . . . 251

6.5.4 Example #4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252

6.5.5 Library Example #1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 253

6.5.6 Library Example #2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 255

6.6 Inter-Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257

6.6.1 Inter-communicator Accessors . . . . . . . . . . . . . . . . . . . . . . 259

6.6.2 Inter-communicator Operations . . . . . . . . . . . . . . . . . . . . . 260

6.6.3 Inter-Communication Examples . . . . . . . . . . . . . . . . . . . . . 263

Example 1: Three-Group “Pipeline” . . . . . . . . . . . . . . . . . . 263

Example 2: Three-Group “Ring” . . . . . . . . . . . . . . . . . . . . 264

6.7 Caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265

6.7.1 Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266

6.7.2 Communicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267

6.7.3 Windows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272

6.7.4 Datatypes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276

6.7.5 Error Class for Invalid Keyval . . . . . . . . . . . . . . . . . . . . . . 279

6.7.6 Attributes Example . . . . . . . . . . . . . . . . . . . . . . . . . . . 279

6.8 Naming Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281

6.9 Formalizing the Loosely Synchronous Model . . . . . . . . . . . . . . . . . . 285

viii

6.9.1 Basic Statements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285 6.9.2 Models of Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
Static Communicator Allocation . . . . . . . . . . . . . . . . . . . . 286 Dynamic Communicator Allocation . . . . . . . . . . . . . . . . . . . 286 The General Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287

7 Process Topologies

289

7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289

7.2 Virtual Topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290

7.3 Embedding in MPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290

7.4 Overview of the Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290

7.5 Topology Constructors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292

7.5.1 Cartesian Constructor . . . . . . . . . . . . . . . . . . . . . . . . . . 292

7.5.2 Cartesian Convenience Function: MPI_DIMS_CREATE . . . . . . . . 293

7.5.3 Graph Constructor . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294

7.5.4 Distributed Graph Constructor . . . . . . . . . . . . . . . . . . . . . 296

7.5.5 Topology Inquiry Functions . . . . . . . . . . . . . . . . . . . . . . . 302

7.5.6 Cartesian Shift Coordinates . . . . . . . . . . . . . . . . . . . . . . . 310

7.5.7 Partitioning of Cartesian Structures . . . . . . . . . . . . . . . . . . 312

7.5.8 Low-Level Topology Functions . . . . . . . . . . . . . . . . . . . . . 312

7.6 Neighborhood Collective Communication . . . . . . . . . . . . . . . . . . . 314

7.6.1 Neighborhood Gather . . . . . . . . . . . . . . . . . . . . . . . . . . 315

7.6.2 Neighbor Alltoall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319

7.7 Nonblocking Neighborhood Communication . . . . . . . . . . . . . . . . . . 324

7.7.1 Nonblocking Neighborhood Gather . . . . . . . . . . . . . . . . . . . 325

7.7.2 Nonblocking Neighborhood Alltoall . . . . . . . . . . . . . . . . . . . 327

7.8 An Application Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330

8 MPI Environmental Management

335

8.1 Implementation Information . . . . . . . . . . . . . . . . . . . . . . . . . . . 335

8.1.1 Version Inquiries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335

8.1.2 Environmental Inquiries . . . . . . . . . . . . . . . . . . . . . . . . . 336

Tag Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337

Host Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337

IO Rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337

Clock Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . 338

Inquire Processor Name . . . . . . . . . . . . . . . . . . . . . . . . . 338

8.2 Memory Allocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339

8.3 Error Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342

8.3.1 Error Handlers for Communicators . . . . . . . . . . . . . . . . . . . 343

8.3.2 Error Handlers for Windows . . . . . . . . . . . . . . . . . . . . . . . 345

8.3.3 Error Handlers for Files . . . . . . . . . . . . . . . . . . . . . . . . . 347

8.3.4 Freeing Errorhandlers and Retrieving Error Strings . . . . . . . . . . 348

8.4 Error Codes and Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349

8.5 Error Classes, Error Codes, and Error Handlers . . . . . . . . . . . . . . . . 352

8.6 Timers and Synchronization . . . . . . . . . . . . . . . . . . . . . . . . . . . 356

8.7 Startup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357

8.7.1 Allowing User Functions at Process Termination . . . . . . . . . . . 363

ix

8.7.2 Determining Whether MPI Has Finished . . . . . . . . . . . . . . . . 363 8.8 Portable MPI Process Startup . . . . . . . . . . . . . . . . . . . . . . . . . . 364

9 The Info Object

367

10 Process Creation and Management

373

10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373

10.2 The Dynamic Process Model . . . . . . . . . . . . . . . . . . . . . . . . . . 374

10.2.1 Starting Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 374

10.2.2 The Runtime Environment . . . . . . . . . . . . . . . . . . . . . . . 374

10.3 Process Manager Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376

10.3.1 Processes in MPI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376

10.3.2 Starting Processes and Establishing Communication . . . . . . . . . 376

10.3.3 Starting Multiple Executables and Establishing Communication . . 381

10.3.4 Reserved Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384

10.3.5 Spawn Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385

Manager-worker Example Using MPI_COMM_SPAWN . . . . . . . . 385

10.4 Establishing Communication . . . . . . . . . . . . . . . . . . . . . . . . . . 387

10.4.1 Names, Addresses, Ports, and All That . . . . . . . . . . . . . . . . 387

10.4.2 Server Routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388

10.4.3 Client Routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390

10.4.4 Name Publishing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392

10.4.5 Reserved Key Values . . . . . . . . . . . . . . . . . . . . . . . . . . . 394

10.4.6 Client/Server Examples . . . . . . . . . . . . . . . . . . . . . . . . . 394

Simplest Example — Completely Portable. . . . . . . . . . . . . . . 394

Ocean/Atmosphere — Relies on Name Publishing . . . . . . . . . . 395

Simple Client-Server Example . . . . . . . . . . . . . . . . . . . . . . 395

10.5 Other Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397

10.5.1 Universe Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397

10.5.2 Singleton MPI_INIT . . . . . . . . . . . . . . . . . . . . . . . . . . . 398

10.5.3 MPI_APPNUM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398

10.5.4 Releasing Connections . . . . . . . . . . . . . . . . . . . . . . . . . . 399

10.5.5 Another Way to Establish MPI Communication . . . . . . . . . . . . 401

11 One-Sided Communications

403

11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403

11.2 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404

11.2.1 Window Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404

11.2.2 Window That Allocates Memory . . . . . . . . . . . . . . . . . . . . 407

11.2.3 Window That Allocates Shared Memory . . . . . . . . . . . . . . . . 408

11.2.4 Window of Dynamically Attached Memory . . . . . . . . . . . . . . 411

11.2.5 Window Destruction . . . . . . . . . . . . . . . . . . . . . . . . . . . 414

11.2.6 Window Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415

11.2.7 Window Info . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417

11.3 Communication Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418

11.3.1 Put . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419

11.3.2 Get . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421

11.3.3 Examples for Communication Calls . . . . . . . . . . . . . . . . . . . 422

x

11.3.4 Accumulate Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 424 Accumulate Function . . . . . . . . . . . . . . . . . . . . . . . . . . 424 Get Accumulate Function . . . . . . . . . . . . . . . . . . . . . . . . 427 Fetch and Op Function . . . . . . . . . . . . . . . . . . . . . . . . . 428 Compare and Swap Function . . . . . . . . . . . . . . . . . . . . . . 429
11.3.5 Request-based RMA Communication Operations . . . . . . . . . . . 430 11.4 Memory Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435 11.5 Synchronization Calls . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437
11.5.1 Fence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440 11.5.2 General Active Target Synchronization . . . . . . . . . . . . . . . . . 442 11.5.3 Lock . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446 11.5.4 Flush and Sync . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449 11.5.5 Assertions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451 11.5.6 Miscellaneous Clariﬁcations . . . . . . . . . . . . . . . . . . . . . . . 452 11.6 Error Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452 11.6.1 Error Handlers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452 11.6.2 Error Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453 11.7 Semantics and Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453 11.7.1 Atomicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461 11.7.2 Ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461 11.7.3 Progress . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462 11.7.4 Registers and Compiler Optimizations . . . . . . . . . . . . . . . . . 464 11.8 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465

12 External Interfaces

473

12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473

12.2 Generalized Requests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473

12.2.1 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478

12.3 Associating Information with Status . . . . . . . . . . . . . . . . . . . . . . 480

12.4 MPI and Threads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482

12.4.1 General . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482

12.4.2 Clariﬁcations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483

12.4.3 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485

13 I/O

489

13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489

13.1.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489

13.2 File Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491

13.2.1 Opening a File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491

13.2.2 Closing a File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493

13.2.3 Deleting a File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 494

13.2.4 Resizing a File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495

13.2.5 Preallocating Space for a File . . . . . . . . . . . . . . . . . . . . . . 496

13.2.6 Querying the Size of a File . . . . . . . . . . . . . . . . . . . . . . . 496

13.2.7 Querying File Parameters . . . . . . . . . . . . . . . . . . . . . . . . 497

13.2.8 File Info . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 498

Reserved File Hints . . . . . . . . . . . . . . . . . . . . . . . . . . . 500

13.3 File Views . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501

xi

13.4 Data Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504 13.4.1 Data Access Routines . . . . . . . . . . . . . . . . . . . . . . . . . . 504 Positioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505 Synchronism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505 Coordination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506 Data Access Conventions . . . . . . . . . . . . . . . . . . . . . . . . 506 13.4.2 Data Access with Explicit Oﬀsets . . . . . . . . . . . . . . . . . . . . 507 13.4.3 Data Access with Individual File Pointers . . . . . . . . . . . . . . . 511 13.4.4 Data Access with Shared File Pointers . . . . . . . . . . . . . . . . . 518 Noncollective Operations . . . . . . . . . . . . . . . . . . . . . . . . 518 Collective Operations . . . . . . . . . . . . . . . . . . . . . . . . . . 520 Seek . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522 13.4.5 Split Collective Data Access Routines . . . . . . . . . . . . . . . . . 523
13.5 File Interoperability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530 13.5.1 Datatypes for File Interoperability . . . . . . . . . . . . . . . . . . . 532 13.5.2 External Data Representation: “external32” . . . . . . . . . . . . . . 534 13.5.3 User-Deﬁned Data Representations . . . . . . . . . . . . . . . . . . . 535 Extent Callback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537 Datarep Conversion Functions . . . . . . . . . . . . . . . . . . . . . 537 13.5.4 Matching Data Representations . . . . . . . . . . . . . . . . . . . . . 540
13.6 Consistency and Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . 540 13.6.1 File Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 540 13.6.2 Random Access vs. Sequential Files . . . . . . . . . . . . . . . . . . 543 13.6.3 Progress . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544 13.6.4 Collective File Operations . . . . . . . . . . . . . . . . . . . . . . . . 544 13.6.5 Type Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544 13.6.6 Miscellaneous Clariﬁcations . . . . . . . . . . . . . . . . . . . . . . . 544 13.6.7 MPI_Oﬀset Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545 13.6.8 Logical vs. Physical File Layout . . . . . . . . . . . . . . . . . . . . . 545 13.6.9 File Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 545 13.6.10 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546 Asynchronous I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
13.7 I/O Error Handling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550 13.8 I/O Error Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550 13.9 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551
13.9.1 Double Buﬀering with Split Collective I/O . . . . . . . . . . . . . . 551 13.9.2 Subarray Filetype Constructor . . . . . . . . . . . . . . . . . . . . . 553

14 Tool Support

555

14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555

14.2 Proﬁling Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555

14.2.1 Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555

14.2.2 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556

14.2.3 Logic of the Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556

14.2.4 Miscellaneous Control of Proﬁling . . . . . . . . . . . . . . . . . . . 557

14.2.5 Proﬁler Implementation Example . . . . . . . . . . . . . . . . . . . . 558

14.2.6 MPI Library Implementation Example . . . . . . . . . . . . . . . . . 558

Systems with Weak Symbols . . . . . . . . . . . . . . . . . . . . . . 558

xii

Systems Without Weak Symbols . . . . . . . . . . . . . . . . . . . . 559 14.2.7 Complications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559
Multiple Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559 Linker Oddities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 560 Fortran Support Methods . . . . . . . . . . . . . . . . . . . . . . . . 560 14.2.8 Multiple Levels of Interception . . . . . . . . . . . . . . . . . . . . . 560 14.3 The MPI Tool Information Interface . . . . . . . . . . . . . . . . . . . . . . 561 14.3.1 Verbosity Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 562 14.3.2 Binding MPI Tool Information Interface Variables to MPI Objects . 562 14.3.3 Convention for Returning Strings . . . . . . . . . . . . . . . . . . . . 563 14.3.4 Initialization and Finalization . . . . . . . . . . . . . . . . . . . . . . 563 14.3.5 Datatype System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565 14.3.6 Control Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567 Control Variable Query Functions . . . . . . . . . . . . . . . . . . . 567 Example: Printing All Control Variables . . . . . . . . . . . . . . . . 569 Handle Allocation and Deallocation . . . . . . . . . . . . . . . . . . 570 Control Variable Access Functions . . . . . . . . . . . . . . . . . . . 571 Example: Reading the Value of a Control Variable . . . . . . . . . . 572 14.3.7 Performance Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 573 Performance Variable Classes . . . . . . . . . . . . . . . . . . . . . . 573 Performance Variable Query Functions . . . . . . . . . . . . . . . . . 575 Performance Experiment Sessions . . . . . . . . . . . . . . . . . . . . 577 Handle Allocation and Deallocation . . . . . . . . . . . . . . . . . . 578 Starting and Stopping of Performance Variables . . . . . . . . . . . 579 Performance Variable Access Functions . . . . . . . . . . . . . . . . 580 Example: Tool to Detect Receives with Long Unexpected Message
Queues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 582 14.3.8 Variable Categorization . . . . . . . . . . . . . . . . . . . . . . . . . 584 14.3.9 Return Codes for the MPI Tool Information Interface . . . . . . . . 588 14.3.10 Proﬁling Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . 588

15 Deprecated Functions

591

15.1 Deprecated since MPI-2.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591

15.2 Deprecated since MPI-2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594

16 Removed Interfaces

595

16.1 Removed MPI-1 Bindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595

16.1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595

16.1.2 Removed MPI-1 Functions . . . . . . . . . . . . . . . . . . . . . . . . 595

16.1.3 Removed MPI-1 Datatypes . . . . . . . . . . . . . . . . . . . . . . . 595

16.1.4 Removed MPI-1 Constants . . . . . . . . . . . . . . . . . . . . . . . . 596

16.1.5 Removed MPI-1 Callback Prototypes . . . . . . . . . . . . . . . . . . 596

16.2 C++ Bindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596

17 Language Bindings

597

17.1 Fortran Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597

17.1.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597

17.1.2 Fortran Support Through the mpi_f08 Module . . . . . . . . . . . . 598

xiii

17.1.3 Fortran Support Through the mpi Module . . . . . . . . . . . . . . . 601 17.1.4 Fortran Support Through the mpif.h Include File . . . . . . . . . . 603 17.1.5 Interface Speciﬁcations, Linker Names and the Proﬁling Interface . . 605 17.1.6 MPI for Diﬀerent Fortran Standard Versions . . . . . . . . . . . . . 609 17.1.7 Requirements on Fortran Compilers . . . . . . . . . . . . . . . . . . 613 17.1.8 Additional Support for Fortran Register-Memory-Synchronization . 615 17.1.9 Additional Support for Fortran Numeric Intrinsic Types . . . . . . . 615
Parameterized Datatypes with Speciﬁed Precision and Exponent Range616 Support for Size-speciﬁc MPI Datatypes . . . . . . . . . . . . . . . . 620 Communication With Size-speciﬁc Types . . . . . . . . . . . . . . . 622 17.1.10 Problems With Fortran Bindings for MPI . . . . . . . . . . . . . . . 624 17.1.11 Problems Due to Strong Typing . . . . . . . . . . . . . . . . . . . . 625 17.1.12 Problems Due to Data Copying and Sequence Association with Subscript Triplets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626 17.1.13 Problems Due to Data Copying and Sequence Association with Vector Subscripts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629 17.1.14 Special Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629 17.1.15 Fortran Derived Types . . . . . . . . . . . . . . . . . . . . . . . . . . 629 17.1.16 Optimization Problems, an Overview . . . . . . . . . . . . . . . . . . 631 17.1.17 Problems with Code Movement and Register Optimization . . . . . 632 Nonblocking Operations . . . . . . . . . . . . . . . . . . . . . . . . . 632 One-sided Communication . . . . . . . . . . . . . . . . . . . . . . . . 633 MPI_BOTTOM and Combining Independent Variables in Datatypes 633 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634 The Fortran ASYNCHRONOUS Attribute . . . . . . . . . . . . . . 635 Calling MPI_F_SYNC_REG . . . . . . . . . . . . . . . . . . . . . . 637 A User Deﬁned Routine Instead of MPI_F_SYNC_REG . . . . . . . 638 Module Variables and COMMON Blocks . . . . . . . . . . . . . . . 639 The (Poorly Performing) Fortran VOLATILE Attribute . . . . . . . 639 The Fortran TARGET Attribute . . . . . . . . . . . . . . . . . . . . 639 17.1.18 Temporary Data Movement and Temporary Memory Modiﬁcation . 639 17.1.19 Permanent Data Movement . . . . . . . . . . . . . . . . . . . . . . . 642 17.1.20 Comparison with C . . . . . . . . . . . . . . . . . . . . . . . . . . . 642 17.2 Language Interoperability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645 17.2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645 17.2.2 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645 17.2.3 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645 17.2.4 Transfer of Handles . . . . . . . . . . . . . . . . . . . . . . . . . . . 646 17.2.5 Status . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 648 17.2.6 MPI Opaque Objects . . . . . . . . . . . . . . . . . . . . . . . . . . 650 Datatypes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 651 Callback Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 652 Error Handlers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653 Reduce Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653 17.2.7 Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653 17.2.8 Extra-State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 657 17.2.9 Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 657 17.2.10 Interlanguage Communication . . . . . . . . . . . . . . . . . . . . . . 658
xiv

A Language Bindings Summary

661

A.1 Deﬁned Values and Handles . . . . . . . . . . . . . . . . . . . . . . . . . . . 661

A.1.1 Deﬁned Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661

A.1.2 Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 676

A.1.3 Prototype Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . 677

C Bindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677

Fortran 2008 Bindings with the mpi_f08 Module . . . . . . . . . . . 678

Fortran Bindings with mpif.h or the mpi Module . . . . . . . . . . . 680

A.1.4 Deprecated Prototype Deﬁnitions . . . . . . . . . . . . . . . . . . . . 682

A.1.5 Info Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683

A.1.6 Info Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 683

A.2 C Bindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685

A.2.1 Point-to-Point Communication C Bindings . . . . . . . . . . . . . . 685

A.2.2 Datatypes C Bindings . . . . . . . . . . . . . . . . . . . . . . . . . . 687

A.2.3 Collective Communication C Bindings . . . . . . . . . . . . . . . . . 689

A.2.4 Groups, Contexts, Communicators, and Caching C Bindings . . . . 691

A.2.5 Process Topologies C Bindings . . . . . . . . . . . . . . . . . . . . . 694

A.2.6 MPI Environmental Management C Bindings . . . . . . . . . . . . . 696

A.2.7 The Info Object C Bindings . . . . . . . . . . . . . . . . . . . . . . . 697

A.2.8 Process Creation and Management C Bindings . . . . . . . . . . . . 697

A.2.9 One-Sided Communications C Bindings . . . . . . . . . . . . . . . . 698

A.2.10 External Interfaces C Bindings . . . . . . . . . . . . . . . . . . . . . 700

A.2.11 I/O C Bindings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 700

A.2.12 Language Bindings C Bindings . . . . . . . . . . . . . . . . . . . . . 703

A.2.13 Tools / Proﬁling Interface C Bindings . . . . . . . . . . . . . . . . . 704

A.2.14 Tools / MPI Tool Information Interface C Bindings . . . . . . . . . 704

A.2.15 Deprecated C Bindings . . . . . . . . . . . . . . . . . . . . . . . . . 705

A.3 Fortran 2008 Bindings with the mpi_f08 Module . . . . . . . . . . . . . . . 707

A.3.1 Point-to-Point Communication Fortran 2008 Bindings . . . . . . . . 707

A.3.2 Datatypes Fortran 2008 Bindings . . . . . . . . . . . . . . . . . . . . 712

A.3.3 Collective Communication Fortran 2008 Bindings . . . . . . . . . . . 717

A.3.4 Groups, Contexts, Communicators, and Caching Fortran 2008 Bindings724

A.3.5 Process Topologies Fortran 2008 Bindings . . . . . . . . . . . . . . . 731

A.3.6 MPI Environmental Management Fortran 2008 Bindings . . . . . . . 736

A.3.7 The Info Object Fortran 2008 Bindings . . . . . . . . . . . . . . . . 739

A.3.8 Process Creation and Management Fortran 2008 Bindings . . . . . . 740

A.3.9 One-Sided Communications Fortran 2008 Bindings . . . . . . . . . . 741

A.3.10 External Interfaces Fortran 2008 Bindings . . . . . . . . . . . . . . . 746

A.3.11 I/O Fortran 2008 Bindings . . . . . . . . . . . . . . . . . . . . . . . 747

A.3.12 Language Bindings Fortran 2008 Bindings . . . . . . . . . . . . . . . 754

A.3.13 Tools / Proﬁling Interface Fortran 2008 Bindings . . . . . . . . . . . 755

A.4 Fortran Bindings with mpif.h or the mpi Module . . . . . . . . . . . . . . . 756

A.4.1 Point-to-Point Communication Fortran Bindings . . . . . . . . . . . 756

A.4.2 Datatypes Fortran Bindings . . . . . . . . . . . . . . . . . . . . . . . 759

A.4.3 Collective Communication Fortran Bindings . . . . . . . . . . . . . . 761

A.4.4 Groups, Contexts, Communicators, and Caching Fortran Bindings . 765

A.4.5 Process Topologies Fortran Bindings . . . . . . . . . . . . . . . . . . 769

A.4.6 MPI Environmental Management Fortran Bindings . . . . . . . . . . 772

xv

A.4.7 The Info Object Fortran Bindings . . . . . . . . . . . . . . . . . . . 774 A.4.8 Process Creation and Management Fortran Bindings . . . . . . . . . 775 A.4.9 One-Sided Communications Fortran Bindings . . . . . . . . . . . . . 776 A.4.10 External Interfaces Fortran Bindings . . . . . . . . . . . . . . . . . . 779 A.4.11 I/O Fortran Bindings . . . . . . . . . . . . . . . . . . . . . . . . . . 780 A.4.12 Language Bindings Fortran Bindings . . . . . . . . . . . . . . . . . . 784 A.4.13 Tools / Proﬁling Interface Fortran Bindings . . . . . . . . . . . . . . 785 A.4.14 Deprecated Fortran Bindings . . . . . . . . . . . . . . . . . . . . . . 785

B Change-Log

787

B.1 Changes from Version 2.2 to Version 3.0 . . . . . . . . . . . . . . . . . . . . 787

B.1.1 Fixes to Errata in Previous Versions of MPI . . . . . . . . . . . . . . 787

B.1.2 Changes in MPI-3.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . 788

B.2 Changes from Version 2.1 to Version 2.2 . . . . . . . . . . . . . . . . . . . . 793

B.3 Changes from Version 2.0 to Version 2.1 . . . . . . . . . . . . . . . . . . . . 796

Bibliography

801

Examples Index

806

MPI Constant and Predeﬁned Handle Index

809

MPI Declarations Index

814

MPI Callback Function Prototype Index

815

MPI Function Index

816

xvi

List of Figures
5.1 Collective comminucations, an overview . . . . . . . . . . . . . . . . . . . . 143 5.2 Intercommunicator allgather . . . . . . . . . . . . . . . . . . . . . . . . . . . 146 5.3 Intercommunicator reduce-scatter . . . . . . . . . . . . . . . . . . . . . . . . 147 5.4 Gather example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 5.5 Gatherv example with strides . . . . . . . . . . . . . . . . . . . . . . . . . . 154 5.6 Gatherv example, 2-dimensional . . . . . . . . . . . . . . . . . . . . . . . . 155 5.7 Gatherv example, 2-dimensional, subarrays with diﬀerent sizes . . . . . . . 156 5.8 Gatherv example, 2-dimensional, subarrays with diﬀerent sizes and strides . 158 5.9 Scatter example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163 5.10 Scatterv example with strides . . . . . . . . . . . . . . . . . . . . . . . . . . 163 5.11 Scatterv example with diﬀerent strides and counts . . . . . . . . . . . . . . 164 5.12 Race conditions with point-to-point and collective communications . . . . . 217 5.13 Overlapping Communicators Example . . . . . . . . . . . . . . . . . . . . . 221
6.1 Intercommunicator creation using MPI_COMM_CREATE . . . . . . . . . . . 242 6.2 Intercommunicator construction with MPI_COMM_SPLIT . . . . . . . . . . 246 6.3 Three-group pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 6.4 Three-group ring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
7.1 Set-up of process structure for two-dimensional parallel Poisson solver. . . . 331 7.2 Communication routine with local data copying and sparse neighborhood
all-to-all. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332 7.3 Communication routine with sparse neighborhood all-to-all-w and without
local data copying. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
11.1 Schematic description of the public/private window operations in the MPI_WIN_SEPARATE memory model for two overlapping windows. . . . . . . 436
11.2 Active target communication . . . . . . . . . . . . . . . . . . . . . . . . . . 439 11.3 Active target communication, with weak synchronization . . . . . . . . . . . 440 11.4 Passive target communication . . . . . . . . . . . . . . . . . . . . . . . . . . 441 11.5 Active target communication with several processes . . . . . . . . . . . . . . 444 11.6 Symmetric communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463 11.7 Deadlock situation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463 11.8 No deadlock . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
13.1 Etypes and ﬁletypes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490 13.2 Partitioning a ﬁle among parallel processes . . . . . . . . . . . . . . . . . . 490 13.3 Displacements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503 13.4 Example array ﬁle layout . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
xvii

13.5 Example local array ﬁletype for process 1 . . . . . . . . . . . . . . . . . . . 554 17.1 Status conversion routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649
xviii

1

2

3

4

5

List of Tables

6 7

8

9

10
2.1 Deprecated and Removed constructs . . . . . . . . . . . . . . . . . . . . . . 18 11

12

3.1 Predeﬁned MPI datatypes corresponding to Fortran datatypes . . . . . . . 25 13

3.2 Predeﬁned MPI datatypes corresponding to C datatypes . . . . . . . . . . 26 14

3.3 Predeﬁned MPI datatypes corresponding to both C and Fortran datatypes

27

15

3.4 Predeﬁned MPI datatypes corresponding to C++ datatypes . . . . . . . . . 27 16

4.1 combiner values returned from MPI_TYPE_GET_ENVELOPE . . . . . . . . 117 17 18

6.1 MPI_COMM_* Function Behavior (in Inter-Communication Mode) . . . . . 259 19
20
8.1 Error classes (Part 1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350 21 8.2 Error classes (Part 2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351 22

23

11.1 C types of attribute value argument to MPI_WIN_GET_ATTR and

24

MPI_WIN_SET_ATTR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416 25

11.2 Error classes in one-sided communication routines . . . . . . . . . . . . . . 453 26

13.1 Data access routines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505 27 13.2 “external32” sizes of predeﬁned datatypes . . . . . . . . . . . . . . . . . . . 536 28 13.3 I/O Error Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551 29
30

14.1 MPI tool information interface verbosity levels . . . . . . . . . . . . . . . . . 562 31 14.2 Constants to identify associations of variables . . . . . . . . . . . . . . . . . 563 32 14.3 MPI datatypes that can be used by the MPI tool information interface . . . 565 33 14.4 Scopes for control variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 569 34 14.5 Return codes used in functions of the MPI tool information interface . . . . 589 35
36
16.1 Removed MPI-1 functions and their replacements . . . . . . . . . . . . . . 595 37 16.2 Removed MPI-1 datatypes and their replacements . . . . . . . . . . . . . . 596 38 16.3 Removed MPI-1 constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596 39 16.4 Removed MPI-1 callback prototypes and their replacements . . . . . . . . . 596 40

41
17.1 Occurrence of Fortran optimization problems . . . . . . . . . . . . . . . . . 632 42

43

44

45

46

47

48

xix

1 Acknowledgments
2

3

4

5

This document is the product of a number of distinct eﬀorts in three distinct phases:

6 one for each of MPI-1, MPI-2, and MPI-3. This section describes these in historical order,

7 starting with MPI-1. Some eﬀorts, particularly parts of MPI-2, had distinct groups of

8 individuals associated with them, and these eﬀorts are detailed separately.

9

10

11

12

This document represents the work of many people who have served on the MPI Forum.

13 The meetings have been attended by dozens of people from many parts of the world. It is

14 the hard and dedicated work of this group that has led to the MPI standard.

15

The technical development was carried out by subgroups, whose work was reviewed

16 by the full committee. During the period of development of the Message-Passing Interface

17 (MPI), many people helped with this eﬀort.

18

Those who served as primary coordinators in MPI-1.0 and MPI-1.1 are:

19

20

• Jack Dongarra, David Walker, Conveners and Meeting Chairs

21
• Ewing Lusk, Bob Knighten, Minutes
22

23

• Marc Snir, William Gropp, Ewing Lusk, Point-to-Point Communication

24

25

• Al Geist, Marc Snir, Steve Otto, Collective Communication

26
• Steve Otto, Editor
27

28

• Rolf Hempel, Process Topologies

29

30

• Ewing Lusk, Language Binding

31

• William Gropp, Environmental Management

32

33

• James Cownie, Proﬁling

34

35

• Tony Skjellum, Lyndon Clarke, Marc Snir, Richard Littleﬁeld, Mark Sears, Groups,

36

Contexts, and Communicators

37

• Steven Huss-Lederman, Initial Implementation Subset

38

39

The following list includes some of the active participants in the MPI-1.0 and MPI-1.1

40 process not mentioned above.

41

42

43

44

45

46

47

48

xx

Ed Anderson Robert Babb

Joe Baron

Eric Barszcz

1

Scott Berryman Rob Bjornson

Nathan Doss

Anne Elster

2

Jim Feeney

Vince Fernando Sam Fineberg Jon Flower

3

Daniel Frye

Ian Glendinning Adam Greenberg Robert Harrison

4

Leslie Hart

Tom Haupt

Don Heller

Tom Henderson

5

Alex Ho

C.T. Howard Ho Gary Howell

John Kapenga

6

James Kohl

Susan Krauss

Bob Leary

Arthur Maccabe

7

Peter Madams Alan Mainwaring Oliver McBryan Phil McKinley

8

Charles Mosher Dan Nessett

Peter Pacheco Howard Palmer

9

Paul Pierce

Sanjay Ranka

Peter Rigsbee

Arch Robison

10

Erich Schikuta Ambuj Singh

Alan Sussman Robert Tomlinson

11

Robert G. Voigt Dennis Weeks Stephen Wheat Steve Zenith

12

13

14
The University of Tennessee and Oak Ridge National Laboratory made the draft avail15
able by anonymous FTP mail servers and were instrumental in distributing the document. 16 The work on the MPI-1 standard was supported in part by ARPA and NSF under grant 17
ASC-9310330, the National Science Foundation Science and Technology Center Cooperative 18
Agreement No. CCR-8809615, and by the Commission of the European Community through 19
Esprit project P6643 (PPPE). 20

21

22

MPI-1.2 and MPI-2.0:

23

Those who served as primary coordinators in MPI-1.2 and MPI-2.0 are:

24

25

• Ewing Lusk, Convener and Meeting Chair

26

27

• Steve Huss-Lederman, Editor

28

29
• Ewing Lusk, Miscellany 30

• Bill Saphir, Process Creation and Management

31

32

• Marc Snir, One-Sided Communications

33

34
• Bill Gropp and Anthony Skjellum, Extended Collective Operations 35

• Steve Huss-Lederman, External Interfaces

36

37

• Bill Nitzberg, I/O

38

39
• Andrew Lumsdaine, Bill Saphir, and Jeﬀ Squyres, Language Bindings 40

• Anthony Skjellum and Arkady Kanevsky, Real-Time

41

42

The following list includes some of the active participants who attended MPI-2 Forum 43

meetings and are not mentioned above.

44

45

46

47

48

xxi

1

Greg Astfalk

Robert Babb

Ed Benson

Rajesh Bordawekar

2

Pete Bradley

Peter Brennan

Ron Brightwell Maciej Brodowicz

3

Eric Brunner

Greg Burns

Margaret Cahir Pang Chen

4

Ying Chen

Albert Cheng

Yong Cho

Joel Clark

5

Lyndon Clarke

Laurie Costello

Dennis Cottel

Jim Cownie

6

Zhenqian Cui

Suresh Damodaran-Kamal

Raja Daoud

7

Judith Devaney

David DiNucci

Doug Doeﬂer

Jack Dongarra

8

Terry Dontje

Nathan Doss

Anne Elster

Mark Fallon

9

Karl Feind

Sam Fineberg

Craig Fischberg Stephen Fleischman

10

Ian Foster

Hubertus Franke

Richard Frost

Al Geist

11

Robert George

David Greenberg

John Hagedorn Kei Harada

12

Leslie Hart

Shane Hebert

Rolf Hempel

Tom Henderson

13

Alex Ho

Hans-Christian Hoppe Joefon Jann

Terry Jones

14

Karl Kesselman

Koichi Konishi

Susan Kraus

Steve Kubica

15

Steve Landherr

Mario Lauria

Mark Law

Juan Leon

16

Lloyd Lewins

Ziyang Lu

Bob Madahar

Peter Madams

17

John May

Oliver McBryan

Brian McCandless Tyce McLarty

18 Thom McMahon Harish Nag

Nick Nevin

Jarek Nieplocha

19

Ron Oldﬁeld

Peter Ossadnik

Steve Otto

Peter Pacheco

20

Yoonho Park

Perry Partow

Pratap Pattnaik Elsie Pierce

21

Paul Pierce

Heidi Poxon

Jean-Pierre Prost Boris Protopopov

22

James Pruyve

Rolf Rabenseifner

Joe Rieken

Peter Rigsbee

23

Tom Robey

Anna Rounbehler

Nobutoshi Sagawa Arindam Saha

24

Eric Salo

Darren Sanders

Eric Sharakan

Andrew Sherman

25

Fred Shirley

Lance Shuler

A. Gordon Smith Ian Stockdale

26

David Taylor

Stephen Taylor

Greg Tensa

Rajeev Thakur

27 Marydell Tholburn Dick Treumann

Simon Tsang

Manuel Ujaldon

28

David Walker

Jerrell Watts

Klaus Wolf

Parkson Wong

29

Dave Wright

30
The MPI Forum also acknowledges and appreciates the valuable input from people via
31
e-mail and in person.
32

33
The following institutions supported the MPI-2 eﬀort through time and travel support
34
for the people listed above.
35

36

Argonne National Laboratory

37

Bolt, Beranek, and Newman

38

California Institute of Technology

39

Center for Computing Sciences

40

Convex Computer Corporation

41

Cray Research

42

Digital Equipment Corporation

43

Dolphin Interconnect Solutions, Inc.

44

Edinburgh Parallel Computing Centre

45

General Electric Company

46

German National Research Center for Information Technology

47

Hewlett-Packard

48

Hitachi

xxii

Hughes Aircraft Company

1

Intel Corporation

2

International Business Machines

3

Khoral Research

4

Lawrence Livermore National Laboratory

5

Los Alamos National Laboratory

6

MPI Software Techology, Inc.

7

Mississippi State University

8

NEC Corporation

9

National Aeronautics and Space Administration

10

National Energy Research Scientiﬁc Computing Center

11

National Institute of Standards and Technology

12

National Oceanic and Atmospheric Adminstration

13

Oak Ridge National Laboratory

14

Ohio State University

15

PALLAS GmbH

16

Paciﬁc Northwest National Laboratory

17

Pratt & Whitney

18

San Diego Supercomputer Center

19

Sanders, A Lockheed-Martin Company

20

Sandia National Laboratories

21

Schlumberger

22

Scientiﬁc Computing Associates, Inc.

23

Silicon Graphics Incorporated

24

Sky Computers

25

Sun Microsystems Computer Corporation

26

Syracuse University

27

The MITRE Corporation

28

Thinking Machines Corporation

29

United States Navy

30

University of Colorado

31

University of Denver

32

University of Houston

33

University of Illinois

34

University of Maryland

35

University of Notre Dame

36

University of San Fransisco

37

University of Stuttgart Computing Center

38

University of Wisconsin

39

40

MPI-2 operated on a very tight budget (in reality, it had no budget when the ﬁrst 41

meeting was announced). Many institutions helped the MPI-2 eﬀort by supporting the 42

eﬀorts and travel of the members of the MPI Forum. Direct support was given by NSF and 43

DARPA under NSF contract CDA-9115428 for travel by U.S. academic participants and 44

Esprit under project HPC Standards (21111) for European participants.

45

46

47

48

xxiii

1 MPI-1.3 and MPI-2.1:
2
3 The editors and organizers of the combined documents have been:

4

• Richard Graham, Convener and Meeting Chair

5

6

• Jack Dongarra, Steering Committee

7
• Al Geist, Steering Committee
8

9

• Bill Gropp, Steering Committee

10

11

• Rainer Keller, Merge of MPI-1.3

12
• Andrew Lumsdaine, Steering Committee
13

14

• Ewing Lusk, Steering Committee, MPI-1.1-Errata (Oct. 12, 1998) MPI-2.1-Errata

15

Ballots 1, 2 (May 15, 2002)

16

17

• Rolf Rabenseifner, Steering Committee, Merge of MPI-2.1 and MPI-2.1-Errata Ballots

18

3, 4 (2008)

19
All chapters have been revisited to achieve a consistent MPI-2.1 text. Those who served
20
as authors for the necessary modiﬁcations are:
21

22

• Bill Gropp, Front matter, Introduction, and Bibliography

23

24

• Richard Graham, Point-to-Point Communication

25
• Adam Moody, Collective Communication
26

27

• Richard Treumann, Groups, Contexts, and Communicators

28

29

• Jesper Larsson Tr¨aﬀ, Process Topologies, Info-Object, and One-Sided Communica-

30

tions

31
• George Bosilca, Environmental Management
32

33

• David Solt, Process Creation and Management

34

35

• Bronis R. de Supinski, External Interfaces, and Proﬁling

36
• Rajeev Thakur, I/O
37

38

• Jeﬀrey M. Squyres, Language Bindings and MPI 2.1 Secretary

39

40

• Rolf Rabenseifner, Deprecated Functions and Annex Change-Log

41
• Alexander Supalov and Denis Nagorny, Annex Language Bindings
42

43

The following list includes some of the active participants who attended MPI-2 Forum

44 meetings and in the e-mail discussions of the errata items and are not mentioned above.

45

46

47

48

xxiv

Pavan Balaji

Purushotham V. Bangalore Brian Barrett

1

Richard Barrett

Christian Bell

Robert Blackmore

2

Gil Bloch

Ron Brightwell

Jeﬀrey Brown

3

Darius Buntinas

Jonathan Carter

Nathan DeBardeleben

4

Terry Dontje

Gabor Dozsa

Edric Ellis

5

Karl Feind

Edgar Gabriel

Patrick Geoﬀray

6

David Gingold

Dave Goodell

Erez Haba

7

Robert Harrison

Thomas Herault

Steve Hodson

8

Torsten Hoeﬂer

Joshua Hursey

Yann Kalemkarian

9

Matthew Koop

Quincey Koziol

Sameer Kumar

10

Miron Livny

Kannan Narasimhan

Mark Pagel

11

Avneesh Pant

Steve Poole

Howard Pritchard

12

Craig Rasmussen Hubert Ritzdorf

Rob Ross

13

Tony Skjellum

Brian Smith

Vinod Tipparaju

14

Jesper Larsson Tr¨aﬀ Keith Underwood

15

16
The MPI Forum also acknowledges and appreciates the valuable input from people via 17
e-mail and in person. 18

19
The following institutions supported the MPI-2 eﬀort through time and travel support 20
for the people listed above. 21

Argonne National Laboratory

22

Bull

23

Cisco Systems, Inc.

24

Cray Inc.

25

The HDF Group

26

Hewlett-Packard

27

IBM T.J. Watson Research

28

Indiana University

29

Institut National de Recherche en Informatique et Automatique (INRIA)

30

Intel Corporation

31

Lawrence Berkeley National Laboratory

32

Lawrence Livermore National Laboratory

33

Los Alamos National Laboratory

34

Mathworks

35

Mellanox Technologies

36

Microsoft

37

Myricom

38

NEC Laboratories Europe, NEC Europe Ltd.

39

Oak Ridge National Laboratory

40

Ohio State University

41

Paciﬁc Northwest National Laboratory

42

QLogic Corporation

43

Sandia National Laboratories

44

SiCortex

45

Silicon Graphics Incorporated

46

Sun Microsystems, Inc.

47

University of Alabama at Birmingham

48

xxv

1

University of Houston

2

University of Illinois at Urbana-Champaign

3

University of Stuttgart, High Performance Computing Center Stuttgart (HLRS)

4

University of Tennessee, Knoxville

5

University of Wisconsin

6

7

8

Funding for the MPI Forum meetings was partially supported by award #CCF-0816909

9 from the National Science Foundation. In addition, the HDF Group provided travel support

10 for one U.S. academic.

11

12
13 MPI-2.2:

14
All chapters have been revisited to achieve a consistent MPI-2.2 text. Those who served as
15
authors for the necessary modiﬁcations are:
16

17

• William Gropp, Front matter, Introduction, and Bibliography; MPI 2.2 chair.

18

19

• Richard Graham, Point-to-Point Communication and Datatypes

20
• Adam Moody, Collective Communication
21

22

• Torsten Hoeﬂer, Collective Communication and Process Topologies

23

24

• Richard Treumann, Groups, Contexts, and Communicators

25
• Jesper Larsson Tr¨aﬀ, Process Topologies, Info-Object and One-Sided Communications
26

27

• George Bosilca, Datatypes and Environmental Management

28

29

• David Solt, Process Creation and Management

30
• Bronis R. de Supinski, External Interfaces, and Proﬁling
31

32

• Rajeev Thakur, I/O

33

34

• Jeﬀrey M. Squyres, Language Bindings and MPI 2.2 Secretary

35

• Rolf Rabenseifner, Deprecated Functions, Annex Change-Log, and Annex Language

36

Bindings

37

38

• Alexander Supalov, Annex Language Bindings

39

40

The following list includes some of the active participants who attended MPI-2 Forum

41 meetings and in the e-mail discussions of the errata items and are not mentioned above.

42

43

44

45

46

47

48

xxvi

Pavan Balaji

Purushotham V. Bangalore Brian Barrett

1

Richard Barrett

Christian Bell

Robert Blackmore

2

Gil Bloch

Ron Brightwell

Greg Bronevetsky

3

Jeﬀ Brown

Darius Buntinas

Jonathan Carter

4

Nathan DeBardeleben Terry Dontje

Gabor Dozsa

5

Edric Ellis

Karl Feind

Edgar Gabriel

6

Patrick Geoﬀray

Johann George

David Gingold

7

David Goodell

Erez Haba

Robert Harrison

8

Thomas Herault

Marc-Andr´e Hermanns

Steve Hodson

9

Joshua Hursey

Yutaka Ishikawa

Bin Jia

10

Hideyuki Jitsumoto Terry Jones

Yann Kalemkarian

11

Ranier Keller

Matthew Koop

Quincey Koziol

12

Manojkumar Krishnan Sameer Kumar

Miron Livny

13

Andrew Lumsdaine Miao Luo

Ewing Lusk

14

Timothy I. Mattox

Kannan Narasimhan

Mark Pagel

15

Avneesh Pant

Steve Poole

Howard Pritchard

16

Craig Rasmussen

Hubert Ritzdorf

Rob Ross

17

Martin Schulz

Pavel Shamis

Galen Shipman

18

Christian Siebert

Anthony Skjellum

Brian Smith

19

Naoki Sueyasu

Vinod Tipparaju

Keith Underwood

20

Rolf Vandevaart

Abhinav Vishnu

Weikuan Yu

21

22
The MPI Forum also acknowledges and appreciates the valuable input from people via 23
e-mail and in person. 24

25
The following institutions supported the MPI-2.2 eﬀort through time and travel support 26
for the people listed above. 27 Argonne National Laboratory 28 Auburn University 29 Bull 30 Cisco Systems, Inc. 31 Cray Inc. 32 Forschungszentrum Ju¨lich 33 Fujitsu 34 The HDF Group 35 Hewlett-Packard 36 International Business Machines 37 Indiana University 38 Institut National de Recherche en Informatique et Automatique (INRIA) 39 Institute for Advanced Science & Engineering Corporation 40 Intel Corporation 41 Lawrence Berkeley National Laboratory 42 Lawrence Livermore National Laboratory 43 Los Alamos National Laboratory 44 Mathworks 45 Mellanox Technologies 46 Microsoft 47 Myricom 48

xxvii

1

NEC Corporation

2

Oak Ridge National Laboratory

3

Ohio State University

4

Paciﬁc Northwest National Laboratory

5

QLogic Corporation

6

RunTime Computing Solutions, LLC

7

Sandia National Laboratories

8

SiCortex, Inc.

9

Silicon Graphics Inc.

10

Sun Microsystems, Inc.

11

Tokyo Institute of Technology

12

University of Alabama at Birmingham

13

University of Houston

14

University of Illinois at Urbana-Champaign

15

University of Stuttgart, High Performance Computing Center Stuttgart (HLRS)

16

University of Tennessee, Knoxville

17

University of Tokyo

18

University of Wisconsin

19

20

Funding for the MPI Forum meetings was partially supported by awards #CCF-

21 0816909 and #CCF-1144042 from the National Science Foundation. In addition, the HDF

22 Group provided travel support for one U.S. academic.

23

24
25 MPI-3:

26 MPI-3 is a signﬁcant eﬀort to extend and modernize the MPI Standard. 27 The editors and organizers of the MPI-3 have been:
28

29

• William Gropp, Steering committee, Front matter, Introduction, Groups, Contexts,

30

and Communicators, One-Sided Communications, and Bibliography

31

32

• Richard Graham, Steering committee, Point-to-Point Communication, Meeting Con-

33

vener, and MPI-3 chair

34

• Torsten Hoeﬂer, Collective Communication, One-Sided Communications, and Process

35

Topologies

36

37

• George Bosilca, Datatypes and Environmental Management

38

39

• David Solt, Process Creation and Management

40

• Bronis R. de Supinski, External Interfaces and Tool Support

41

42

• Rajeev Thakur, I/O and One-Sided Communications

43

44

• Darius Buntinas, Info Object

45

• Jeﬀrey M. Squyres, Language Bindings and MPI 3.0 Secretary

46

47

• Rolf Rabenseifner, Steering committee, Terms and Deﬁnitions, and Fortran Bindings,

48

Deprecated Functions, Annex Change-Log, and Annex Language Bindings

xxviii

• Craig Rasmussen, Fortran Bindings

1

2

The following list includes some of the active participants who attended MPI-3 Forum 3

meetings or participated in the e-mail discussions and who are not mentioned above.

4

Tatsuya Abe

Tomoya Adachi

Sadaf Alam

5

Reinhold Bader

Pavan Balaji

Purushotham V. Bangalore

6

Brian Barrett

Richard Barrett

Robert Blackmore

7

Aurelien Bouteiller

Ron Brightwell

Greg Bronevetsky

8

Jed Brown

Darius Buntinas

Devendar Bureddy

9

Arno Candel

George Carr

Mohamad Chaarawi

10

Raghunath Raja Chandrasekar James Dinan

Terry Dontje

11

Edgar Gabriel

Balazs Geroﬁ

Brice Goglin

12

David Goodell

Manjunath Gorentla Erez Haba

13

Jeﬀ Hammond

Thomas Herault

Marc-Andr´e Hermanns

14

Jennifer Herrett-Skjellum

Nathan Hjelm

Atsushi Hori

15

Joshua Hursey

Marty Itzkowitz

Yutaka Ishikawa

16

Nysal Jan

Bin Jia

Hideyuki Jitsumoto

17

Yann Kalemkarian

Krishna Kandalla Takahiro Kawashima

18

Chulho Kim

Dries Kimpe

Christof Klausecker

19

Alice Koniges

Quincey Koziol

Dieter Kranzlmueller

20

Manojkumar Krishnan

Sameer Kumar

Eric Lantz

21

Jay Lofstead

Bill Long

Andrew Lumsdaine

22

Miao Luo

Ewing Lusk

Adam Moody

23

Nick M. Maclaren

Amith Mamidala

Guillaume Mercier

24

Scott McMillan

Douglas Miller

Kathryn Mohror

25

Tim Murray

Tomotake Nakamura Takeshi Nanri

26

Steve Oyanagi

Mark Pagel

Swann Perarnau

27

Sreeram Potluri

Howard Pritchard Rolf Riesen

28

Hubert Ritzdorf

Kuninobu Sasaki

Timo Schneider

29

Martin Schulz

Gilad Shainer

Christian Siebert

30

Anthony Skjellum

Brian Smith

Marc Snir

31

Raﬀaele Giuseppe Solca

Shinji Sumimoto

Alexander Supalov

32

Sayantan Sur

Masamichi Takagi Fabian Tillier

33

Vinod Tipparaju

Jesper Larsson Tr¨aﬀ Richard Treumann

34

Keith Underwood

Rolf Vandevaart

Anh Vo

35

Abhinav Vishnu

Min Xie

Enqiang Zhou

36

37

The MPI Forum also acknowledges and appreciates the valuable input from people via 38

e-mail and in person.

39

The MPI Forum also thanks those that provided feedback during the public comment 40

period. In particular, the Forum would like to thank Jeremiah Wilcock for providing detailed 41

comments on the entire draft standard.

42

43

The following institutions supported the MPI-3 eﬀort through time and travel support 44

for the people listed above.

45

Argonne National Laboratory

46

Bull

47

Cisco Systems, Inc.

48

xxix

1

Cray Inc.

2

CSCS

3

ETH Zurich

4

Fujitsu Ltd.

5

German Research School for Simulation Sciences

6

The HDF Group

7

Hewlett-Packard

8

International Business Machines

9

IBM India Private Ltd

10

Indiana University

11

Institut National de Recherche en Informatique et Automatique (INRIA)

12

Institute for Advanced Science & Engineering Corporation

13

Intel Corporation

14

Lawrence Berkeley National Laboratory

15

Lawrence Livermore National Laboratory

16

Los Alamos National Laboratory

17

Mellanox Technologies, Inc.

18

Microsoft Corporation

19

NEC Corporation

20

National Oceanic and Atmospheric Administration, Global Systems Division

21

NVIDIA Corporation

22

Oak Ridge National Laboratory

23

The Ohio State University

24

Oracle America

25

Platform Computing

26

RIKEN AICS

27

RunTime Computing Solutions, LLC

28

Sandia National Laboratories

29

Technical University of Chemnitz

30

Tokyo Institute of Technology

31

University of Alabama at Birmingham

32

University of Chicago

33

University of Houston

34

University of Illinois at Urbana-Champaign

35

University of Stuttgart, High Performance Computing Center Stuttgart (HLRS)

36

University of Tennessee, Knoxville

37

University of Tokyo

38

39

Funding for the MPI Forum meetings was partially supported by awards #CCF-

40 0816909 and #CCF-1144042 from the National Science Foundation. In addition, the HDF

41 Group and Sandia National Laboratories provided travel support for one U.S. academic

42 each.

43

44

45

46

47

48

xxx

1

2

3

4

5

Chapter 1

6

7

8

9

Introduction to MPI

10

11

12

13

1.1 Overview and Goals

14

15

MPI (Message-Passing Interface) is a message-passing library interface speciﬁcation. All 16

parts of this deﬁnition are signiﬁcant. MPI addresses primarily the message-passing parallel 17

programming model, in which data is moved from the address space of one process to 18

that of another process through cooperative operations on each process. Extensions to the 19

“classical” message-passing model are provided in collective operations, remote-memory 20

access operations, dynamic process creation, and parallel I/O. MPI is a speciﬁcation, not 21

an implementation; there are multiple implementations of MPI. This speciﬁcation is for a 22

library interface; MPI is not a language, and all MPI operations are expressed as functions, 23

subroutines, or methods, according to the appropriate language bindings which, for C and 24

Fortran, are part of the MPI standard. The standard has been deﬁned through an open 25

process by a community of parallel computing vendors, computer scientists, and application 26

developers. The next few sections provide an overview of the history of MPI’s development. 27

The main advantages of establishing a message-passing standard are portability and 28

ease of use. In a distributed memory communication environment in which the higher level 29

routines and/or abstractions are built upon lower level message-passing routines the beneﬁts 30

of standardization are particularly apparent. Furthermore, the deﬁnition of a message- 31

passing standard, such as that proposed here, provides vendors with a clearly deﬁned base 32

set of routines that they can implement eﬃciently, or in some cases for which they can 33

provide hardware support, thereby enhancing scalability.

34

The goal of the Message-Passing Interface simply stated is to develop a widely used 35

standard for writing message-passing programs. As such the interface should establish a 36

practical, portable, eﬃcient, and ﬂexible standard for message passing.

37

A complete list of goals follows.

38

39

• Design an application programming interface (not necessarily for compilers or a system 40

implementation library).

41

42

• Allow eﬃcient communication: Avoid memory-to-memory copying, allow overlap of 43

computation and communication, and oﬄoad to communication co-processors, where 44

available.

45

46
• Allow for implementations that can be used in a heterogeneous environment. 47

• Allow convenient C and Fortran bindings for the interface.

48

1

2

CHAPTER 1. INTRODUCTION TO MPI

1

• Assume a reliable communication interface: the user need not cope with communica-

2

tion failures. Such failures are dealt with by the underlying communication subsystem.

3

4

• Deﬁne an interface that can be implemented on many vendor’s platforms, with no

5

signiﬁcant changes in the underlying communication and system software.

6
• Semantics of the interface should be language independent.
7

8

• The interface should be designed to allow for thread safety.

9

10
1.2 Background of MPI-1.0
11

12
MPI sought to make use of the most attractive features of a number of existing message-
13
passing systems, rather than selecting one of them and adopting it as the standard. Thus,
14
MPI was strongly inﬂuenced by work at the IBM T. J. Watson Research Center [1, 2],
15
Intel’s NX/2 [50], Express [13], nCUBE’s Vertex [46], p4 [8, 9], and PARMACS [5, 10].
16
Other important contributions have come from Zipcode [53, 54], Chimp [19, 20], PVM
17
[4, 17], Chameleon [27], and PICL [25].
18
The MPI standardization eﬀort involved about 60 people from 40 organizations mainly
19
from the United States and Europe. Most of the major vendors of concurrent computers
20
were involved in MPI, along with researchers from universities, government laboratories, and
21
industry. The standardization process began with the Workshop on Standards for Message-
22
Passing in a Distributed Memory Environment, sponsored by the Center for Research on
23
Parallel Computing, held April 29-30, 1992, in Williamsburg, Virginia [60]. At this workshop
24
the basic features essential to a standard message-passing interface were discussed, and a
25
working group established to continue the standardization process.
26
A preliminary draft proposal, known as MPI-1, was put forward by Dongarra, Hempel,
27
Hey, and Walker in November 1992, and a revised version was completed in February
28
1993 [18]. MPI-1 embodied the main features that were identiﬁed at the Williamsburg
29
workshop as being necessary in a message passing standard. Since MPI-1 was primarily
30
intended to promote discussion and “get the ball rolling,” it focused mainly on point-to-point
31
communications. MPI-1 brought to the forefront a number of important standardization
32
issues, but did not include any collective communication routines and was not thread-safe.
33
In November 1992, a meeting of the MPI working group was held in Minneapolis, at
34
which it was decided to place the standardization process on a more formal footing, and to
35
generally adopt the procedures and organization of the High Performance Fortran Forum.
36
Subcommittees were formed for the major component areas of the standard, and an email
37
discussion service established for each. In addition, the goal of producing a draft MPI
38
standard by the Fall of 1993 was set. To achieve this goal the MPI working group met every
39
6 weeks for two days throughout the ﬁrst 9 months of 1993, and presented the draft MPI
40
standard at the Supercomputing 93 conference in November 1993. These meetings and the
41
email discussion together constituted the MPI Forum, membership of which has been open
42
to all members of the high performance computing community.
43

44
45 1.3 Background of MPI-1.1, MPI-1.2, and MPI-2.0

46
47 Beginning in March 1995, the MPI Forum began meeting to consider corrections and exten48 sions to the original MPI Standard document [22]. The ﬁrst product of these deliberations

1.4. BACKGROUND OF MPI-1.3 AND MPI-2.1

3

was Version 1.1 of the MPI speciﬁcation, released in June of 1995 [23] (see

1

http://www.mpi-forum.org for oﬃcial MPI document releases). At that time, eﬀort fo- 2

cused in ﬁve areas.

3

4

1. Further corrections and clariﬁcations for the MPI-1.1 document.

5

6
2. Additions to MPI-1.1 that do not signiﬁcantly change its types of functionality (new 7 datatype constructors, language interoperability, etc.). 8

3. Completely new types of functionality (dynamic processes, one-sided communication, 9

parallel I/O, etc.) that are what everyone thinks of as “MPI-2 functionality.”

10

11

4. Bindings for Fortran 90 and C++. MPI-2 speciﬁes C++ bindings for both MPI-1 and 12

MPI-2 functions, and extensions to the Fortran 77 binding of MPI-1 and MPI-2 to 13

handle Fortran 90 issues.

14

15
5. Discussions of areas in which the MPI process and framework seem likely to be useful, 16 but where more discussion and experience are needed before standardization (e.g., 17 zero-copy semantics on shared-memory machines, real-time speciﬁcations). 18

Corrections and clariﬁcations (items of type 1 in the above list) were collected in Chap- 19

ter 3 of the MPI-2 document: “Version 1.2 of MPI.” That chapter also contains the function 20

for identifying the version number. Additions to MPI-1.1 (items of types 2, 3, and 4 in the 21

above list) are in the remaining chapters of the MPI-2 document, and constitute the speciﬁ- 22

cation for MPI-2. Items of type 5 in the above list have been moved to a separate document, 23

the “MPI Journal of Development” (JOD), and are not part of the MPI-2 Standard.

24

This structure makes it easy for users and implementors to understand what level of 25

MPI compliance a given implementation has:

26

27

• MPI-1 compliance will mean compliance with MPI-1.3. This is a useful level of com- 28

pliance. It means that the implementation conforms to the clariﬁcations of MPI-1.1 29

function behavior given in Chapter 3 of the MPI-2 document. Some implementations 30

may require changes to be MPI-1 compliant.

31

• MPI-2 compliance will mean compliance with all of MPI-2.1.

32

33

• The MPI Journal of Development is not part of the MPI Standard.

34

35

It is to be emphasized that forward compatibility is preserved. That is, a valid MPI-1.1 36

program is both a valid MPI-1.3 program and a valid MPI-2.1 program, and a valid MPI-1.3 37

program is a valid MPI-2.1 program.

38

39

1.4 Background of MPI-1.3 and MPI-2.1

40

41

After the release of MPI-2.0, the MPI Forum kept working on errata and clariﬁcations for 42

both standard documents (MPI-1.1 and MPI-2.0). The short document “Errata for MPI-1.1” 43

was released October 12, 1998. On July 5, 2001, a ﬁrst ballot of errata and clariﬁcations for 44

MPI-2.0 was released, and a second ballot was voted on May 22, 2002. Both votes were done 45

electronically. Both ballots were combined into one document: “Errata for MPI-2,” May 46

15, 2002. This errata process was then interrupted, but the Forum and its e-mail reﬂectors 47

kept working on new requests for clariﬁcation.

48

4

CHAPTER 1. INTRODUCTION TO MPI

1

Restarting regular work of the MPI Forum was initiated in three meetings, at Eu-

2 roPVM/MPI’06 in Bonn, at EuroPVM/MPI’07 in Paris, and at SC’07 in Reno. In De-

3 cember 2007, a steering committee started the organization of new MPI Forum meetings at

4 regular 8-weeks intervals. At the January 14–16, 2008 meeting in Chicago, the MPI Forum

5 decided to combine the existing and future MPI documents to one document for each ver-

6 sion of the MPI standard. For technical and historical reasons, this series was started with

7 MPI-1.3. Additional Ballots 3 and 4 solved old questions from the errata list started in 1995

8 up to new questions from the last years. After all documents (MPI-1.1, MPI-2, Errata for

9 MPI-1.1 (Oct. 12, 1998), and MPI-2.1 Ballots 1-4) were combined into one draft document,

10 for each chapter, a chapter author and review team were deﬁned. They cleaned up the

11 document to achieve a consistent MPI-2.1 document. The ﬁnal MPI-2.1 standard document

12 was ﬁnished in June 2008, and ﬁnally released with a second vote in September 2008 in

13 the meeting at Dublin, just before EuroPVM/MPI’08. The major work of the current MPI

14 Forum is the preparation of MPI-3.

15

16
1.5 Background of MPI-2.2
17

18
MPI-2.2 is a minor update to the MPI-2.1 standard. This version addresses additional errors
19
and ambiguities that were not corrected in the MPI-2.1 standard as well as a small number
20
of extensions to MPI-2.1 that met the following criteria:
21

22

• Any correct MPI-2.1 program is a correct MPI-2.2 program.

23

24

• Any extension must have signiﬁcant beneﬁt for users.

25
• Any extension must not require signiﬁcant implementation eﬀort. To that end, all
26
such changes are accompanied by an open source implementation.
27

28 The discussions of MPI-2.2 proceeded concurrently with the MPI-3 discussions; in some 29 cases, extensions were proposed for MPI-2.2 but were later moved to MPI-3.
30

31
1.6 Background of MPI-3.0
32

33
MPI-3.0 is a major update to the MPI standard. The updates include the extension of
34
collective operations to include nonblocking versions, extensions to the one-sided operations,
35
and a new Fortran 2008 binding. In addition, the deprecated C++ bindings have been
36
removed, as well as many of the deprecated routines and MPI objects (such as the MPI_UB
37
datatype).
38

39
40 1.7 Who Should Use This Standard?

41
42 This standard is intended for use by all those who want to write portable message-passing 43 programs in Fortran and C (and access the C bindings from C++). This includes individual 44 application programmers, developers of software designed to run on parallel machines, and 45 creators of environments and tools. In order to be attractive to this wide audience, the 46 standard must provide a simple, easy-to-use interface for the basic user while not seman47 tically precluding the high-performance message-passing operations available on advanced 48 machines.

1.8. WHAT PLATFORMS ARE TARGETS FOR IMPLEMENTATION?

5

1.8 What Platforms Are Targets For Implementation?

1

2

The attractiveness of the message-passing paradigm at least partially stems from its wide 3

portability. Programs expressed this way may run on distributed-memory multiprocessors, 4

networks of workstations, and combinations of all of these. In addition, shared-memory 5

implementations, including those for multi-core processors and hybrid architectures, are 6

possible. The paradigm will not be made obsolete by architectures combining the shared- 7

and distributed-memory views, or by increases in network speeds. It thus should be both 8

possible and useful to implement this standard on a great variety of machines, including 9

those “machines” consisting of collections of other machines, parallel or not, connected by 10

a communication network.

11

The interface is suitable for use by fully general MIMD programs, as well as those writ- 12

ten in the more restricted style of SPMD. MPI provides many features intended to improve 13

performance on scalable parallel computers with specialized interprocessor communication 14

hardware. Thus, we expect that native, high-performance implementations of MPI will be 15

provided on such machines. At the same time, implementations of MPI on top of stan- 16

dard Unix interprocessor communication protocols will provide portability to workstation 17

clusters and heterogenous networks of workstations.

18

19

1.9 What Is Included In The Standard?

20

21

The standard includes:

22

23

• Point-to-point communication,

24

25

• Datatypes,

26

27
• Collective operations, 28

• Process groups,

29

30

• Communication contexts,

31

32
• Process topologies, 33

• Environmental management and inquiry,

34

35

• The Info object,

36

37
• Process creation and management, 38

• One-sided communication,

39

40

• External interfaces,

41

42
• Parallel ﬁle I/O,
43

• Language bindings for Fortran and C,

44

45

• Tool support.

46

47

48

6

CHAPTER 1. INTRODUCTION TO MPI

1 1.10 What Is Not Included In The Standard?

2
3 The standard does not specify:

4

5

• Operations that require more operating system support than is currently standard;

6

for example, interrupt-driven receives, remote execution, or active messages,

7
• Program construction tools,
8

9

• Debugging facilities.

10

11

There are many features that have been considered and not included in this standard.

12 This happened for a number of reasons, one of which is the time constraint that was self-

13 imposed in ﬁnishing the standard. Features that are not included can always be oﬀered as

14 extensions by speciﬁc implementations. Perhaps future versions of MPI will address some

15 of these issues.

16

17 1.11 Organization of this Document
18

19 The following is a list of the remaining chapters in this document, along with a brief 20 description of each.
21

22

• Chapter 2, MPI Terms and Conventions, explains notational terms and conventions

23

used throughout the MPI document.

24

25

• Chapter 3, Point to Point Communication, deﬁnes the basic, pairwise communication

26

subset of MPI. Send and receive are found here, along with many associated functions

27

designed to make basic communication powerful and eﬃcient.

28
• Chapter 4, Datatypes, deﬁnes a method to describe any data layout, e.g., an array of
29
structures in the memory, which can be used as message send or receive buﬀer.
30

31

• Chapter 5, Collective Communications, deﬁnes process-group collective communication

32

operations. Well known examples of this are barrier and broadcast over a group of

33

processes (not necessarily all the processes). With MPI-2, the semantics of collective

34

communication was extended to include intercommunicators. It also adds two new

35

collective operations. MPI-3 adds nonblocking collective operations.

36

37

• Chapter 6, Groups, Contexts, Communicators, and Caching, shows how groups of pro-

38

cesses are formed and manipulated, how unique communication contexts are obtained,

39

and how the two are bound together into a communicator.

40
• Chapter 7, Process Topologies, explains a set of utility functions meant to assist in
41
the mapping of process groups (a linearly ordered set) to richer topological structures
42
such as multi-dimensional grids.
43

44

• Chapter 8, MPI Environmental Management, explains how the programmer can manage

45

and make inquiries of the current MPI environment. These functions are needed for the

46

writing of correct, robust programs, and are especially important for the construction

47

of highly-portable message-passing programs.

48

1.11. ORGANIZATION OF THIS DOCUMENT

7

• Chapter 9, The Info Object, deﬁnes an opaque object, that is used as input in several 1

MPI routines.

2

3

• Chapter 10, Process Creation and Management, deﬁnes routines that allow for creation 4

of processes.

5

6
• Chapter 11, One-Sided Communications, deﬁnes communication routines that can be 7 completed by a single process. These include shared-memory operations (put/get) 8 and remote accumulate operations. 9

• Chapter 12, External Interfaces, deﬁnes routines designed to allow developers to layer 10

on top of MPI. This includes generalized requests, routines that decode MPI opaque 11

objects, and threads.

12

13

• Chapter 13, I/O, deﬁnes MPI support for parallel I/O.

14

15
• Chapter 14, Tool Support, covers interfaces that allow debuggers, performance ana16 lyzers, and other tools to obtain data about the operation of MPI processes. This 17 chapter includes Section 14.2 (Proﬁling Interface), which was a chapter in previous 18 versions of MPI. 19

• Chapter 15, Deprecated Functions, describes routines that are kept for reference. How- 20

ever usage of these functions is discouraged, as they may be deleted in future versions 21

of the standard.

22

23

• Chapter 16, Removed Interfaces, describes routines and constructs that have been 24

removed from MPI. These were deprecated in MPI-2, and the MPI Forum decided to 25

remove these from the MPI-3 standard.

26

27
• Chapter 17, Language Bindings, discusses Fortran issues, and describes language in-

teroperability aspects between C and Fortran.

28

29

The Appendices are:

30

31

• Annex A, Language Bindings Summary, gives speciﬁc syntax in C and Fortran, for all 32

MPI functions, constants, and types.

33

34

• Annex B, Change-Log, summarizes some changes since the previous version of the 35

standard.

36

37
• Several Index pages show the locations of examples, constants and predeﬁned handles, 38 callback routine prototypes, and all MPI functions. 39

MPI provides various interfaces to facilitate interoperability of distinct MPI imple- 40

mentations. Among these are the canonical data representation for MPI I/O and for 41

MPI_PACK_EXTERNAL and MPI_UNPACK_EXTERNAL. The deﬁnition of an actual bind- 42

ing of these interfaces that will enable interoperability is outside the scope of this document. 43

A separate document consists of ideas that were discussed in the MPI Forum during the 44

MPI-2 development and deemed to have value, but are not included in the MPI Standard. 45

They are part of the “Journal of Development” (JOD), lest good ideas be lost and in order 46

to provide a starting point for further work. The chapters in the JOD are

47

48

8

CHAPTER 1. INTRODUCTION TO MPI

1

• Chapter 2, Spawning Independent Processes, includes some elements of dynamic pro-

2

cess management, in particular management of processes with which the spawning

3

processes do not intend to communicate, that the Forum discussed at length but

4

ultimately decided not to include in the MPI Standard.

5

6

• Chapter 3, Threads and MPI, describes some of the expected interaction between an

7

MPI implementation and a thread library in a multi-threaded environment.

8
• Chapter 4, Communicator ID, describes an approach to providing identiﬁers for com-
9
municators.
10

11

• Chapter 5, Miscellany, discusses Miscellaneous topics in the MPI JOD, in particu-

12

lar single-copy routines for use in shared-memory environments and new datatype

13

constructors.

14

15

• Chapter 6, Toward a Full Fortran 90 Interface, describes an approach to providing a

16

more elaborate Fortran 90 interface.

17
• Chapter 7, Split Collective Communication, describes a speciﬁcation for certain non-
18
blocking collective operations.
19

20

• Chapter 8, Real-Time MPI, discusses MPI support for real time processing.

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

44

45

46

47

48

1

2

3

4

5

Chapter 2

6

7

8

9

MPI Terms and Conventions

10

11

12

13

This chapter explains notational terms and conventions used throughout the MPI document, 14

some of the choices that have been made, and the rationale behind those choices.

15

16

17

2.1 Document Notation

18

19
Rationale. Throughout this document, the rationale for the design choices made in 20
the interface speciﬁcation is set oﬀ in this format. Some readers may wish to skip 21
these sections, while readers interested in interface design may want to read them 22
carefully. (End of rationale.) 23

24
Advice to users. Throughout this document, material aimed at users and that 25
illustrates usage is set oﬀ in this format. Some readers may wish to skip these sections, 26
while readers interested in programming in MPI may want to read them carefully. (End 27
of advice to users.) 28

Advice to implementors. Throughout this document, material that is primarily 29

commentary to implementors is set oﬀ in this format. Some readers may wish to skip 30

these sections, while readers interested in MPI implementations may want to read 31

them carefully. (End of advice to implementors.)

32

33

34

2.2 Naming Conventions

35

36

In many cases MPI names for C functions are of the form MPI_Class_action_subset. This 37

convention originated with MPI-1. Since MPI-2 an attempt has been made to standardize 38

the names of MPI functions according to the following rules.

39

40
1. In C, all routines associated with a particular type of MPI object should be of the 41 form MPI_Class_action_subset or, if no subset exists, of the form MPI_Class_action. 42 In Fortran, all routines associated with a particular type of MPI object should be of 43 the form MPI_CLASS_ACTION_SUBSET or, if no subset exists, of the form 44 MPI_CLASS_ACTION. 45

2. If the routine is not associated with a class, the name should be of the form

46

MPI_Action_subset in C and MPI_ACTION_SUBSET in Fortran.

47

48

9

10

CHAPTER 2. MPI TERMS AND CONVENTIONS

1

3. The names of certain actions have been standardized. In particular, Create creates

2

a new object, Get retrieves information about an object, Set sets this information,

3

Delete deletes information, Is asks whether or not an object has a certain property.

4

5

C and Fortran names for some MPI functions (that were deﬁned during the MPI-1

6 process) violate these rules in several cases. The most common exceptions are the omission

7 of the Class name from the routine and the omission of the Action where one can be

8 inferred.

9

MPI identiﬁers are limited to 30 characters (31 with the proﬁling interface). This is

10 done to avoid exceeding the limit on some compilation systems.

11

12 2.3 Procedure Speciﬁcation
13

14 MPI procedures are speciﬁed using a language-independent notation. The arguments of 15 procedure calls are marked as IN, OUT, or INOUT. The meanings of these are:
16

17

• IN: the call may use the input value but does not update the argument from the

18

perspective of the caller at any time during the call’s execution,

19

20

• OUT: the call may update the argument but does not use its input value,

21

• INOUT: the call may both use and update the argument.

22

23

There is one special case — if an argument is a handle to an opaque object (these

24 terms are deﬁned in Section 2.5.1), and the object is updated by the procedure call, then

25 the argument is marked INOUT or OUT. It is marked this way even though the handle itself

26 is not modiﬁed — we use the INOUT or OUT attribute to denote that what the handle

27 references is updated.

28

29

Rationale. The deﬁnition of MPI tries to avoid, to the largest possible extent, the use

30

of INOUT arguments, because such use is error-prone, especially for scalar arguments.

31

(End of rationale.)

32

33

MPI’s use of IN, OUT, and INOUT is intended to indicate to the user how an argument

34 is to be used, but does not provide a rigorous classiﬁcation that can be translated directly

35 into all language bindings (e.g., INTENT in Fortran 90 bindings or const in C bindings).

36 For instance, the “constant” MPI_BOTTOM can usually be passed to OUT buﬀer arguments.

37 Similarly, MPI_STATUS_IGNORE can be passed as the OUT status argument.

38

A common occurrence for MPI functions is an argument that is used as IN by some pro-

39 cesses and OUT by other processes. Such an argument is, syntactically, an INOUT argument

40 and is marked as such, although, semantically, it is not used in one call both for input and

41 for output on a single process.

42

Another frequent situation arises when an argument value is needed only by a subset

43 of the processes. When an argument is not signiﬁcant at a process then an arbitrary value

44 can be passed as an argument.

45

Unless speciﬁed otherwise, an argument of type OUT or type INOUT cannot be aliased

46 with any other argument passed to an MPI procedure. An example of argument aliasing in

47 C appears below. If we deﬁne a C procedure like this,

48

2.4. SEMANTIC TERMS

11

void copyIntBuffer( int *pin, int *pout, int len )

1

{ int i;

2

for (i=0; i<len; ++i) *pout++ = *pin++;

3

}

4

5
then a call to it in the following code fragment has aliased arguments. 6

int a[10];

7

copyIntBuffer( a, a+3, 7);

8

9
Although the C language allows this, such usage of MPI procedures is forbidden unless 10
otherwise speciﬁed. Note that Fortran prohibits aliasing of arguments. 11 All MPI functions are ﬁrst speciﬁed in the language-independent notation. Immediately 12
below this, language dependent bindings follow: 13

• The ISO C version of the function.

14

15

• The Fortran version used with USE mpi_f08.

16

• The Fortran version of the same function used with USE mpi or INCLUDE ’mpif.h’. 17

18

“Fortran” in this document refers to Fortran 90 and higher; see Section 2.6.

19

20

2.4 Semantic Terms

21

22

When discussing MPI procedures the following semantic terms are used.

23

24

nonblocking A procedure is nonblocking if the procedure may return before the opera- 25

tion completes, and before the user is allowed to reuse resources (such as buﬀers) 26

speciﬁed in the call. A nonblocking request is started by the call that initiates it, 27

e.g., MPI_ISEND. The word complete is used with respect to operations, requests, 28

and communications. An operation completes when the user is allowed to reuse 29

resources, and any output buﬀers have been updated; i.e., a call to MPI_TEST will 30

return ﬂag = true. A request is completed by a call to wait, which returns, or 31

a test or get status call which returns ﬂag = true. This completing call has two ef- 32

fects: the status is extracted from the request; in the case of test and wait, if the 33

request was nonpersistent, it is freed, and becomes inactive if it was persistent. A 34

communication completes when all participating operations complete.

35

blocking A procedure is blocking if return from the procedure indicates the user is allowed 36

to reuse resources speciﬁed in the call.

37

38

local A procedure is local if completion of the procedure depends only on the local executing 39

process.

40

41
non-local A procedure is non-local if completion of the operation may require the exe42 cution of some MPI procedure on another process. Such an operation may require 43 communication occurring with another user process. 44

collective A procedure is collective if all processes in a process group need to invoke the 45

procedure. A collective call may or may not be synchronizing. Collective calls over 46

the same communicator must be executed in the same order by all members of the 47

process group.

48

12

CHAPTER 2. MPI TERMS AND CONVENTIONS

1 predeﬁned A predeﬁned datatype is a datatype with a predeﬁned (constant) name (such

2

as MPI_INT, MPI_FLOAT_INT, or MPI_PACKED) or a datatype constructed with

3

MPI_TYPE_CREATE_F90_INTEGER, MPI_TYPE_CREATE_F90_REAL, or

4

MPI_TYPE_CREATE_F90_COMPLEX. The former are named whereas the latter are

5

unnamed.

6

7 derived A derived datatype is any datatype that is not predeﬁned.

8
portable A datatype is portable if it is a predeﬁned datatype, or it is derived from
9
a portable datatype using only the type constructors MPI_TYPE_CONTIGUOUS,
10
MPI_TYPE_VECTOR, MPI_TYPE_INDEXED,
11
MPI_TYPE_CREATE_INDEXED_BLOCK, MPI_TYPE_CREATE_SUBARRAY,
12
MPI_TYPE_DUP, and MPI_TYPE_CREATE_DARRAY. Such a datatype is portable
13
because all displacements in the datatype are in terms of extents of one predeﬁned
14
datatype. Therefore, if such a datatype ﬁts a data layout in one memory, it will
15
ﬁt the corresponding data layout in another memory, if the same declarations were
16
used, even if the two systems have diﬀerent architectures. On the other hand, if a
17
datatype was constructed using MPI_TYPE_CREATE_HINDEXED,
18
MPI_TYPE_CREATE_HINDEXED_BLOCK, MPI_TYPE_CREATE_HVECTOR or
19
MPI_TYPE_CREATE_STRUCT, then the datatype contains explicit byte displace-
20
ments (e.g., providing padding to meet alignment restrictions). These displacements
21
are unlikely to be chosen correctly if they ﬁt data layout on one memory, but are
22
used for data layouts on another process, running on a processor with a diﬀerent
23
architecture.
24

25 equivalent Two datatypes are equivalent if they appear to have been created with the same

26

sequence of calls (and arguments) and thus have the same typemap. Two equivalent

27

datatypes do not necessarily have the same cached attributes or the same names.

28

29
2.5 Data Types
30

31 2.5.1 Opaque Objects
32

33 MPI manages system memory that is used for buﬀering messages and for storing internal

34 representations of various MPI objects such as groups, communicators, datatypes, etc. This

35 memory is not directly accessible to the user, and objects stored there are opaque: their

36 size and shape is not visible to the user. Opaque objects are accessed via handles, which

37 exist in user space. MPI procedures that operate on opaque objects are passed handle

38 arguments to access these objects. In addition to their use by MPI calls for object access,

39 handles can participate in assignments and comparisons.

40

In Fortran with USE mpi or INCLUDE ’mpif.h’, all handles have type INTEGER. In

41 Fortran with USE mpi_f08, and in C, a diﬀerent handle type is deﬁned for each category of

42 objects. With Fortran USE mpi_f08, the handles are deﬁned as Fortran BIND(C) derived

43 types that consist of only one element INTEGER :: MPI_VAL. The internal handle value is

44 identical to the Fortran INTEGER value used in the mpi module and mpif.h. The operators

45 .EQ., .NE., == and /= are overloaded to allow the comparison of these handles. The type

46 names are identical to the names in C, except that they are not case sensitive. For example:

47

48

2.5. DATA TYPES

13

TYPE, BIND(C) :: MPI_Comm

1

INTEGER :: MPI_VAL

2

END TYPE MPI_Comm

3

4
The C types must support the use of the assignment and equality operators. 5

Advice to implementors. In Fortran, the handle can be an index into a table of 6

opaque objects in a system table; in C it can be such an index or a pointer to the 7

object. (End of advice to implementors.)

8

9

10

Rationale. Since the Fortran integer values are equivalent, applications can easily 11

convert MPI handles between all three supported Fortran methods. For example, an 12

integer communicator handle COMM can be converted directly into an exactly equivalent 13

mpi_f08 communicator handle named comm_f08 by comm_f08%MPI_VAL=COMM, and 14

vice versa. The use of the INTEGER deﬁned handles and the BIND(C) derived type 15

handles is diﬀerent: Fortran 2003 (and later) deﬁne that BIND(C) derived types can 16

be used within user deﬁned common blocks, but it is up to the rules of the companion 17

C compiler how many numerical storage units are used for these BIND(C) derived type 18

handles. Most compilers use one unit for both, the INTEGER handles and the handles 19

deﬁned as BIND(C) derived types. (End of rationale.)

20

21
Advice to users. If a user wants to substitute mpif.h or the mpi module by the 22
mpi_f08 module and the application program stores a handle in a Fortran common 23
block then it is necessary to change the Fortran support method in all application 24
routines that use this common block, because the number of numerical storage units 25
of such a handle can be diﬀerent in the two modules. (End of advice to users.) 26

Opaque objects are allocated and deallocated by calls that are speciﬁc to each object 27

type. These are listed in the sections where the objects are described. The calls accept a 28

handle argument of matching type. In an allocate call this is an OUT argument that returns 29

a valid reference to the object. In a call to deallocate this is an INOUT argument which 30

returns with an “invalid handle” value. MPI provides an “invalid handle” constant for each 31

object type. Comparisons to this constant are used to test for validity of the handle.

32

A call to a deallocate routine invalidates the handle and marks the object for deal- 33

location. The object is not accessible to the user after the call. However, MPI need not 34

deallocate the object immediately. Any operation pending (at the time of the deallocate) 35

that involves this object will complete normally; the object will be deallocated afterwards. 36

An opaque object and its handle are signiﬁcant only at the process where the object 37

was created and cannot be transferred to another process.

38

MPI provides certain predeﬁned opaque objects and predeﬁned, static handles to these 39

objects. The user must not free such objects.

40

41

Rationale. This design hides the internal representation used for MPI data structures, 42

thus allowing similar calls in C and Fortran. It also avoids conﬂicts with the typing 43

rules in these languages, and easily allows future extensions of functionality. The 44

mechanism for opaque objects used here loosely follows the POSIX Fortran binding 45

standard.

46

The explicit separation of handles in user space and objects in system space allows 47 space-reclaiming and deallocation calls to be made at appropriate points in the user 48

14

CHAPTER 2. MPI TERMS AND CONVENTIONS

1

program. If the opaque objects were in user space, one would have to be very careful

2

not to go out of scope before any pending operation requiring that object completed.

3

The speciﬁed design allows an object to be marked for deallocation, the user program

4

can then go out of scope, and the object itself still persists until any pending operations

5

are complete.

6
The requirement that handles support assignment/comparison is made since such
7
operations are common. This restricts the domain of possible implementations. The
8
alternative would have been to allow handles to have been an arbitrary, opaque type.
9
This would force the introduction of routines to do assignment and comparison, adding
10
complexity, and was therefore ruled out. (End of rationale.)
11

12

Advice to users. A user may accidentally create a dangling reference by assigning to a

13

handle the value of another handle, and then deallocating the object associated with

14

these handles. Conversely, if a handle variable is deallocated before the associated

15

object is freed, then the object becomes inaccessible (this may occur, for example, if

16

the handle is a local variable within a subroutine, and the subroutine is exited before

17

the associated object is deallocated). It is the user’s responsibility to avoid adding or

18

deleting references to opaque objects, except as a result of MPI calls that allocate or

19

deallocate such objects. (End of advice to users.)

20

21

Advice to implementors. The intended semantics of opaque objects is that opaque

22

objects are separate from one another; each call to allocate such an object copies

23

all the information required for the object. Implementations may avoid excessive

24

copying by substituting referencing for copying. For example, a derived datatype

25

may contain references to its components, rather then copies of its components; a

26

call to MPI_COMM_GROUP may return a reference to the group associated with the

27

communicator, rather than a copy of this group. In such cases, the implementation

28

must maintain reference counts, and allocate and deallocate objects in such a way that

29

the visible eﬀect is as if the objects were copied. (End of advice to implementors.)

30

31 2.5.2 Array Arguments
32
33 An MPI call may need an argument that is an array of opaque objects, or an array of 34 handles. The array-of-handles is a regular array with entries that are handles to objects 35 of the same type in consecutive locations in the array. Whenever such an array is used, 36 an additional len argument is required to indicate the number of valid entries (unless this 37 number can be derived otherwise). The valid entries are at the beginning of the array; 38 len indicates how many of them there are, and need not be the size of the entire array. 39 The same approach is followed for other array arguments. In some cases NULL handles are 40 considered valid entries. When a NULL argument is desired for an array of statuses, one 41 uses MPI_STATUSES_IGNORE.

42

43 2.5.3 State

44
MPI procedures use at various places arguments with state types. The values of such a data
45
type are all identiﬁed by names, and no operation is deﬁned on them. For example, the
46
MPI_TYPE_CREATE_SUBARRAY routine has a state argument order with values
47
MPI_ORDER_C and MPI_ORDER_FORTRAN.
48

2.5. DATA TYPES

15

2.5.4 Named Constants

1

2

MPI procedures sometimes assign a special meaning to a special value of a basic type argu- 3

ment; e.g., tag is an integer-valued argument of point-to-point communication operations, 4

with a special wild-card value, MPI_ANY_TAG. Such arguments will have a range of regular 5

values, which is a proper subrange of the range of values of the corresponding basic type; 6

special values (such as MPI_ANY_TAG) will be outside the regular range. The range of regu- 7

lar values, such as tag, can be queried using environmental inquiry functions (Chapter 7 of 8

the MPI-1 document). The range of other values, such as source, depends on values given 9

by other MPI routines (in the case of source it is the communicator size).

10

MPI also provides predeﬁned named constant handles, such as MPI_COMM_WORLD.

11

All named constants, with the exceptions noted below for Fortran, can be used in 12

initialization expressions or assignments, but not necessarily in array declarations or as 13

labels in C switch or Fortran select/case statements. This implies named constants 14

to be link-time but not necessarily compile-time constants. The named constants listed 15

below are required to be compile-time constants in both C and Fortran. These constants 16

do not change values during execution. Opaque objects accessed by constant handles are 17

deﬁned and do not change value between MPI initialization (MPI_INIT) and MPI completion 18

(MPI_FINALIZE). The handles themselves are constants and can be also used in initialization 19

expressions or assignments. 20

The constants that are required to be compile-time constants (and can thus be used 21

for array length declarations and labels in C switch and Fortran case/select statements) 22

are: 23

MPI_MAX_PROCESSOR_NAME

24

MPI_MAX_LIBRARY_VERSION_STRING

25

MPI_MAX_ERROR_STRING

26

MPI_MAX_DATAREP_STRING

27

MPI_MAX_INFO_KEY

28

MPI_MAX_INFO_VAL

29

MPI_MAX_OBJECT_NAME

30

MPI_MAX_PORT_NAME

31

MPI_VERSION

32

MPI_SUBVERSION

33

MPI_STATUS_SIZE (Fortran only)

34

MPI_ADDRESS_KIND (Fortran only)

35

MPI_COUNT_KIND (Fortran only)

36

MPI_INTEGER_KIND (Fortran only)

37

MPI_OFFSET_KIND (Fortran only)

38

MPI_SUBARRAYS_SUPPORTED (Fortran only)

39

MPI_ASYNC_PROTECTS_NONBLOCKING (Fortran only)

40

The constants that cannot be used in initialization expressions or assignments in For41

tran are:

42

MPI_BOTTOM

43

MPI_STATUS_IGNORE

44

MPI_STATUSES_IGNORE

45

MPI_ERRCODES_IGNORE

46

MPI_IN_PLACE

47

MPI_ARGV_NULL

48

16

CHAPTER 2. MPI TERMS AND CONVENTIONS

1

MPI_ARGVS_NULL

2

MPI_UNWEIGHTED

3

MPI_WEIGHTS_EMPTY

4

5

Advice to implementors. In Fortran the implementation of these special constants

6

may require the use of language constructs that are outside the Fortran standard.

7

Using special values for the constants (e.g., by deﬁning them through PARAMETER

8

statements) is not possible because an implementation cannot distinguish these val-

9

ues from valid data. Typically, these constants are implemented as predeﬁned static

10

variables (e.g., a variable in an MPI-declared COMMON block), relying on the fact that

11

the target compiler passes data by address. Inside the subroutine, this address can

12

be extracted by some mechanism outside the Fortran standard (e.g., by Fortran ex-

13

tensions or by implementing the function in C). (End of advice to implementors.)

14

15 2.5.5 Choice
16
17 MPI functions sometimes use arguments with a choice (or union) data type. Distinct calls to 18 the same routine may pass by reference actual arguments of diﬀerent types. The mechanism 19 for providing such arguments will diﬀer from language to language. For Fortran with the 20 include ﬁle mpif.h or the mpi module, the document uses <type> to represent a choice 21 variable; with the Fortran mpi_f08 module, such arguments are declared with the Fortran 22 2008 + TR 29113 syntax TYPE(*), DIMENSION(..); for C, we use void *.

23
Advice to implementors. Implementors can freely choose how to implement choice
24
arguments in the mpi module, e.g., with a non-standard compiler-dependent method
25
that has the quality of the call mechanism in the implicit Fortran interfaces, or with
26
the method deﬁned for the mpi_f08 module. See details in Section 17.1.1 on page 597.
27
(End of advice to implementors.)
28

29

30
2.5.6 Addresses
31

32 Some MPI procedures use address arguments that represent an absolute address in the 33 calling program. The datatype of such an argument is MPI_Aint in C and 34 INTEGER (KIND=MPI_ADDRESS_KIND) in Fortran. These types must have the same width 35 and encode address values in the same manner such that address values in one language 36 may be passed directly to another language without conversion. There is the MPI constant 37 MPI_BOTTOM to indicate the start of the address range.
38

39 2.5.7 File Oﬀsets
40
41 For I/O there is a need to give the size, displacement, and oﬀset into a ﬁle. These quantities 42 can easily be larger than 32 bits which can be the default size of a Fortran integer. To 43 overcome this, these quantities are declared to be INTEGER (KIND=MPI_OFFSET_KIND) in 44 Fortran. In C one uses MPI_Oﬀset. These types must have the same width and encode 45 address values in the same manner such that oﬀset values in one language may be passed 46 directly to another language without conversion.

47

48

2.6. LANGUAGE BINDING

17

2.5.8 Counts

1

2

As described above, MPI deﬁnes types (e.g., MPI_Aint) to address locations within memory 3

and other types (e.g., MPI_Oﬀset) to address locations within ﬁles. In addition, some MPI 4

procedures use count arguments that represent a number of MPI datatypes on which to 5

operate. At times, one needs a single type that can be used to address locations within 6

either memory or ﬁles as well as express count values, and that type is MPI_Count in C 7

and INTEGER (KIND=MPI_COUNT_KIND) in Fortran. These types must have the same width 8

and encode values in the same manner such that count values in one language may be 9

passed directly to another language without conversion. The size of the MPI_Count type 10

is determined by the MPI implementation with the restriction that it must be minimally 11

capable of encoding any value that may be stored in a variable of type int, MPI_Aint, or 12

MPI_Oﬀset in C and of type INTEGER, INTEGER (KIND=MPI_ADDRESS_KIND), or

13

INTEGER (KIND=MPI_OFFSET_KIND) in Fortran.

14

Rationale. Count values logically need to be large enough to encode any value used 15

for expressing element counts, type maps in memory, type maps in ﬁle views, etc. For 16

backward compatibility reasons, many MPI routines still use int in C and INTEGER 17

in Fortran as the type of count arguments. (End of rationale.)

18

19

20

21

2.6 Language Binding

22

23
This section deﬁnes the rules for MPI language binding in general and for Fortran, and ISO 24
C, in particular. (Note that ANSI C has been replaced by ISO C.) Deﬁned here are various 25
object representations, as well as the naming conventions used for expressing this standard. 26
The actual calling sequences are deﬁned elsewhere. 27 MPI bindings are for Fortran 90 or later, though they were originally designed to be 28
usable in Fortran 77 environments. With the mpi_f08 module, two new Fortran features, 29
assumed type and assumed rank, are also required, see Section 2.5.5 on page 16. 30 Since the word PARAMETER is a keyword in the Fortran language, we use the word 31
“argument” to denote the arguments to a subroutine. These are normally referred to 32
as parameters in C, however, we expect that C programmers will understand the word 33
“argument” (which has no speciﬁc meaning in C), thus allowing us to avoid unnecessary 34
confusion for Fortran programmers. 35 Since Fortran is case insensitive, linkers may use either lower case or upper case when 36
resolving Fortran names. Users of case sensitive languages should avoid the “mpi_” and 37
“pmpi_” preﬁxes. 38

39
2.6.1 Deprecated and Removed Names and Functions 40

A number of chapters refer to deprecated or replaced MPI constructs. These are constructs 41 that continue to be part of the MPI standard, as documented in Chapter 15 on page 591, 42 but that users are recommended not to continue using, since better solutions were provided 43 with newer versions of MPI. For example, the Fortran binding for MPI-1 functions that have 44 address arguments uses INTEGER. This is not consistent with the C binding, and causes 45 problems on machines with 32 bit INTEGERs and 64 bit addresses. In MPI-2, these functions 46 were given new names with new bindings for the address arguments. The use of the old 47 functions is deprecated. For consistency, here and in a few other cases, new C functions are 48

18

CHAPTER 2. MPI TERMS AND CONVENTIONS

1 also provided, even though the new functions are equivalent to the old functions. The old

2 names are deprecated.

3

Some of the deprecated constructs are now removed, as documented in Chapter 16 on

4 page 595. They may still be provided by an implementation for backwards compatibility,

5 but are not required.

6

Table 2.1 shows a list of all of the deprecated and removed constructs. Note that some

7 C typedefs and Fortran subroutine names are included in this list; they are the types of

8 callback functions.

9

10

Deprecated or removed

construct

11

MPI_ADDRESS

12

MPI_TYPE_HINDEXED

13

MPI_TYPE_HVECTOR

14

MPI_TYPE_STRUCT

MPI_TYPE_EXTENT
15
MPI_TYPE_UB

16

MPI_TYPE_LB

17

MPI_LB1

18

MPI_UB1

19

MPI_ERRHANDLER_CREATE

MPI_ERRHANDLER_GET

20

MPI_ERRHANDLER_SET

21

MPI_Handler_function2

deprecated since MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0

removed since MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0 MPI-3.0

Replacement
MPI_GET_ADDRESS MPI_TYPE_CREATE_HINDEXED MPI_TYPE_CREATE_HVECTOR MPI_TYPE_CREATE_STRUCT MPI_TYPE_GET_EXTENT MPI_TYPE_GET_EXTENT MPI_TYPE_GET_EXTENT MPI_TYPE_CREATE_RESIZED MPI_TYPE_CREATE_RESIZED MPI_COMM_CREATE_ERRHANDLER MPI_COMM_GET_ERRHANDLER MPI_COMM_SET_ERRHANDLER MPI_Comm_errhandler_function2

22

MPI_KEYVAL_CREATE

MPI-2.0

MPI_COMM_CREATE_KEYVAL

23

MPI_KEYVAL_FREE

MPI_DUP_FN3

24

MPI_NULL_COPY_FN3

MPI-2.0 MPI-2.0 MPI-2.0

MPI_COMM_FREE_KEYVAL MPI_COMM_DUP_FN3 MPI_COMM_NULL_COPY_FN3

25

MPI_NULL_DELETE_FN3

26

MPI_Copy_function2

27

COPY_FUNCTION3

MPI_Delete_function2

28

DELETE_FUNCTION3

29

MPI_ATTR_DELETE

MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0 MPI-2.0

MPI_COMM_NULL_DELETE_FN3 MPI_Comm_copy_attr_function2 COMM_COPY_ATTR_FUNCTION3 MPI_Comm_delete_attr_function2 COMM_DELETE_ATTR_FUNCTION3
MPI_COMM_DELETE_ATTR

30

MPI_ATTR_GET

MPI-2.0

MPI_COMM_GET_ATTR

31

MPI_ATTR_PUT

MPI-2.0

MPI_COMM_SET_ATTR

32

MPI_COMBINER_HVECTOR_INTEGER4 -

MPI_COMBINER_HINDEXED_INTEGER4 -

MPI-3.0 MPI_COMBINER_HVECTOR4 MPI-3.0 MPI_COMBINER_HINDEXED4

33

MPI_COMBINER_STRUCT_INTEGER4

-

MPI-3.0 MPI_COMBINER_STRUCT4

34

MPI::...

MPI-2.2

MPI-3.0 C language binding

35

1 Predeﬁned datatype.

36

2 Callback prototype deﬁnition.

3 Predeﬁned callback routine.

37

4 Constant.

38

Other entries are regular MPI routines.

39

40

41

Table 2.1: Deprecated and Removed constructs

42

43
44 2.6.2 Fortran Binding Issues

45
Originally, MPI-1.1 provided bindings for Fortran 77. These bindings are retained, but they 46 are now interpreted in the context of the Fortran 90 standard. MPI can still be used with
47
most Fortran 77 compilers, as noted below. When the term “Fortran” is used it means
48

2.6. LANGUAGE BINDING

19

Fortran 90 or later; it means Fortran 2008 + TR 29113 and later if the mpi_f08 module is 1

used.

2

All MPI names have an MPI_ preﬁx, and all characters are capitals. Programs must 3

not declare names, e.g., for variables, subroutines, functions, parameters, derived types, 4

abstract interfaces, or modules, beginning with the preﬁx MPI_. To avoid conﬂicting with 5

the proﬁling interface, programs must also avoid subroutines and functions with the preﬁx 6

PMPI_. This is mandated to avoid possible name collisions.

7

All MPI Fortran subroutines have a return code in the last argument. With USE 8

mpi_f08, this last argument is declared as OPTIONAL, except for user-deﬁned callback func- 9

tions (e.g., COMM_COPY_ATTR_FUNCTION) and their predeﬁned callbacks (e.g.,

10

MPI_NULL_COPY_FN). A few MPI operations which are functions do not have the return 11

code argument. The return code value for successful completion is MPI_SUCCESS. Other 12

error codes are implementation dependent; see the error codes in Chapter 8 and Annex A. 13

Constants representing the maximum length of a string are one smaller in Fortran than 14

in C as discussed in Section 17.2.9.

15

Handles are represented in Fortran as INTEGERs, or as a BIND(C) derived type with the 16

mpi_f08 module; see Section 2.5.1 on page 12. Binary-valued variables are of type LOGICAL. 17

Array arguments are indexed from one.

18

The older MPI Fortran bindings (mpif.h and use mpi) are inconsistent with the For- 19

tran standard in several respects. These inconsistencies, such as register optimization prob- 20

lems, have implications for user codes that are discussed in detail in Section 17.1.16.

21

22

2.6.3 C Binding Issues

23

24

We use the ISO C declaration format. All MPI names have an MPI_ preﬁx, deﬁned constants 25

are in all capital letters, and deﬁned types and functions have one capital letter after 26

the preﬁx. Programs must not declare names (identiﬁers), e.g., for variables, functions, 27

constants, types, or macros, beginning with the preﬁx MPI_. To support the proﬁling 28

interface, programs must not declare functions with names beginning with the preﬁx PMPI_. 29

The deﬁnition of named constants, function prototypes, and type deﬁnitions must be 30

supplied in an include ﬁle mpi.h.

31

Almost all C functions return an error code. The successful return code will be

32

MPI_SUCCESS, but failure return codes are implementation dependent.

33

Type declarations are provided for handles to each category of opaque objects.

34

Array arguments are indexed from zero.

35

Logical ﬂags are integers with value 0 meaning “false” and a non-zero value meaning 36

“true.”

37

Choice arguments are pointers of type void *.

38

Address arguments are of MPI deﬁned type MPI_Aint. File displacements are of type 39

MPI_Oﬀset. MPI_Aint is deﬁned to be an integer of the size needed to hold any valid address 40

on the target architecture. MPI_Oﬀset is deﬁned to be an integer of the size needed to hold 41

any valid ﬁle size on the target architecture.

42

43

2.6.4 Functions and Macros

44

45
An implementation is allowed to implement MPI_WTIME, MPI_WTICK, PMPI_WTIME, 46
PMPI_WTICK, and the handle-conversion functions (MPI_Group_f2c, etc.) in Section 17.2.4, 47
and no others, as macros in C. 48

20

CHAPTER 2. MPI TERMS AND CONVENTIONS

1

Advice to implementors. Implementors should document which routines are imple-

2

mented as macros. (End of advice to implementors.)

3

4

Advice to users. If these routines are implemented as macros, they will not work

5

with the MPI proﬁling interface. (End of advice to users.)

6

7 2.7 Processes
8

9 An MPI program consists of autonomous processes, executing their own code, in an MIMD

10 style. The codes executed by each process need not be identical. The processes communicate

11 via calls to MPI communication primitives. Typically, each process executes in its own

12 address space, although shared-memory implementations of MPI are possible.

13

This document speciﬁes the behavior of a parallel program assuming that only MPI

14 calls are used. The interaction of an MPI program with other possible means of commu-

15 nication, I/O, and process management is not speciﬁed. Unless otherwise stated in the

16 speciﬁcation of the standard, MPI places no requirements on the result of its interaction

17 with external mechanisms that provide similar or equivalent functionality. This includes,

18 but is not limited to, interactions with external mechanisms for process control, shared and

19 remote memory access, ﬁle system access and control, interprocess communication, process

20 signaling, and terminal I/O. High quality implementations should strive to make the results

21 of such interactions intuitive to users, and attempt to document restrictions where deemed

22 necessary.

23

24

Advice to implementors. Implementations that support such additional mechanisms

25

for functionality supported within MPI are expected to document how these interact

26

with MPI. (End of advice to implementors.)

27

28

The interaction of MPI and threads is deﬁned in Section 12.4.

29

30 2.8 Error Handling
31

32 MPI provides the user with reliable message transmission. A message sent is always received

33 correctly, and the user does not need to check for transmission errors, time-outs, or other

34 error conditions. In other words, MPI does not provide mechanisms for dealing with failures

35 in the communication system. If the MPI implementation is built on an unreliable underly-

36 ing mechanism, then it is the job of the implementor of the MPI subsystem to insulate the

37 user from this unreliability, or to reﬂect unrecoverable errors as failures. Whenever possible,

38 such failures will be reﬂected as errors in the relevant communication call. Similarly, MPI

39 itself provides no mechanisms for handling processor failures.

40

Of course, MPI programs may still be erroneous. A program error can occur when

41 an MPI call is made with an incorrect argument (non-existing destination in a send oper-

42 ation, buﬀer too small in a receive operation, etc.). This type of error would occur in any

43 implementation. In addition, a resource error may occur when a program exceeds the

44 amount of available system resources (number of pending messages, system buﬀers, etc.).

45 The occurrence of this type of error depends on the amount of available resources in the

46 system and the resource allocation mechanism used; this may diﬀer from system to system.

47 A high-quality implementation will provide generous limits on the important resources so

48 as to alleviate the portability problem this represents.

2.9. IMPLEMENTATION ISSUES

21

In C and Fortran, almost all MPI calls return a code that indicates successful completion 1

of the operation. Whenever possible, MPI calls return an error code if an error occurred 2

during the call. By default, an error detected during the execution of the MPI library 3

causes the parallel computation to abort, except for ﬁle operations. However, MPI provides 4

mechanisms for users to change this default and to handle recoverable errors. The user may 5

specify that no error is fatal, and handle error codes returned by MPI calls by himself or 6

herself. Also, the user may provide his or her own error-handling routines, which will be 7

invoked whenever an MPI call returns abnormally. The MPI error handling facilities are 8

described in Section 8.3.

9

Several factors limit the ability of MPI calls to return with meaningful error codes 10

when an error occurs. MPI may not be able to detect some errors; other errors may be too 11

expensive to detect in normal execution mode; ﬁnally some errors may be “catastrophic” 12

and may prevent MPI from returning control to the caller in a consistent state.

13

Another subtle issue arises because of the nature of asynchronous communications: MPI 14

calls may initiate operations that continue asynchronously after the call returned. Thus, the 15

operation may return with a code indicating successful completion, yet later cause an error 16

exception to be raised. If there is a subsequent call that relates to the same operation (e.g., 17

a call that veriﬁes that an asynchronous operation has completed) then the error argument 18

associated with this call will be used to indicate the nature of the error. In a few cases, the 19

error may occur after all calls that relate to the operation have completed, so that no error 20

value can be used to indicate the nature of the error (e.g., an error on the receiver in a send 21

with the ready mode). Such an error must be treated as fatal, since information cannot be 22

returned for the user to recover from it.

23

This document does not specify the state of a computation after an erroneous MPI call 24

has occurred. The desired behavior is that a relevant error code be returned, and the eﬀect 25

of the error be localized to the greatest possible extent. E.g., it is highly desirable that an 26

erroneous receive call will not cause any part of the receiver’s memory to be overwritten, 27

beyond the area speciﬁed for receiving the message.

28

Implementations may go beyond this document in supporting in a meaningful manner 29

MPI calls that are deﬁned here to be erroneous. For example, MPI speciﬁes strict type 30

matching rules between matching send and receive operations: it is erroneous to send a 31

ﬂoating point variable and receive an integer. Implementations may go beyond these type 32

matching rules, and provide automatic type conversion in such situations. It will be helpful 33

to generate warnings for such non-conforming behavior.

34

MPI deﬁnes a way for users to create new error codes as deﬁned in Section 8.5.

35

36

37
2.9 Implementation Issues 38

39
There are a number of areas where an MPI implementation may interact with the operating 40
environment and system. While MPI does not mandate that any services (such as signal 41
handling) be provided, it does strongly suggest the behavior to be provided if those services 42
are available. This is an important point in achieving portability across platforms that 43
provide the same set of services. 44

45
2.9.1 Independence of Basic Runtime Routines 46

MPI programs require that library routines that are part of the basic language environment 47

(such as write in Fortran and printf and malloc in ISO C) and are executed after

48

22

CHAPTER 2. MPI TERMS AND CONVENTIONS

1 MPI_INIT and before MPI_FINALIZE operate independently and that their completion is

2 independent of the action of other processes in an MPI program.

3

Note that this in no way prevents the creation of library routines that provide parallel

4 services whose operation is collective. However, the following program is expected to com-

5 plete in an ISO C environment regardless of the size of MPI_COMM_WORLD (assuming that

6 printf is available at the executing nodes).

7

8 int rank;

9 MPI_Init((void *)0, (void *)0);

10 MPI_Comm_rank(MPI_COMM_WORLD, &rank);

11 if (rank == 0) printf("Starting program\n");

12 MPI_Finalize();

13
The corresponding Fortran programs are also expected to complete.
14
An example of what is not required is any particular ordering of the action of these
15
routines when called by several tasks. For example, MPI makes neither requirements nor
16
recommendations for the output from the following program (again assuming that I/O is
17
available at the executing nodes).
18

19 MPI_Comm_rank(MPI_COMM_WORLD, &rank);

20 printf("Output from task rank %d\n", rank);

21

22

In addition, calls that fail because of resource exhaustion or other error are not con-

23 sidered a violation of the requirements here (however, they are required to complete, just

24 not to complete successfully).

25
26 2.9.2 Interaction with Signals

27
MPI does not specify the interaction of processes with signals and does not require that MPI
28
be signal safe. The implementation may reserve some signals for its own use. It is required
29
that the implementation document which signals it uses, and it is strongly recommended
30
that it not use SIGALRM, SIGFPE, or SIGIO. Implementations may also prohibit the use of
31
MPI calls from within signal handlers.
32
In multithreaded environments, users can avoid conﬂicts between signals and the MPI
33
library by catching signals only on threads that do not execute MPI calls. High quality
34
single-threaded implementations will be signal safe: an MPI call suspended by a signal will
35
resume and complete normally after the signal is handled.
36

37
38 2.10 Examples

39
40 The examples in this document are for illustration purposes only. They are not intended 41 to specify the standard. Furthermore, the examples have not been carefully checked or 42 veriﬁed.

43

44

45

46

47

48

1

2

3

4

5

Chapter 3

6

7

8

9

Point-to-Point Communication

10

11

12

13

3.1 Introduction

14

15

Sending and receiving of messages by processes is the basic MPI communication mechanism. 16

The basic point-to-point communication operations are send and receive. Their use is 17

illustrated in the example below.

18

19

#include "mpi.h"

20

int main( int argc, char *argv[])

21

{

22

char message[20];

23

int myrank;

24

MPI_Status status;

25

MPI_Init( &argc, &argv );

26

MPI_Comm_rank( MPI_COMM_WORLD, &myrank );

27

if (myrank == 0) /* code for process zero */

28

{

29

strcpy(message,"Hello, there");

30

MPI_Send(message, strlen(message)+1, MPI_CHAR, 1, 99, MPI_COMM_WORLD); 31

}

32

else if (myrank == 1) /* code for process one */

33

{

34

MPI_Recv(message, 20, MPI_CHAR, 0, 99, MPI_COMM_WORLD, &status);

35

printf("received :%s:\n", message);

36

}

37

MPI_Finalize();

38

return 0;

39

}

40

41

In this example, process zero (myrank = 0) sends a message to process one using the 42 send operation MPI_SEND. The operation speciﬁes a send buﬀer in the sender memory 43 from which the message data is taken. In the example above, the send buﬀer consists of 44 the storage containing the variable message in the memory of process zero. The location, 45 size and type of the send buﬀer are speciﬁed by the ﬁrst three parameters of the send 46 operation. The message sent will contain the 13 characters of this variable. In addition, 47 the send operation associates an envelope with the message. This envelope speciﬁes the 48

23

24

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 message destination and contains distinguishing information that can be used by the receive

2 operation to select a particular message. The last three parameters of the send operation,

3 along with the rank of the sender, specify the envelope for the message sent. Process one

4 (myrank = 1) receives this message with the receive operation MPI_RECV. The message to

5 be received is selected according to the value of its envelope, and the message data is stored

6 into the receive buﬀer. In the example above, the receive buﬀer consists of the storage

7 containing the string message in the memory of process one. The ﬁrst three parameters

8 of the receive operation specify the location, size and type of the receive buﬀer. The next

9 three parameters are used for selecting the incoming message. The last parameter is used

10 to return information on the message just received.

11

The next sections describe the blocking send and receive operations. We discuss send,

12 receive, blocking communication semantics, type matching requirements, type conversion in

13 heterogeneous environments, and more general communication modes. Nonblocking com-

14 munication is addressed next, followed by probing and canceling a message, channel-like

15 constructs and send-receive operations, ending with a description of the “dummy” process,

16 MPI_PROC_NULL.

17

18
3.2 Blocking Send and Receive Operations
19

20 3.2.1 Blocking Send
21
22 The syntax of the blocking send operation is given below.

23

24
25 MPI_SEND(buf, count, datatype, dest, tag, comm)

26

IN

buf

initial address of send buﬀer (choice)

27

IN

28

29
IN
30

31

IN

count
datatype dest

number of elements in send buﬀer (non-negative integer) datatype of each send buﬀer element (handle) rank of destination (integer)

32

IN

tag

message tag (integer)

33

IN

34

comm

communicator (handle)

35
int MPI_Send(const void* buf, int count, MPI_Datatype datatype, int dest,
36
int tag, MPI_Comm comm)
37

38 MPI_Send(buf, count, datatype, dest, tag, comm, ierror) BIND(C)

39

TYPE(*), DIMENSION(..), INTENT(IN) :: buf

40

INTEGER, INTENT(IN) :: count, dest, tag

41

TYPE(MPI_Datatype), INTENT(IN) :: datatype

42

TYPE(MPI_Comm), INTENT(IN) :: comm

43

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

44

45 MPI_SEND(BUF, COUNT, DATATYPE, DEST, TAG, COMM, IERROR)

46

<type> BUF(*)

INTEGER COUNT, DATATYPE, DEST, TAG, COMM, IERROR
47

48

The blocking semantics of this call are described in Section 3.4.

3.2. BLOCKING SEND AND RECEIVE OPERATIONS

25

3.2.2 Message Data

1

2

The send buﬀer speciﬁed by the MPI_SEND operation consists of count successive entries of 3

the type indicated by datatype, starting with the entry at address buf. Note that we specify 4

the message length in terms of number of elements, not number of bytes. The former is 5

machine independent and closer to the application level.

6

The data part of the message consists of a sequence of count values, each of the type 7

indicated by datatype. count may be zero, in which case the data part of the message is 8

empty. The basic datatypes that can be speciﬁed for message data values correspond to the 9

basic datatypes of the host language. Possible values of this argument for Fortran and the 10

corresponding Fortran types are listed in Table 3.1. 11

MPI datatype

Fortran datatype

12

MPI_INTEGER

INTEGER

13

MPI_REAL

REAL

14

MPI_DOUBLE_PRECISION DOUBLE PRECISION

15

MPI_COMPLEX

COMPLEX

16

MPI_LOGICAL

LOGICAL

17

MPI_CHARACTER

CHARACTER(1)

18

MPI_BYTE

19

MPI_PACKED

20

21

22

Table 3.1: Predeﬁned MPI datatypes corresponding to Fortran datatypes

23

24

Possible values for this argument for C and the corresponding C types are listed in 25

Table 3.2.

26

The datatypes MPI_BYTE and MPI_PACKED do not correspond to a Fortran or C 27

datatype. A value of type MPI_BYTE consists of a byte (8 binary digits). A byte is 28

uninterpreted and is diﬀerent from a character. Diﬀerent machines may have diﬀerent 29

representations for characters, or may use more than one byte to represent characters. On 30

the other hand, a byte has the same binary value on all machines. The use of the type 31

MPI_PACKED is explained in Section 4.2.

32

MPI requires support of these datatypes, which match the basic datatypes of Fortran 33

and ISO C. Additional MPI datatypes should be provided if the host language has additional 34

data types: MPI_DOUBLE_COMPLEX for double precision complex in Fortran declared 35

to be of type DOUBLE COMPLEX; MPI_REAL2, MPI_REAL4, and MPI_REAL8 for Fortran 36

reals, declared to be of type REAL*2, REAL*4 and REAL*8, respectively; MPI_INTEGER1, 37

MPI_INTEGER2, and MPI_INTEGER4 for Fortran integers, declared to be of type

38

INTEGER*1, INTEGER*2, and INTEGER*4, respectively; etc.

39

40
Rationale. One goal of the design is to allow for MPI to be implemented as a 41
library, with no need for additional preprocessing or compilation. Thus, one cannot 42
assume that a communication call has information on the datatype of variables in the 43
communication buﬀer; this information must be supplied by an explicit argument. 44
The need for such datatype information will become clear in Section 3.3.2. (End of 45
rationale.) 46

The datatypes MPI_AINT, MPI_OFFSET, and MPI_COUNT correspond to the MPI- 47

deﬁned C types MPI_Aint, MPI_Oﬀset, and MPI_Count and their Fortran equivalents

48

26

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

MPI datatype

C datatype

2

MPI_CHAR

char

3

(treated as printable character)

4

MPI_SHORT

signed short int

5

MPI_INT

signed int

6

MPI_LONG

signed long int

7

MPI_LONG_LONG_INT

signed long long int

8

MPI_LONG_LONG (as a synonym)

signed long long int

9

MPI_SIGNED_CHAR

signed char

10

(treated as integral value)

11

MPI_UNSIGNED_CHAR

unsigned char

12

(treated as integral value)

13

MPI_UNSIGNED_SHORT

unsigned short int

14

MPI_UNSIGNED

unsigned int

15

MPI_UNSIGNED_LONG

unsigned long int

16

MPI_UNSIGNED_LONG_LONG

unsigned long long int

17

MPI_FLOAT

float

18

MPI_DOUBLE

double

19

MPI_LONG_DOUBLE

long double

20

MPI_WCHAR

wchar_t

21

(deﬁned in <stddef.h>)

22

(treated as printable character)

23

MPI_C_BOOL

_Bool

24

MPI_INT8_T

int8_t

25

MPI_INT16_T

int16_t

26

MPI_INT32_T

int32_t

27

MPI_INT64_T

int64_t

28

MPI_UINT8_T

uint8_t

29

MPI_UINT16_T

uint16_t

30

MPI_UINT32_T

uint32_t

31

MPI_UINT64_T

uint64_t

32

MPI_C_COMPLEX

float _Complex

33

MPI_C_FLOAT_COMPLEX (as a synonym) float _Complex

34

MPI_C_DOUBLE_COMPLEX

double _Complex

35

MPI_C_LONG_DOUBLE_COMPLEX

long double _Complex

36

MPI_BYTE

37

MPI_PACKED

38

39

40

Table 3.2: Predeﬁned MPI datatypes corresponding to C datatypes

41

42 INTEGER (KIND=MPI_ADDRESS_KIND), INTEGER (KIND=MPI_OFFSET_KIND), and

43 INTEGER (KIND=MPI_COUNT_KIND). This is described in Table 3.3. All predeﬁned datatype

44 handles are available in all language bindings. See Sections 17.2.6 and 17.2.10 on page 650

45 and 658 for information on interlanguage communication with these types.

46

If there is an accompanying C++ compiler then the datatypes in Table 3.4 are also

47 supported in C and Fortran.

48

3.2. BLOCKING SEND AND RECEIVE OPERATIONS

27

MPI datatype C datatype Fortran datatype

1

MPI_AINT

MPI_Aint INTEGER (KIND=MPI_ADDRESS_KIND)

2

MPI_OFFSET MPI_Offset INTEGER (KIND=MPI_OFFSET_KIND)

3

MPI_COUNT MPI_Count INTEGER (KIND=MPI_COUNT_KIND)

4

5

6

Table 3.3: Predeﬁned MPI datatypes corresponding to both C and Fortran datatypes

7

MPI datatype

C++ datatype

8

9

MPI_CXX_BOOL

bool

10

MPI_CXX_FLOAT_COMPLEX

std::complex<ﬂoat>

11

MPI_CXX_DOUBLE_COMPLEX

std::complex<double>

12

MPI_CXX_LONG_DOUBLE_COMPLEX std::complex<long double>

13

14

Table 3.4: Predeﬁned MPI datatypes corresponding to C++ datatypes

15

16

3.2.3 Message Envelope

17

18

In addition to the data part, messages carry information that can be used to distinguish 19

messages and selectively receive them. This information consists of a ﬁxed number of ﬁelds, 20

which we collectively call the message envelope. These ﬁelds are

21

22
source 23
destination 24 tag 25
communicator 26

The message source is implicitly determined by the identity of the message sender. The 27

other ﬁelds are speciﬁed by arguments in the send operation.

28

The message destination is speciﬁed by the dest argument.

29

The integer-valued message tag is speciﬁed by the tag argument. This integer can be 30

used by the program to distinguish diﬀerent types of messages. The range of valid tag 31

values is 0,...,UB, where the value of UB is implementation dependent. It can be found by 32

querying the value of the attribute MPI_TAG_UB, as described in Chapter 8. MPI requires 33

that UB be no less than 32767.

34

The comm argument speciﬁes the communicator that is used for the send operation. 35

Communicators are explained in Chapter 6; below is a brief summary of their usage.

36

A communicator speciﬁes the communication context for a communication operation. 37

Each communication context provides a separate “communication universe”: messages are 38

always received within the context they were sent, and messages sent in diﬀerent contexts 39

do not interfere.

40

The communicator also speciﬁes the set of processes that share this communication 41

context. This process group is ordered and processes are identiﬁed by their rank within 42

this group. Thus, the range of valid values for dest is 0, ..., n-1∪{MPI_PROC_NULL}, where 43

n is the number of processes in the group. (If the communicator is an inter-communicator, 44

then destinations are identiﬁed by their rank in the remote group. See Chapter 6.)

45

A predeﬁned communicator MPI_COMM_WORLD is provided by MPI. It allows com- 46

munication with all processes that are accessible after MPI initialization and processes are 47

identiﬁed by their rank in the group of MPI_COMM_WORLD.

48

28

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

Advice to users. Users that are comfortable with the notion of a ﬂat name space

2

for processes, and a single communication context, as oﬀered by most existing com-

3

munication libraries, need only use the predeﬁned variable MPI_COMM_WORLD as the

4

comm argument. This will allow communication with all the processes available at

5

initialization time.

6
Users may deﬁne new communicators, as explained in Chapter 6. Communicators
7
provide an important encapsulation mechanism for libraries and modules. They allow
8
modules to have their own disjoint communication universe and their own process
9
numbering scheme. (End of advice to users.)
10

11

Advice to implementors. The message envelope would normally be encoded by a

12

ﬁxed-length message header. However, the actual encoding is implementation depen-

13

dent. Some of the information (e.g., source or destination) may be implicit, and need

14

not be explicitly carried by messages. Also, processes may be identiﬁed by relative

15

ranks, or absolute ids, etc. (End of advice to implementors.)

16

17
3.2.4 Blocking Receive
18

19 The syntax of the blocking receive operation is given below.

20

21
22 MPI_RECV (buf, count, datatype, source, tag, comm, status)

23

OUT

buf

initial address of receive buﬀer (choice)

24
IN
25

26

27

IN

count datatype

number of elements in receive buﬀer (non-negative integer)
datatype of each receive buﬀer element (handle)

28

IN

source

rank of source or MPI_ANY_SOURCE (integer)

29
IN

30

31

IN

tag comm

message tag or MPI_ANY_TAG (integer) communicator (handle)

32

OUT status

status object (Status)

33

34 int MPI_Recv(void* buf, int count, MPI_Datatype datatype, int source,

35

int tag, MPI_Comm comm, MPI_Status *status)

36

37 MPI_Recv(buf, count, datatype, source, tag, comm, status, ierror) BIND(C)

38

TYPE(*), DIMENSION(..) :: buf

39

INTEGER, INTENT(IN) :: count, source, tag

40

TYPE(MPI_Datatype), INTENT(IN) :: datatype

41

TYPE(MPI_Comm), INTENT(IN) :: comm

42

TYPE(MPI_Status) :: status

43

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

44 MPI_RECV(BUF, COUNT, DATATYPE, SOURCE, TAG, COMM, STATUS, IERROR)

45

<type> BUF(*)

46

INTEGER COUNT, DATATYPE, SOURCE, TAG, COMM, STATUS(MPI_STATUS_SIZE),

47

IERROR

48

3.2. BLOCKING SEND AND RECEIVE OPERATIONS

29

The blocking semantics of this call are described in Section 3.4.

1

The receive buﬀer consists of the storage containing count consecutive elements of the 2

type speciﬁed by datatype, starting at address buf. The length of the received message must 3

be less than or equal to the length of the receive buﬀer. An overﬂow error occurs if all 4

incoming data does not ﬁt, without truncation, into the receive buﬀer.

5

If a message that is shorter than the receive buﬀer arrives, then only those locations 6

corresponding to the (shorter) message are modiﬁed.

7

8

Advice to users. The MPI_PROBE function described in Section 3.8 can be used to 9

receive messages of unknown length. (End of advice to users.)

10

11

Advice to implementors. Even though no speciﬁc behavior is mandated by MPI for 12

erroneous programs, the recommended handling of overﬂow situations is to return in 13

status information about the source and tag of the incoming message. The receive 14

operation will return an error code. A quality implementation will also ensure that 15

no memory that is outside the receive buﬀer will ever be overwritten.

16

In the case of a message shorter than the receive buﬀer, MPI is quite strict in that it 17

allows no modiﬁcation of the other locations. A more lenient statement would allow 18

for some optimizations but this is not allowed. The implementation must be ready to 19

end a copy into the receiver memory exactly at the end of the receive buﬀer, even if 20

it is an odd address. (End of advice to implementors.)

21

22

The selection of a message by a receive operation is governed by the value of the 23

message envelope. A message can be received by a receive operation if its envelope matches 24

the source, tag and comm values speciﬁed by the receive operation. The receiver may 25

specify a wildcard MPI_ANY_SOURCE value for source, and/or a wildcard MPI_ANY_TAG 26

value for tag, indicating that any source and/or tag are acceptable. It cannot specify a 27

wildcard value for comm. Thus, a message can be received by a receive operation only 28

if it is addressed to the receiving process, has a matching communicator, has matching 29

source unless source=MPI_ANY_SOURCE in the pattern, and has a matching tag unless 30

tag=MPI_ANY_TAG in the pattern.

31

The message tag is speciﬁed by the tag argument of the receive operation. The 32

argument source, if diﬀerent from MPI_ANY_SOURCE, is speciﬁed as a rank within the 33

process group associated with that same communicator (remote process group, for in- 34

tercommunicators). Thus, the range of valid values for the source argument is {0,...,n- 35

1}∪{MPI_ANY_SOURCE},∪{MPI_PROC_NULL}, where n is the number of processes in this 36

group.

37

Note the asymmetry between send and receive operations: A receive operation may 38

accept messages from an arbitrary sender, on the other hand, a send operation must specify 39

a unique receiver. This matches a “push” communication mechanism, where data transfer 40

is eﬀected by the sender (rather than a “pull” mechanism, where data transfer is eﬀected 41

by the receiver).

42

Source = destination is allowed, that is, a process can send a message to itself. (How- 43

ever, it is unsafe to do so with the blocking send and receive operations described above, 44

since this may lead to deadlock. See Section 3.5.)

45

46

Advice to implementors. Message context and other communicator information can 47 be implemented as an additional tag ﬁeld. It diﬀers from the regular message tag 48

30

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

in that wild card matching is not allowed on this ﬁeld, and that value setting for

2

this ﬁeld is controlled by communicator manipulation functions. (End of advice to

3

implementors.)

4

5

The use of dest or source=MPI_PROC_NULL to deﬁne a “dummy” destination or source

6 in any send or receive call is described in Section 3.11 on page 81.

7
8 3.2.5 Return Status

9
The source or tag of a received message may not be known if wildcard values were used
10
in the receive operation. Also, if multiple requests are completed by a single MPI function
11
(see Section 3.7.5), a distinct error code may need to be returned for each request. The
12
information is returned by the status argument of MPI_RECV. The type of status is MPI-
13
deﬁned. Status variables need to be explicitly allocated by the user, that is, they are not
14
system objects.
15
In C, status is a structure that contains three ﬁelds named MPI_SOURCE, MPI_TAG,
16
and MPI_ERROR; the structure may contain additional ﬁelds. Thus,
17
status.MPI_SOURCE, status.MPI_TAG and status.MPI_ERROR contain the source, tag, and
18
error code, respectively, of the received message.
19
In Fortran with USE mpi or INCLUDE ’mpif.h’, status is an array of INTEGERs of size
20
MPI_STATUS_SIZE. The constants MPI_SOURCE, MPI_TAG and MPI_ERROR are the indices
21
of the entries that store the source, tag and error ﬁelds. Thus, status(MPI_SOURCE),
22
status(MPI_TAG) and status(MPI_ERROR) contain, respectively, the source, tag and error
23
code of the received message.
24
With Fortran USE mpi_f08, status is deﬁned as the Fortran BIND(C) derived type
25
TYPE(MPI_Status) containing three public ﬁelds named MPI_SOURCE,
26
MPI_TAG, and MPI_ERROR. TYPE(MPI_Status) may contain additional, implementation-
27
speciﬁc ﬁelds. Thus, status%MPI_SOURCE, status%MPI_TAG and status%MPI_ERROR con-
28
tain the source, tag, and error code of a received message respectively. Additionally, within
29
both the mpi and the mpi_f08 modules, the constants MPI_STATUS_SIZE, MPI_SOURCE,
30
MPI_TAG, MPI_ERROR, and TYPE(MPI_Status) are deﬁned to allow conversion between both
31
status representations. Conversion routines are provided in Section 17.2.5 on page 648.
32

33

Rationale. The Fortran TYPE(MPI_Status) is deﬁned as a BIND(C) derived type so

34

that it can be used at any location where the status integer array representation can

35

be used, e.g., in user deﬁned common blocks. (End of rationale.)

36

37

Rationale. It is allowed to have the same name (e.g., MPI_SOURCE) deﬁned as a

38

constant (e.g., Fortran parameter) and as a ﬁeld of a derived type. (End of rationale.)

39
In general, message-passing calls do not modify the value of the error code ﬁeld of
40
status variables. This ﬁeld may be updated only by the functions in Section 3.7.5 which
41
return multiple statuses. The ﬁeld is updated if and only if such function returns with an
42
error code of MPI_ERR_IN_STATUS.
43

44

Rationale. The error ﬁeld in status is not needed for calls that return only one status,

45

such as MPI_WAIT, since that would only duplicate the information returned by the

46

function itself. The current design avoids the additional overhead of setting it, in such

47

cases. The ﬁeld is needed for calls that return multiple statuses, since each request

48

may have had a diﬀerent failure. (End of rationale.)

3.2. BLOCKING SEND AND RECEIVE OPERATIONS

31

The status argument also returns information on the length of the message received. 1

However, this information is not directly available as a ﬁeld of the status variable and a call 2

to MPI_GET_COUNT is required to “decode” this information.

3

4

5

MPI_GET_COUNT(status, datatype, count)

6

IN

status

IN

datatype

OUT

count

return status of receive operation (Status)

7

8
datatype of each receive buﬀer entry (handle) 9

number of received entries (integer)

10

11

int MPI_Get_count(const MPI_Status *status, MPI_Datatype datatype,

12

int *count)

13

14
MPI_Get_count(status, datatype, count, ierror) BIND(C) 15 TYPE(MPI_Status), INTENT(IN) :: status 16 TYPE(MPI_Datatype), INTENT(IN) :: datatype 17 INTEGER, INTENT(OUT) :: count 18 INTEGER, OPTIONAL, INTENT(OUT) :: ierror 19

MPI_GET_COUNT(STATUS, DATATYPE, COUNT, IERROR)

20

INTEGER STATUS(MPI_STATUS_SIZE), DATATYPE, COUNT, IERROR

21

22
Returns the number of entries received. (Again, we count entries, each of type datatype, 23
not bytes.) The datatype argument should match the argument provided by the receive call 24
that set the status variable. If the number of entries received exceeds the limits of the count 25
parameter, then MPI_GET_COUNT sets the value of count to MPI_UNDEFINED. There are 26 other situations where the value of count can be set to MPI_UNDEFINED; see Section 4.1.11. 27

28
Rationale. Some message-passing libraries use INOUT count, tag and 29
source arguments, thus using them both to specify the selection criteria for incoming 30
messages and return the actual envelope values of the received message. The use of a 31
separate status argument prevents errors that are often attached with INOUT argument 32
(e.g., using the MPI_ANY_TAG constant as the tag in a receive). Some libraries use 33
calls that refer implicitly to the “last message received.” This is not thread safe. 34

The datatype argument is passed to MPI_GET_COUNT so as to improve performance. 35

A message might be received without counting the number of elements it contains, 36

and the count value is often not needed. Also, this allows the same function to be 37

used after a call to MPI_PROBE or MPI_IPROBE. With a status from MPI_PROBE 38

or MPI_IPROBE, the same datatypes are allowed as in a call to MPI_RECV to receive 39

this message. (End of rationale.)

40

41

The value returned as the count argument of MPI_GET_COUNT for a datatype of length 42

zero where zero bytes have been transferred is zero. If the number of bytes transferred is 43

greater than zero, MPI_UNDEFINED is returned.

44

45
Rationale. Zero-length datatypes may be created in a number of cases. An important 46
case is MPI_TYPE_CREATE_DARRAY, where the deﬁnition of the particular darray 47
results in an empty block on some MPI process. Programs written in an SPMD style 48

32

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

will not check for this special case and may want to use MPI_GET_COUNT to check

2

the status. (End of rationale.)

3

4

Advice to users. The buﬀer size required for the receive can be aﬀected by data con-

5

versions and by the stride of the receive datatype. In most cases, the safest approach

6

is to use the same datatype with MPI_GET_COUNT and the receive. (End of advice

7

to users.)

8

9

All send and receive operations use the buf, count, datatype, source, dest, tag, comm,

10 and status arguments in the same way as the blocking MPI_SEND and MPI_RECV operations

11 described in this section.

12

13 3.2.6 Passing MPI_STATUS_IGNORE for Status

14
Every call to MPI_RECV includes a status argument, wherein the system can return details
15
about the message received. There are also a number of other MPI calls where status
16
is returned. An object of type MPI_Status is not an MPI opaque object; its structure
17
is declared in mpi.h and mpif.h, and it exists in the user’s program. In many cases,
18
application programs are constructed so that it is unnecessary for them to examine the
19
status ﬁelds. In these cases, it is a waste for the user to allocate a status object, and it is
20
particularly wasteful for the MPI implementation to ﬁll in ﬁelds in this object.
21
To cope with this problem, there are two predeﬁned constants, MPI_STATUS_IGNORE
22
and MPI_STATUSES_IGNORE, which when passed to a receive, probe, wait, or test function,
23
inform the implementation that the status ﬁelds are not to be ﬁlled in. Note that
24
MPI_STATUS_IGNORE is not a special type of MPI_Status object; rather, it is a special value
25
for the argument. In C one would expect it to be NULL, not the address of a special
26
MPI_Status.
27
MPI_STATUS_IGNORE, and the array version MPI_STATUSES_IGNORE, can be used every-
28
where a status argument is passed to a receive, wait, or test function. MPI_STATUS_IGNORE
29
cannot be used when status is an IN argument. Note that in Fortran MPI_STATUS_IGNORE
30
and MPI_STATUSES_IGNORE are objects like MPI_BOTTOM (not usable for initialization or
31
assignment). See Section 2.5.4.
32
In general, this optimization can apply to all functions for which status or an array of
33
statuses is an OUT argument. Note that this converts status into an INOUT argument. The
34
functions that can be passed MPI_STATUS_IGNORE are all the various forms of MPI_RECV,
35
MPI_PROBE, MPI_TEST, and MPI_WAIT, as well as MPI_REQUEST_GET_STATUS. When
36
an array is passed, as in the MPI_{TEST|WAIT}{ALL|SOME} functions, a separate constant,
37
MPI_STATUSES_IGNORE, is passed for the array argument. It is possible for an MPI function
38
to return MPI_ERR_IN_STATUS even when MPI_STATUS_IGNORE or MPI_STATUSES_IGNORE
39
has been passed to that function.
40
MPI_STATUS_IGNORE and MPI_STATUSES_IGNORE are not required to have the same
41
values in C and Fortran.
42
It is not allowed to have some of the statuses in an array of statuses for
43
MPI_{TEST|WAIT}{ALL|SOME} functions set to MPI_STATUS_IGNORE; one either speciﬁes
44
ignoring all of the statuses in such a call with MPI_STATUSES_IGNORE, or none of them by
45
passing normal statuses in all positions in the array of statuses.
46

47

48

3.3. DATA TYPE MATCHING AND DATA CONVERSION

33

3.3 Data Type Matching and Data Conversion

1

2

3.3.1 Type Matching Rules

3

4
One can think of message transfer as consisting of the following three phases. 5

1. Data is pulled out of the send buﬀer and a message is assembled.

6

7

2. A message is transferred from sender to receiver.

8

9
3. Data is pulled from the incoming message and disassembled into the receive buﬀer. 10

Type matching has to be observed at each of these three phases: The type of each 11

variable in the sender buﬀer has to match the type speciﬁed for that entry by the send 12

operation; the type speciﬁed by the send operation has to match the type speciﬁed by the 13

receive operation; and the type of each variable in the receive buﬀer has to match the type 14

speciﬁed for that entry by the receive operation. A program that fails to observe these three 15

rules is erroneous.

16

To deﬁne type matching more precisely, we need to deal with two issues: matching of 17

types of the host language with types speciﬁed in communication operations; and matching 18

of types at sender and receiver.

19

The types of a send and receive match (phase two) if both operations use identical 20

names. That is, MPI_INTEGER matches MPI_INTEGER, MPI_REAL matches MPI_REAL, 21

and so on. There is one exception to this rule, discussed in Section 4.2: the type

22

MPI_PACKED can match any other type.

23

The type of a variable in a host program matches the type speciﬁed in the commu- 24

nication operation if the datatype name used by that operation corresponds to the basic 25

type of the host program variable. For example, an entry with type name MPI_INTEGER 26

matches a Fortran variable of type INTEGER. A table giving this correspondence for Fortran 27

and C appears in Section 3.2.2. There are two exceptions to this last rule: an entry with 28

type name MPI_BYTE or MPI_PACKED can be used to match any byte of storage (on a 29

byte-addressable machine), irrespective of the datatype of the variable that contains this 30

byte. The type MPI_PACKED is used to send data that has been explicitly packed, or 31

receive data that will be explicitly unpacked, see Section 4.2. The type MPI_BYTE allows 32

one to transfer the binary value of a byte in memory unchanged.

33

To summarize, the type matching rules fall into the three categories below.

34

35

• Communication of typed values (e.g., with datatype diﬀerent from MPI_BYTE), where 36

the datatypes of the corresponding entries in the sender program, in the send call, in 37

the receive call and in the receiver program must all match.

38

39
• Communication of untyped values (e.g., of datatype MPI_BYTE), where both sender 40 and receiver use the datatype MPI_BYTE. In this case, there are no requirements on 41 the types of the corresponding entries in the sender and the receiver programs, nor is 42 it required that they be the same. 43

• Communication involving packed data, where MPI_PACKED is used.

44

45

The following examples illustrate the ﬁrst two cases.

46

47
Example 3.1 Sender and receiver specify matching types. 48

34

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 CALL MPI_COMM_RANK(comm, rank, ierr)

2 IF (rank.EQ.0) THEN

3

CALL MPI_SEND(a(1), 10, MPI_REAL, 1, tag, comm, ierr)

4 ELSE IF (rank.EQ.1) THEN

5

CALL MPI_RECV(b(1), 15, MPI_REAL, 0, tag, comm, status, ierr)

6 END IF

7

8

This code is correct if both a and b are real arrays of size ≥ 10. (In Fortran, it might

9 be correct to use this code even if a or b have size < 10: e.g., when a(1) can be equivalenced

10 to an array with ten reals.)

11
Example 3.2 Sender and receiver do not specify matching types.
12

13 CALL MPI_COMM_RANK(comm, rank, ierr)

14 IF (rank.EQ.0) THEN

15

CALL MPI_SEND(a(1), 10, MPI_REAL, 1, tag, comm, ierr)

16 ELSE IF (rank.EQ.1) THEN

17

CALL MPI_RECV(b(1), 40, MPI_BYTE, 0, tag, comm, status, ierr)

18 END IF

19

20

This code is erroneous, since sender and receiver do not provide matching datatype

21 arguments.

22
23 Example 3.3 Sender and receiver specify communication of untyped values.

24 CALL MPI_COMM_RANK(comm, rank, ierr)

25 IF (rank.EQ.0) THEN

26

CALL MPI_SEND(a(1), 40, MPI_BYTE, 1, tag, comm, ierr)

27 ELSE IF (rank.EQ.1) THEN

28

CALL MPI_RECV(b(1), 60, MPI_BYTE, 0, tag, comm, status, ierr)

29 END IF

30

31

This code is correct, irrespective of the type and size of a and b (unless this results in

32 an out of bounds memory access).

33
Advice to users. If a buﬀer of type MPI_BYTE is passed as an argument to MPI_SEND,
34
then MPI will send the data stored at contiguous locations, starting from the address
35
indicated by the buf argument. This may have unexpected results when the data
36
layout is not as a casual user would expect it to be. For example, some Fortran
37
compilers implement variables of type CHARACTER as a structure that contains the
38
character length and a pointer to the actual string. In such an environment, sending
39
and receiving a Fortran CHARACTER variable using the MPI_BYTE type will not have
40
the anticipated result of transferring the character string. For this reason, the user is
41
advised to use typed communications whenever possible. (End of advice to users.)
42

43
44 Type MPI_CHARACTER

45 The type MPI_CHARACTER matches one character of a Fortran variable of type CHARACTER, 46 rather than the entire character string stored in the variable. Fortran variables of type 47 CHARACTER or substrings are transferred as if they were arrays of characters. This is 48 illustrated in the example below.

3.3. DATA TYPE MATCHING AND DATA CONVERSION

35

Example 3.4

1

Transfer of Fortran CHARACTERs.

2

3

CHARACTER*10 a

4

CHARACTER*10 b

5

6

CALL MPI_COMM_RANK(comm, rank, ierr)

7

IF (rank.EQ.0) THEN

8

CALL MPI_SEND(a, 5, MPI_CHARACTER, 1, tag, comm, ierr)

9

ELSE IF (rank.EQ.1) THEN

10

CALL MPI_RECV(b(6:10), 5, MPI_CHARACTER, 0, tag, comm, status, ierr)

11

END IF

12

13

The last ﬁve characters of string b at process 1 are replaced by the ﬁrst ﬁve characters 14

of string a at process 0.

15

16

Rationale. The alternative choice would be for MPI_CHARACTER to match a char- 17

acter of arbitrary length. This runs into problems.

18

A Fortran character variable is a constant length string, with no special termina- 19

tion symbol. There is no ﬁxed convention on how to represent characters, and how 20

to store their length. Some compilers pass a character argument to a routine as a 21

pair of arguments, one holding the address of the string and the other holding the 22

length of string. Consider the case of an MPI communication call that is passed a 23

communication buﬀer with type deﬁned by a derived datatype (Section 4.1). If this 24

communicator buﬀer contains variables of type CHARACTER then the information on 25

their length will not be passed to the MPI routine.

26

27
This problem forces us to provide explicit information on character length with the 28
MPI call. One could add a length parameter to the type MPI_CHARACTER, but this 29
does not add much convenience and the same functionality can be achieved by deﬁning 30
a suitable derived datatype. (End of rationale.) 31

Advice to implementors. Some compilers pass Fortran CHARACTER arguments as a 32

structure with a length and a pointer to the actual string. In such an environment, 33

the MPI call needs to dereference the pointer in order to reach the string. (End of 34

advice to implementors.)

35

36

3.3.2 Data Conversion

37

38

One of the goals of MPI is to support parallel computations across heterogeneous environ- 39

ments. Communication in a heterogeneous environment may require data conversions. We 40

use the following terminology.

41

42
type conversion changes the datatype of a value, e.g., by rounding a REAL to an INTEGER. 43

representation conversion changes the binary representation of a value, e.g., from Hex 44

ﬂoating point to IEEE ﬂoating point.

45

46

The type matching rules imply that MPI communication never entails type conversion. 47

On the other hand, MPI requires that a representation conversion be performed when a 48

36

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 typed value is transferred across environments that use diﬀerent representations for the

2 datatype of this value. MPI does not specify rules for representation conversion. Such

3 conversion is expected to preserve integer, logical and character values, and to convert a

4 ﬂoating point value to the nearest value that can be represented on the target system.

5

Overﬂow and underﬂow exceptions may occur during ﬂoating point conversions. Con-

6 version of integers or characters may also lead to exceptions when a value that can be

7 represented in one system cannot be represented in the other system. An exception occur-

8 ring during representation conversion results in a failure of the communication. An error

9 occurs either in the send operation, or the receive operation, or both.

10

If a value sent in a message is untyped (i.e., of type MPI_BYTE), then the binary

11 representation of the byte stored at the receiver is identical to the binary representation

12 of the byte loaded at the sender. This holds true, whether sender and receiver run in the

13 same or in distinct environments. No representation conversion is required. (Note that

14 representation conversion may occur when values of type MPI_CHARACTER or MPI_CHAR

15 are transferred, for example, from an EBCDIC encoding to an ASCII encoding.)

16

No conversion need occur when an MPI program executes in a homogeneous system,

17 where all processes run in the same environment.

18

Consider the three examples, 3.1–3.3. The ﬁrst program is correct, assuming that a and

19 b are REAL arrays of size ≥ 10. If the sender and receiver execute in diﬀerent environments,

20 then the ten real values that are fetched from the send buﬀer will be converted to the

21 representation for reals on the receiver site before they are stored in the receive buﬀer.

22 While the number of real elements fetched from the send buﬀer equal the number of real

23 elements stored in the receive buﬀer, the number of bytes stored need not equal the number

24 of bytes loaded. For example, the sender may use a four byte representation and the receiver

25 an eight byte representation for reals.

26

The second program is erroneous, and its behavior is undeﬁned.

27

The third program is correct. The exact same sequence of forty bytes that were loaded

28 from the send buﬀer will be stored in the receive buﬀer, even if sender and receiver run in

29 a diﬀerent environment. The message sent has exactly the same length (in bytes) and the

30 same binary representation as the message received. If a and b are of diﬀerent types, or if

31 they are of the same type but diﬀerent data representations are used, then the bits stored

32 in the receive buﬀer may encode values that are diﬀerent from the values they encoded in

33 the send buﬀer.

34

Data representation conversion also applies to the envelope of a message: source, des-

35 tination and tag are all integers that may need to be converted.

36

37

Advice to implementors. The current deﬁnition does not require messages to carry

38

data type information. Both sender and receiver provide complete data type infor-

39

mation. In a heterogeneous environment, one can either use a machine independent

40

encoding such as XDR, or have the receiver convert from the sender representation

41

to its own, or even have the sender do the conversion.

42

Additional type information might be added to messages in order to allow the sys-

43

tem to detect mismatches between datatype at sender and receiver. This might be

44

particularly useful in a slower but safer debug mode. (End of advice to implementors.)

45

46

MPI requires support for inter-language communication, i.e., if messages are sent by a

47 C or C++ process and received by a Fortran process, or vice-versa. The behavior is deﬁned

48 in Section 17.2 on page 645.

3.4. COMMUNICATION MODES

37

3.4 Communication Modes

1

2

The send call described in Section 3.2.1 is blocking: it does not return until the message 3

data and envelope have been safely stored away so that the sender is free to modify the 4

send buﬀer. The message might be copied directly into the matching receive buﬀer, or it 5

might be copied into a temporary system buﬀer.

6

Message buﬀering decouples the send and receive operations. A blocking send can com- 7

plete as soon as the message was buﬀered, even if no matching receive has been executed by 8

the receiver. On the other hand, message buﬀering can be expensive, as it entails additional 9

memory-to-memory copying, and it requires the allocation of memory for buﬀering. MPI 10

oﬀers the choice of several communication modes that allow one to control the choice of the 11

communication protocol.

12

The send call described in Section 3.2.1 uses the standard communication mode. In 13

this mode, it is up to MPI to decide whether outgoing messages will be buﬀered. MPI may 14

buﬀer outgoing messages. In such a case, the send call may complete before a matching 15

receive is invoked. On the other hand, buﬀer space may be unavailable, or MPI may choose 16

not to buﬀer outgoing messages, for performance reasons. In this case, the send call will 17

not complete until a matching receive has been posted, and the data has been moved to the 18

receiver.

19

Thus, a send in standard mode can be started whether or not a matching receive has 20

been posted. It may complete before a matching receive is posted. The standard mode send 21

is non-local: successful completion of the send operation may depend on the occurrence 22

of a matching receive.

23

24

Rationale. The reluctance of MPI to mandate whether standard sends are buﬀering 25

or not stems from the desire to achieve portable programs. Since any system will run 26

out of buﬀer resources as message sizes are increased, and some implementations may 27

want to provide little buﬀering, MPI takes the position that correct (and therefore, 28

portable) programs do not rely on system buﬀering in standard mode. Buﬀering may 29

improve the performance of a correct program, but it doesn’t aﬀect the result of the 30

program. If the user wishes to guarantee a certain amount of buﬀering, the user- 31

provided buﬀer system of Section 3.6 should be used, along with the buﬀered-mode 32

send. (End of rationale.)

33

34

There are three additional communication modes.

35

A buﬀered mode send operation can be started whether or not a matching receive 36

has been posted. It may complete before a matching receive is posted. However, unlike 37

the standard send, this operation is local, and its completion does not depend on the 38

occurrence of a matching receive. Thus, if a send is executed and no matching receive is 39

posted, then MPI must buﬀer the outgoing message, so as to allow the send call to complete. 40

An error will occur if there is insuﬃcient buﬀer space. The amount of available buﬀer space 41

is controlled by the user — see Section 3.6. Buﬀer allocation by the user may be required 42

for the buﬀered mode to be eﬀective.

43

A send that uses the synchronous mode can be started whether or not a matching 44

receive was posted. However, the send will complete successfully only if a matching receive is 45

posted, and the receive operation has started to receive the message sent by the synchronous 46

send. Thus, the completion of a synchronous send not only indicates that the send buﬀer 47

can be reused, but it also indicates that the receiver has reached a certain point in its 48

38

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 execution, namely that it has started executing the matching receive. If both sends and

2 receives are blocking operations then the use of the synchronous mode provides synchronous

3 communication semantics: a communication does not complete at either end before both

4 processes rendezvous at the communication. A send executed in this mode is non-local.

5

A send that uses the ready communication mode may be started only if the matching

6 receive is already posted. Otherwise, the operation is erroneous and its outcome is unde-

7 ﬁned. On some systems, this allows the removal of a hand-shake operation that is otherwise

8 required and results in improved performance. The completion of the send operation does

9 not depend on the status of a matching receive, and merely indicates that the send buﬀer

10 can be reused. A send operation that uses the ready mode has the same semantics as a

11 standard send operation, or a synchronous send operation; it is merely that the sender

12 provides additional information to the system (namely that a matching receive is already

13 posted), that can save some overhead. In a correct program, therefore, a ready send could

14 be replaced by a standard send with no eﬀect on the behavior of the program other than

15 performance.

16

Three additional send functions are provided for the three additional communication

17 modes. The communication mode is indicated by a one letter preﬁx: B for buﬀered, S for

18 synchronous, and R for ready.

19

20
21 MPI_BSEND (buf, count, datatype, dest, tag, comm)

22

IN

23
IN
24

25

26

IN

buf count
datatype

initial address of send buﬀer (choice) number of elements in send buﬀer (non-negative integer) datatype of each send buﬀer element (handle)

27

IN

28
IN
29

30

IN

dest tag comm

rank of destination (integer) message tag (integer) communicator (handle)

31

32 int MPI_Bsend(const void* buf, int count, MPI_Datatype datatype, int dest,

33

int tag, MPI_Comm comm)

34
MPI_Bsend(buf, count, datatype, dest, tag, comm, ierror) BIND(C)
35
TYPE(*), DIMENSION(..), INTENT(IN) :: buf
36
INTEGER, INTENT(IN) :: count, dest, tag
37
TYPE(MPI_Datatype), INTENT(IN) :: datatype
38
TYPE(MPI_Comm), INTENT(IN) :: comm
39
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
40

41 MPI_BSEND(BUF, COUNT, DATATYPE, DEST, TAG, COMM, IERROR)

42

<type> BUF(*)

43

INTEGER COUNT, DATATYPE, DEST, TAG, COMM, IERROR

44
Send in buﬀered mode.
45

46

47

48

3.4. COMMUNICATION MODES

39

MPI_SSEND (buf, count, datatype, dest, tag, comm)

1

IN

buf

2
initial address of send buﬀer (choice) 3

IN

count

number of elements in send buﬀer (non-negative inte- 4

ger)

5

IN

datatype

IN

dest

IN

tag

datatype of each send buﬀer element (handle)

6

7
rank of destination (integer) 8

message tag (integer)

9

IN

comm

communicator (handle)

10

11

int MPI_Ssend(const void* buf, int count, MPI_Datatype datatype, int dest,

12

int tag, MPI_Comm comm)

13

14

MPI_Ssend(buf, count, datatype, dest, tag, comm, ierror) BIND(C)

15

TYPE(*), DIMENSION(..), INTENT(IN) :: buf

16

INTEGER, INTENT(IN) :: count, dest, tag

17

TYPE(MPI_Datatype), INTENT(IN) :: datatype

18

TYPE(MPI_Comm), INTENT(IN) :: comm

19

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

20

21
MPI_SSEND(BUF, COUNT, DATATYPE, DEST, TAG, COMM, IERROR) 22 <type> BUF(*) 23 INTEGER COUNT, DATATYPE, DEST, TAG, COMM, IERROR 24

Send in synchronous mode.

25

26

27

MPI_RSEND (buf, count, datatype, dest, tag, comm)

28

IN

buf

initial address of send buﬀer (choice)

29

IN

count

IN

datatype

IN

dest

number of elements in send buﬀer (non-negative inte- 30

ger)

31

32

datatype of each send buﬀer element (handle)

33

rank of destination (integer)

34

IN

tag

message tag (integer)

35

36

IN

comm

communicator (handle)

37

38

int MPI_Rsend(const void* buf, int count, MPI_Datatype datatype, int dest,

39

int tag, MPI_Comm comm)

40

41
MPI_Rsend(buf, count, datatype, dest, tag, comm, ierror) BIND(C)
42
TYPE(*), DIMENSION(..), INTENT(IN) :: buf
43
INTEGER, INTENT(IN) :: count, dest, tag
44
TYPE(MPI_Datatype), INTENT(IN) :: datatype
45
TYPE(MPI_Comm), INTENT(IN) :: comm
46
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
47

MPI_RSEND(BUF, COUNT, DATATYPE, DEST, TAG, COMM, IERROR)

48

40

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

<type> BUF(*)

2

INTEGER COUNT, DATATYPE, DEST, TAG, COMM, IERROR

3

4

Send in ready mode.

5

There is only one receive operation, but it matches any of the send modes. The receive

6 operation described in the last section is blocking: it returns only after the receive buﬀer

7 contains the newly received message. A receive can complete before the matching send has

8 completed (of course, it can complete only after the matching send has started).

9

In a multithreaded implementation of MPI, the system may de-schedule a thread that

10 is blocked on a send or receive operation, and schedule another thread for execution in

11 the same address space. In such a case it is the user’s responsibility not to modify a

12 communication buﬀer until the communication completes. Otherwise, the outcome of the

13 computation is undeﬁned.

14
Advice to implementors. Since a synchronous send cannot complete before a matching
15
receive is posted, one will not normally buﬀer messages sent by such an operation.
16

17

It is recommended to choose buﬀering over blocking the sender, whenever possible,

18

for standard sends. The programmer can signal his or her preference for blocking the

19

sender until a matching receive occurs by using the synchronous send mode.

20

A possible communication protocol for the various communication modes is outlined

21

below.

22
ready send: The message is sent as soon as possible.
23

24

synchronous send: The sender sends a request-to-send message. The receiver stores

25

this request. When a matching receive is posted, the receiver sends back a permission-

26

to-send message, and the sender now sends the message.

27

standard send: First protocol may be used for short messages, and second protocol for

28

long messages.

29
buﬀered send: The sender copies the message into a buﬀer and then sends it with a
30
nonblocking send (using the same protocol as for standard send).
31

32

Additional control messages might be needed for ﬂow control and error recovery. Of

33

course, there are many other possible protocols.

34

Ready send can be implemented as a standard send. In this case there will be no

35

performance advantage (or disadvantage) for the use of ready send.

36
A standard send can be implemented as a synchronous send. In such a case, no data
37
buﬀering is needed. However, users may expect some buﬀering.
38

39

In a multithreaded environment, the execution of a blocking communication should

40

block only the executing thread, allowing the thread scheduler to de-schedule this

41

thread and schedule another thread for execution. (End of advice to implementors.)

42

43 3.5 Semantics of Point-to-Point Communication
44

45 A valid MPI implementation guarantees certain general properties of point-to-point com46 munication, which are described in this section.
47

48

3.5. SEMANTICS OF POINT-TO-POINT COMMUNICATION

41

Order Messages are non-overtaking: If a sender sends two messages in succession to the 1

same destination, and both match the same receive, then this operation cannot receive the 2

second message if the ﬁrst one is still pending. If a receiver posts two receives in succession, 3

and both match the same message, then the second receive operation cannot be satisﬁed 4

by this message, if the ﬁrst one is still pending. This requirement facilitates matching of 5

sends to receives. It guarantees that message-passing code is deterministic, if processes are 6

single-threaded and the wildcard MPI_ANY_SOURCE is not used in receives. (Some of the 7

calls described later, such as MPI_CANCEL or MPI_WAITANY, are additional sources of 8

nondeterminism.)

9

If a process has a single thread of execution, then any two communications executed 10

by this process are ordered. On the other hand, if the process is multithreaded, then the 11

semantics of thread execution may not deﬁne a relative order between two send operations 12

executed by two distinct threads. The operations are logically concurrent, even if one 13

physically precedes the other. In such a case, the two messages sent can be received in 14

any order. Similarly, if two receive operations that are logically concurrent receive two 15

successively sent messages, then the two messages can match the two receives in either 16

order.

17

18

Example 3.5 An example of non-overtaking messages.

19

20

CALL MPI_COMM_RANK(comm, rank, ierr)

21

IF (rank.EQ.0) THEN

22

CALL MPI_BSEND(buf1, count, MPI_REAL, 1, tag, comm, ierr)

23

CALL MPI_BSEND(buf2, count, MPI_REAL, 1, tag, comm, ierr)

24

ELSE IF (rank.EQ.1) THEN

25

CALL MPI_RECV(buf1, count, MPI_REAL, 0, MPI_ANY_TAG, comm, status, ierr) 26

CALL MPI_RECV(buf2, count, MPI_REAL, 0, tag, comm, status, ierr)

27

END IF

28

29

The message sent by the ﬁrst send must be received by the ﬁrst receive, and the message 30

sent by the second send must be received by the second receive.

31

32

Progress If a pair of matching send and receives have been initiated on two processes, then 33

at least one of these two operations will complete, independently of other actions in the 34

system: the send operation will complete, unless the receive is satisﬁed by another message, 35

and completes; the receive operation will complete, unless the message sent is consumed by 36

another matching receive that was posted at the same destination process.

37

38
Example 3.6 An example of two, intertwined matching pairs. 39

CALL MPI_COMM_RANK(comm, rank, ierr)

40

IF (rank.EQ.0) THEN

41

CALL MPI_BSEND(buf1, count, MPI_REAL, 1, tag1, comm, ierr)

42

CALL MPI_SSEND(buf2, count, MPI_REAL, 1, tag2, comm, ierr)

43

ELSE IF (rank.EQ.1) THEN

44

CALL MPI_RECV(buf1, count, MPI_REAL, 0, tag2, comm, status, ierr)

45

CALL MPI_RECV(buf2, count, MPI_REAL, 0, tag1, comm, status, ierr)

46

END IF

47

48

42

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 Both processes invoke their ﬁrst communication call. Since the ﬁrst send of process zero 2 uses the buﬀered mode, it must complete, irrespective of the state of process one. Since 3 no matching receive is posted, the message will be copied into buﬀer space. (If insuﬃcient 4 buﬀer space is available, then the program will fail.) The second send is then invoked. At 5 that point, a matching pair of send and receive operation is enabled, and both operations 6 must complete. Process one next invokes its second receive call, which will be satisﬁed by 7 the buﬀered message. Note that process one received the messages in the reverse order they 8 were sent.
9

10 Fairness MPI makes no guarantee of fairness in the handling of communication. Suppose 11 that a send is posted. Then it is possible that the destination process repeatedly posts a 12 receive that matches this send, yet the message is never received, because it is each time 13 overtaken by another message, sent from another source. Similarly, suppose that a receive 14 was posted by a multithreaded process. Then it is possible that messages that match this 15 receive are repeatedly received, yet the receive is never satisﬁed, because it is overtaken 16 by other receives posted at this node (by other executing threads). It is the programmer’s 17 responsibility to prevent starvation in such situations.
18

19 Resource limitations Any pending communication operation consumes system resources

20 that are limited. Errors may occur when lack of resources prevent the execution of an MPI

21 call. A quality implementation will use a (small) ﬁxed amount of resources for each pending

22 send in the ready or synchronous mode and for each pending receive. However, buﬀer space

23 may be consumed to store messages sent in standard mode, and must be consumed to store

24 messages sent in buﬀered mode, when no matching receive is available. The amount of space

25 available for buﬀering will be much smaller than program data memory on many systems.

26 Then, it will be easy to write programs that overrun available buﬀer space.

27

MPI allows the user to provide buﬀer memory for messages sent in the buﬀered mode.

28 Furthermore, MPI speciﬁes a detailed operational model for the use of this buﬀer. An MPI

29 implementation is required to do no worse than implied by this model. This allows users to

30 avoid buﬀer overﬂows when they use buﬀered sends. Buﬀer allocation and use is described

31 in Section 3.6.

32

A buﬀered send operation that cannot complete because of a lack of buﬀer space is

33 erroneous. When such a situation is detected, an error is signaled that may cause the

34 program to terminate abnormally. On the other hand, a standard send operation that

35 cannot complete because of lack of buﬀer space will merely block, waiting for buﬀer space

36 to become available or for a matching receive to be posted. This behavior is preferable in

37 many situations. Consider a situation where a producer repeatedly produces new values

38 and sends them to a consumer. Assume that the producer produces new values faster

39 than the consumer can consume them. If buﬀered sends are used, then a buﬀer overﬂow

40 will result. Additional synchronization has to be added to the program so as to prevent

41 this from occurring. If standard sends are used, then the producer will be automatically

42 throttled, as its send operations will block when buﬀer space is unavailable.

43

In some situations, a lack of buﬀer space leads to deadlock situations. This is illustrated

44 by the examples below.

45

46 Example 3.7 An exchange of messages.
47

48

3.5. SEMANTICS OF POINT-TO-POINT COMMUNICATION

43

CALL MPI_COMM_RANK(comm, rank, ierr)

1

IF (rank.EQ.0) THEN

2

CALL MPI_SEND(sendbuf, count, MPI_REAL, 1, tag, comm, ierr)

3

CALL MPI_RECV(recvbuf, count, MPI_REAL, 1, tag, comm, status, ierr)

4

ELSE IF (rank.EQ.1) THEN

5

CALL MPI_RECV(recvbuf, count, MPI_REAL, 0, tag, comm, status, ierr)

6

CALL MPI_SEND(sendbuf, count, MPI_REAL, 0, tag, comm, ierr)

7

END IF

8

9

This program will succeed even if no buﬀer space for data is available. The standard send 10

operation can be replaced, in this example, with a synchronous send.

11

12
Example 3.8 An errant attempt to exchange messages. 13

CALL MPI_COMM_RANK(comm, rank, ierr)

14

IF (rank.EQ.0) THEN

15

CALL MPI_RECV(recvbuf, count, MPI_REAL, 1, tag, comm, status, ierr)

16

CALL MPI_SEND(sendbuf, count, MPI_REAL, 1, tag, comm, ierr)

17

ELSE IF (rank.EQ.1) THEN

18

CALL MPI_RECV(recvbuf, count, MPI_REAL, 0, tag, comm, status, ierr)

19

CALL MPI_SEND(sendbuf, count, MPI_REAL, 0, tag, comm, ierr)

20

END IF

21

22
The receive operation of the ﬁrst process must complete before its send, and can complete 23
only if the matching send of the second processor is executed. The receive operation of the 24
second process must complete before its send and can complete only if the matching send 25
of the ﬁrst process is executed. This program will always deadlock. The same holds for any 26
other send mode. 27

28
Example 3.9 An exchange that relies on buﬀering. 29

CALL MPI_COMM_RANK(comm, rank, ierr)

30

IF (rank.EQ.0) THEN

31

CALL MPI_SEND(sendbuf, count, MPI_REAL, 1, tag, comm, ierr)

32

CALL MPI_RECV(recvbuf, count, MPI_REAL, 1, tag, comm, status, ierr)

33

ELSE IF (rank.EQ.1) THEN

34

CALL MPI_SEND(sendbuf, count, MPI_REAL, 0, tag, comm, ierr)

35

CALL MPI_RECV(recvbuf, count, MPI_REAL, 0, tag, comm, status, ierr)

36

END IF

37

38
The message sent by each process has to be copied out before the send operation returns 39
and the receive operation starts. For the program to complete, it is necessary that at least 40
one of the two messages sent be buﬀered. Thus, this program can succeed only if the 41
communication system can buﬀer at least count words of data. 42

Advice to users. When standard send operations are used, then a deadlock situation 43

may occur where both processes are blocked because buﬀer space is not available. The 44

same will certainly happen, if the synchronous mode is used. If the buﬀered mode is 45

used, and not enough buﬀer space is available, then the program will not complete 46

either. However, rather than a deadlock situation, we shall have a buﬀer overﬂow 47

error.

48

44

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

A program is “safe” if no message buﬀering is required for the program to complete.

2

One can replace all sends in such program with synchronous sends, and the pro-

3

gram will still run correctly. This conservative programming style provides the best

4

portability, since program completion does not depend on the amount of buﬀer space

5

available or on the communication protocol used.

6
Many programmers prefer to have more leeway and opt to use the “unsafe” program-
7
ming style shown in Example 3.9. In such cases, the use of standard sends is likely
8
to provide the best compromise between performance and robustness: quality imple-
9
mentations will provide suﬃcient buﬀering so that “common practice” programs will
10
not deadlock. The buﬀered send mode can be used for programs that require more
11
buﬀering, or in situations where the programmer wants more control. This mode
12
might also be used for debugging purposes, as buﬀer overﬂow conditions are easier to
13
diagnose than deadlock conditions.
14

15

Nonblocking message-passing operations, as described in Section 3.7, can be used to

16

avoid the need for buﬀering outgoing messages. This prevents deadlocks due to lack

17

of buﬀer space, and improves performance, by allowing overlap of computation and

18

communication, and avoiding the overheads of allocating buﬀers and copying messages

19

into buﬀers. (End of advice to users.)

20

21 3.6 Buﬀer Allocation and Usage
22

23 A user may specify a buﬀer to be used for buﬀering messages sent in buﬀered mode. Buﬀer24 ing is done by the sender.
25

26
27 MPI_BUFFER_ATTACH(buﬀer, size)

28

IN

29
IN
30

buﬀer size

initial buﬀer address (choice) buﬀer size, in bytes (non-negative integer)

31
32 int MPI_Buffer_attach(void* buffer, int size)

33 MPI_Buffer_attach(buffer, size, ierror) BIND(C)

34

TYPE(*), DIMENSION(..), ASYNCHRONOUS :: buffer

35

INTEGER, INTENT(IN) :: size

36

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

37

38 MPI_BUFFER_ATTACH(BUFFER, SIZE, IERROR)

39

<type> BUFFER(*)

40

INTEGER SIZE, IERROR

41

Provides to MPI a buﬀer in the user’s memory to be used for buﬀering outgoing mes-

42 sages. The buﬀer is used only by messages sent in buﬀered mode. Only one buﬀer can be

43 attached to a process at a time. In C, buﬀer is the starting address of a memory region. In

44 Fortran, one can pass the ﬁrst element of a memory region or a whole array, which must be

45 ‘simply contiguous’ (for ‘simply contiguous,’ see also Section 17.1.12 on page 626).

46

47

48

3.6. BUFFER ALLOCATION AND USAGE

45

MPI_BUFFER_DETACH(buﬀer_addr, size)

1

OUT buﬀer_addr

OUT

size

2
initial buﬀer address (choice) 3

buﬀer size, in bytes (non-negative integer)

4

5

int MPI_Buffer_detach(void* buffer_addr, int* size)

6

7
MPI_Buffer_detach(buffer_addr, size, ierror) BIND(C) 8 USE, INTRINSIC :: ISO_C_BINDING, ONLY : C_PTR 9 TYPE(C_PTR), INTENT(OUT) :: buffer_addr 10 INTEGER, INTENT(OUT) :: size 11 INTEGER, OPTIONAL, INTENT(OUT) :: ierror 12

MPI_BUFFER_DETACH(BUFFER_ADDR, SIZE, IERROR)

13

<type> BUFFER_ADDR(*)

14

INTEGER SIZE, IERROR

15

16
Detach the buﬀer currently associated with MPI. The call returns the address and the 17
size of the detached buﬀer. This operation will block until all messages currently in the 18
buﬀer have been transmitted. Upon return of this function, the user may reuse or deallocate 19
the space taken by the buﬀer. 20

Example 3.10 Calls to attach and detach buﬀers.

21

22

#define BUFFSIZE 10000

23

int size;

24

char *buff;

25

MPI_Buffer_attach( malloc(BUFFSIZE), BUFFSIZE);

26

/* a buffer of 10000 bytes can now be used by MPI_Bsend */

27

MPI_Buffer_detach( &buff, &size);

28

/* Buffer size reduced to zero */

29

MPI_Buffer_attach( buff, size);

30

/* Buffer of 10000 bytes available again */

31

Advice to users. Even though the C functions MPI_Buﬀer_attach and

32

MPI_Buﬀer_detach both have a ﬁrst argument of type void*, these arguments are used 33

diﬀerently: A pointer to the buﬀer is passed to MPI_Buﬀer_attach; the address of the 34

pointer is passed to MPI_Buﬀer_detach, so that this call can return the pointer value. 35

In Fortran with the mpi module or mpif.h, the type of the buﬀer_addr argument is 36

wrongly deﬁned and the argument is therefore unused. In Fortran with the mpi_f08 37

module, the address of the buﬀer is returned as TYPE(C_PTR), see also Example 8.1 38

on page 341 about the use of C_PTR pointers. (End of advice to users.)

39

40

Rationale. Both arguments are deﬁned to be of type void* (rather than

41

void* and void**, respectively), so as to avoid complex type casts. E.g., in the last 42

example, &buﬀ, which is of type char**, can be passed as argument to

43

MPI_Buﬀer_detach without type casting. If the formal parameter had type void** 44

then we would need a type cast before and after the call. (End of rationale.)

45

The statements made in this section describe the behavior of MPI for buﬀered-mode 46

sends. When no buﬀer is currently associated, MPI behaves as if a zero-sized buﬀer is 47

associated with the process.

48

46

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

MPI must provide as much buﬀering for outgoing messages as if outgoing message

2 data were buﬀered by the sending process, in the speciﬁed buﬀer space, using a circular,

3 contiguous-space allocation policy. We outline below a model implementation that deﬁnes

4 this policy. MPI may provide more buﬀering, and may use a better buﬀer allocation algo-

5 rithm than described below. On the other hand, MPI may signal an error whenever the

6 simple buﬀering allocator described below would run out of space. In particular, if no buﬀer

7 is explicitly associated with the process, then any buﬀered send may cause an error.

8

MPI does not provide mechanisms for querying or controlling buﬀering done by standard

9 mode sends. It is expected that vendors will provide such information for their implemen-

10 tations.

11

12

Rationale. There is a wide spectrum of possible implementations of buﬀered com-

13

munication: buﬀering can be done at sender, at receiver, or both; buﬀers can be

14

dedicated to one sender-receiver pair, or be shared by all communications; buﬀering

15

can be done in real or in virtual memory; it can use dedicated memory, or memory

16

shared by other processes; buﬀer space may be allocated statically or be changed dy-

17

namically; etc. It does not seem feasible to provide a portable mechanism for querying

18

or controlling buﬀering that would be compatible with all these choices, yet provide

19

meaningful information. (End of rationale.)

20

21 3.6.1 Model Implementation of Buﬀered Mode

22

23 The model implementation uses the packing and unpacking functions described in Sec-

24 tion 4.2 and the nonblocking communication functions described in Section 3.7.

25

We assume that a circular queue of pending message entries (PME) is maintained.

26 Each entry contains a communication request handle that identiﬁes a pending nonblocking

27 send, a pointer to the next entry and the packed message data. The entries are stored in

28 successive locations in the buﬀer. Free space is available between the queue tail and the

29 queue head.

30

A buﬀered send call results in the execution of the following code.

31

• Traverse sequentially the PME queue from head towards the tail, deleting all entries

32

for communications that have completed, up to the ﬁrst entry with an uncompleted

33

request; update queue head to point to that entry.

34

35

• Compute the number, n, of bytes needed to store an entry for the new message. An up-

36

per bound on n can be computed as follows: A call to the function

37

MPI_PACK_SIZE(count, datatype, comm, size), with the count, datatype and comm

38

arguments used in the MPI_BSEND call, returns an upper bound on the amount

39

of space needed to buﬀer the message data (see Section 4.2). The MPI constant

40

MPI_BSEND_OVERHEAD provides an upper bound on the additional space consumed

41

by the entry (e.g., for pointers or envelope information).

42

43

• Find the next contiguous empty space of n bytes in buﬀer (space following queue tail,

44

or space at start of buﬀer if queue tail is too close to end of buﬀer). If space is not

45

found then raise buﬀer overﬂow error.

46

• Append to end of PME queue in contiguous space the new entry that contains request

47

handle, next pointer and packed message data; MPI_PACK is used to pack data.

48

3.7. NONBLOCKING COMMUNICATION

47

• Post nonblocking send (standard mode) for packed data.

1

2

• Return

3

4

3.7 Nonblocking Communication

5

6

One can improve performance on many systems by overlapping communication and com- 7

putation. This is especially true on systems where communication can be executed au- 8

tonomously by an intelligent communication controller. Light-weight threads are one mech- 9

anism for achieving such overlap. An alternative mechanism that often leads to better 10

performance is to use nonblocking communication. A nonblocking send start call ini- 11

tiates the send operation, but does not complete it. The send start call can return before 12

the message was copied out of the send buﬀer. A separate send complete call is needed 13

to complete the communication, i.e., to verify that the data has been copied out of the send 14

buﬀer. With suitable hardware, the transfer of data out of the sender memory may proceed 15

concurrently with computations done at the sender after the send was initiated and before it 16

completed. Similarly, a nonblocking receive start call initiates the receive operation, but 17

does not complete it. The call can return before a message is stored into the receive buﬀer. 18

A separate receive complete call is needed to complete the receive operation and verify 19

that the data has been received into the receive buﬀer. With suitable hardware, the transfer 20

of data into the receiver memory may proceed concurrently with computations done after 21

the receive was initiated and before it completed. The use of nonblocking receives may also 22

avoid system buﬀering and memory-to-memory copying, as information is provided early 23

on the location of the receive buﬀer.

24

Nonblocking send start calls can use the same four modes as blocking sends: standard, 25

buﬀered, synchronous and ready. These carry the same meaning. Sends of all modes, ready 26

excepted, can be started whether a matching receive has been posted or not; a nonblocking 27

ready send can be started only if a matching receive is posted. In all cases, the send start call 28

is local: it returns immediately, irrespective of the status of other processes. If the call causes 29

some system resource to be exhausted, then it will fail and return an error code. Quality 30

implementations of MPI should ensure that this happens only in “pathological” cases. That 31

is, an MPI implementation should be able to support a large number of pending nonblocking 32

operations.

33

The send-complete call returns when data has been copied out of the send buﬀer. It 34

may carry additional meaning, depending on the send mode.

35

If the send mode is synchronous, then the send can complete only if a matching receive 36

has started. That is, a receive has been posted, and has been matched with the send. In 37

this case, the send-complete call is non-local. Note that a synchronous, nonblocking send 38

may complete, if matched by a nonblocking receive, before the receive complete call occurs. 39

(It can complete as soon as the sender “knows” the transfer will complete, but before the 40

receiver “knows” the transfer will complete.)

41

If the send mode is buﬀered then the message must be buﬀered if there is no pending 42

receive. In this case, the send-complete call is local, and must succeed irrespective of the 43

status of a matching receive.

44

If the send mode is standard then the send-complete call may return before a matching 45

receive is posted, if the message is buﬀered. On the other hand, the receive-complete may 46

not complete until a matching receive is posted, and the message was copied into the receive 47

buﬀer.

48

48

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

Nonblocking sends can be matched with blocking receives, and vice-versa.

2

3

Advice to users. The completion of a send operation may be delayed, for standard

4

mode, and must be delayed, for synchronous mode, until a matching receive is posted.

5

The use of nonblocking sends in these two cases allows the sender to proceed ahead

6

of the receiver, so that the computation is more tolerant of ﬂuctuations in the speeds

7

of the two processes.

8

Nonblocking sends in the buﬀered and ready modes have a more limited impact, e.g.,

9

the blocking version of buﬀered send is capable of completing regardless of when a

10

matching receive call is made. However, separating the start from the completion

11

of these sends still gives some opportunity for optimization within the MPI library.

12

For example, starting a buﬀered send gives an implementation more ﬂexibility in

13

determining if and how the message is buﬀered. There are also advantages for both

14

nonblocking buﬀered and ready modes when data copying can be done concurrently

15

with computation.

16

17

The message-passing model implies that communication is initiated by the sender.

18

The communication will generally have lower overhead if a receive is already posted

19

when the sender initiates the communication (data can be moved directly to the

20

receive buﬀer, and there is no need to queue a pending send request). However, a

21

receive operation can complete only after the matching send has occurred. The use

22

of nonblocking receives allows one to achieve lower communication overheads without

23

blocking the receiver while it waits for the send. (End of advice to users.)

24
25 3.7.1 Communication Request Objects

26 Nonblocking communications use opaque request objects to identify communication oper27 ations and match the operation that initiates the communication with the operation that 28 terminates it. These are system objects that are accessed via a handle. A request object 29 identiﬁes various properties of a communication operation, such as the send mode, the com30 munication buﬀer that is associated with it, its context, the tag and destination arguments 31 to be used for a send, or the tag and source arguments to be used for a receive. In addition, 32 this object stores information about the status of the pending communication operation.
33

34
3.7.2 Communication Initiation
35

36 We use the same naming conventions as for blocking communication: a preﬁx of B, S, or 37 R is used for buﬀered, synchronous or ready mode. In addition a preﬁx of I (for immediate) 38 indicates that the call is nonblocking.

39

40

41

42

43

44

45

46

47

48

3.7. NONBLOCKING COMMUNICATION

49

MPI_ISEND(buf, count, datatype, dest, tag, comm, request)

1

IN

buf

2
initial address of send buﬀer (choice) 3

IN

count

number of elements in send buﬀer (non-negative inte- 4

ger)

5

IN

datatype

IN

dest

IN

tag

datatype of each send buﬀer element (handle)

6

7
rank of destination (integer) 8

message tag (integer)

9

IN

comm

communicator (handle)

10

11

OUT request

communication request (handle)

12

13

int MPI_Isend(const void* buf, int count, MPI_Datatype datatype, int dest,

14

int tag, MPI_Comm comm, MPI_Request *request)

15

MPI_Isend(buf, count, datatype, dest, tag, comm, request, ierror) BIND(C)

16

TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf

17

INTEGER, INTENT(IN) :: count, dest, tag

18

TYPE(MPI_Datatype), INTENT(IN) :: datatype

19

TYPE(MPI_Comm), INTENT(IN) :: comm

20

TYPE(MPI_Request), INTENT(OUT) :: request

21

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

22

23

MPI_ISEND(BUF, COUNT, DATATYPE, DEST, TAG, COMM, REQUEST, IERROR)

24

<type> BUF(*)

25

INTEGER COUNT, DATATYPE, DEST, TAG, COMM, REQUEST, IERROR

26

Start a standard mode, nonblocking send.

27

28

29

MPI_IBSEND(buf, count, datatype, dest, tag, comm, request)

30

IN

buf

IN

count

IN

datatype

initial address of send buﬀer (choice)

31

32

number of elements in send buﬀer (non-negative inte- 33

ger)

34

datatype of each send buﬀer element (handle)

35

IN

dest

rank of destination (integer)

36

37

IN

tag

message tag (integer)

38

IN

comm

communicator (handle)

39

OUT request

communication request (handle)

40

41

42
int MPI_Ibsend(const void* buf, int count, MPI_Datatype datatype, int dest, 43 int tag, MPI_Comm comm, MPI_Request *request) 44

MPI_Ibsend(buf, count, datatype, dest, tag, comm, request, ierror) BIND(C)

45

TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf

46

INTEGER, INTENT(IN) :: count, dest, tag

47

TYPE(MPI_Datatype), INTENT(IN) :: datatype

48

50

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

TYPE(MPI_Comm), INTENT(IN) :: comm

2

TYPE(MPI_Request), INTENT(OUT) :: request

3

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

4

5 MPI_IBSEND(BUF, COUNT, DATATYPE, DEST, TAG, COMM, REQUEST, IERROR)

6

<type> BUF(*)

7

INTEGER COUNT, DATATYPE, DEST, TAG, COMM, REQUEST, IERROR

8

Start a buﬀered mode, nonblocking send.

9

10
11 MPI_ISSEND(buf, count, datatype, dest, tag, comm, request)

12

IN

13
IN
14

15

16

IN

buf count
datatype

initial address of send buﬀer (choice) number of elements in send buﬀer (non-negative integer) datatype of each send buﬀer element (handle)

17

IN

18
IN
19

20

IN

dest tag comm

rank of destination (integer) message tag (integer) communicator (handle)

21

OUT request

22

communication request (handle)

23
int MPI_Issend(const void* buf, int count, MPI_Datatype datatype, int dest,
24
int tag, MPI_Comm comm, MPI_Request *request)
25

26 MPI_Issend(buf, count, datatype, dest, tag, comm, request, ierror) BIND(C)

27

TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf

28

INTEGER, INTENT(IN) :: count, dest, tag

29

TYPE(MPI_Datatype), INTENT(IN) :: datatype

30

TYPE(MPI_Comm), INTENT(IN) :: comm

31

TYPE(MPI_Request), INTENT(OUT) :: request

32

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

33
MPI_ISSEND(BUF, COUNT, DATATYPE, DEST, TAG, COMM, REQUEST, IERROR)
34
<type> BUF(*)
35
INTEGER COUNT, DATATYPE, DEST, TAG, COMM, REQUEST, IERROR
36

37

Start a synchronous mode, nonblocking send.

38

39

40

41

42

43

44

45

46

47

48

3.7. NONBLOCKING COMMUNICATION

51

MPI_IRSEND(buf, count, datatype, dest, tag, comm, request)

1

IN

buf

2
initial address of send buﬀer (choice) 3

IN

count

number of elements in send buﬀer (non-negative inte- 4

ger)

5

IN

datatype

IN

dest

IN

tag

datatype of each send buﬀer element (handle)

6

7
rank of destination (integer) 8

message tag (integer)

9

IN

comm

communicator (handle)

10

11

OUT request

communication request (handle)

12

13

int MPI_Irsend(const void* buf, int count, MPI_Datatype datatype, int dest, 14

int tag, MPI_Comm comm, MPI_Request *request)

15

MPI_Irsend(buf, count, datatype, dest, tag, comm, request, ierror) BIND(C)

16

TYPE(*), DIMENSION(..), INTENT(IN), ASYNCHRONOUS :: buf

17

INTEGER, INTENT(IN) :: count, dest, tag

18

TYPE(MPI_Datatype), INTENT(IN) :: datatype

19

TYPE(MPI_Comm), INTENT(IN) :: comm

20

TYPE(MPI_Request), INTENT(OUT) :: request

21

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

22

23

MPI_IRSEND(BUF, COUNT, DATATYPE, DEST, TAG, COMM, REQUEST, IERROR)

24

<type> BUF(*)

25

INTEGER COUNT, DATATYPE, DEST, TAG, COMM, REQUEST, IERROR

26

Start a ready mode nonblocking send.

27

28

29

MPI_IRECV (buf, count, datatype, source, tag, comm, request)

30

OUT

buf

IN

count

IN

datatype

initial address of receive buﬀer (choice)

31

32

number of elements in receive buﬀer (non-negative in- 33

teger)

34

datatype of each receive buﬀer element (handle)

35

IN

source

IN

tag

IN

comm

rank of source or MPI_ANY_SOURCE (integer)

36

37

message tag or MPI_ANY_TAG (integer)

38

communicator (handle)

39

OUT request

communication request (handle)

40

41

42
int MPI_Irecv(void* buf, int count, MPI_Datatype datatype, int source, 43 int tag, MPI_Comm comm, MPI_Request *request) 44

MPI_Irecv(buf, count, datatype, source, tag, comm, request, ierror) BIND(C) 45

TYPE(*), DIMENSION(..), ASYNCHRONOUS :: buf

46

INTEGER, INTENT(IN) :: count, source, tag

47

TYPE(MPI_Datatype), INTENT(IN) :: datatype

48

52

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

TYPE(MPI_Comm), INTENT(IN) :: comm

2

TYPE(MPI_Request), INTENT(OUT) :: request

3

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

4

5 MPI_IRECV(BUF, COUNT, DATATYPE, SOURCE, TAG, COMM, REQUEST, IERROR)

6

<type> BUF(*)

7

INTEGER COUNT, DATATYPE, SOURCE, TAG, COMM, REQUEST, IERROR

8

Start a nonblocking receive.

9

These calls allocate a communication request object and associate it with the request

10 handle (the argument request). The request can be used later to query the status of the

11 communication or wait for its completion.

12

A nonblocking send call indicates that the system may start copying data out of the

13 send buﬀer. The sender should not modify any part of the send buﬀer after a nonblocking

14 send operation is called, until the send completes.

15

A nonblocking receive call indicates that the system may start writing data into the re-

16 ceive buﬀer. The receiver should not access any part of the receive buﬀer after a nonblocking

17 receive operation is called, until the receive completes.

18

19

Advice to users. To prevent problems with the argument copying and register opti-

20

mization done by Fortran compilers, please note the hints in Sections 17.1.10-17.1.20,

21

especially in Sections 17.1.12 and 17.1.13 on pages 626-629 about “Problems Due to

22

Data Copying and Sequence Association with Subscript Triplets” and “Vector Subscripts”,

23

and in Sections 17.1.16 to 17.1.19 on pages 631 to 642 about “Optimization Problems”,

24

“Code Movements and Register Optimization”, “Temporary Data Movements” and “Per-

25

manent Data Movements”. (End of advice to users.)

26

27 3.7.3 Communication Completion

28

29 The functions MPI_WAIT and MPI_TEST are used to complete a nonblocking communica-

30 tion. The completion of a send operation indicates that the sender is now free to update the

31 locations in the send buﬀer (the send operation itself leaves the content of the send buﬀer

32 unchanged). It does not indicate that the message has been received, rather, it may have

33 been buﬀered by the communication subsystem. However, if a synchronous mode send was

34 used, the completion of the send operation indicates that a matching receive was initiated,

35 and that the message will eventually be received by this matching receive.

36

The completion of a receive operation indicates that the receive buﬀer contains the

37 received message, the receiver is now free to access it, and that the status object is set. It

38 does not indicate that the matching send operation has completed (but indicates, of course,

39 that the send was initiated).

40

We shall use the following terminology: A null handle is a handle with value

41 MPI_REQUEST_NULL. A persistent request and the handle to it are inactive if the re-

42 quest is not associated with any ongoing communication (see Section 3.9). A handle is

43 active if it is neither null nor inactive. An empty status is a status which is set to

44 return tag = MPI_ANY_TAG, source = MPI_ANY_SOURCE, error = MPI_SUCCESS, and is

45 also internally conﬁgured so that calls to MPI_GET_COUNT, MPI_GET_ELEMENTS, and

46 MPI_GET_ELEMENTS_X return count = 0 and MPI_TEST_CANCELLED returns false. We

47 set a status variable to empty when the value returned by it is not signiﬁcant. Status is set

48 in this way so as to prevent errors due to accesses of stale information.

3.7. NONBLOCKING COMMUNICATION

53

The ﬁelds in a status object returned by a call to MPI_WAIT, MPI_TEST, or any 1

of the other derived functions (MPI_{TEST|WAIT}{ALL|SOME|ANY}), where the request 2

corresponds to a send call, are undeﬁned, with two exceptions: The error status ﬁeld will 3

contain valid information if the wait or test call returned with MPI_ERR_IN_STATUS; and 4

the returned status can be queried by the call MPI_TEST_CANCELLED.

5

Error codes belonging to the error class MPI_ERR_IN_STATUS should be returned only by 6

the MPI completion functions that take arrays of MPI_Status. For the functions MPI_TEST, 7

MPI_TESTANY, MPI_WAIT, and MPI_WAITANY, which return a single MPI_Status value, 8

the normal MPI error return process should be used (not the MPI_ERROR ﬁeld in the

9

MPI_Status argument).

10

11

12

MPI_WAIT(request, status)

13

INOUT request

request (handle)

14

15

OUT status

status object (Status)

16

17

int MPI_Wait(MPI_Request *request, MPI_Status *status)

18

MPI_Wait(request, status, ierror) BIND(C)

19

TYPE(MPI_Request), INTENT(INOUT) :: request

20

TYPE(MPI_Status) :: status

21

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

22

23

MPI_WAIT(REQUEST, STATUS, IERROR)

24

INTEGER REQUEST, STATUS(MPI_STATUS_SIZE), IERROR

25

A call to MPI_WAIT returns when the operation identiﬁed by request is complete. If 26

the request is an active persistent request, it is marked inactive. Any other type of request 27

is and the request handle is set to MPI_REQUEST_NULL. MPI_WAIT is a non-local operation. 28

The call returns, in status, information on the completed operation. The content of 29

the status object for a receive operation can be accessed as described in Section 3.2.5. The 30

status object for a send operation may be queried by a call to MPI_TEST_CANCELLED 31

(see Section 3.8).

32

One is allowed to call MPI_WAIT with a null or inactive request argument. In this case 33

the operation returns immediately with empty status.

34

35

Advice to users. Successful return of MPI_WAIT after a MPI_IBSEND implies that 36

the user send buﬀer can be reused — i.e., data has been sent out or copied into 37

a buﬀer attached with MPI_BUFFER_ATTACH. Note that, at this point, we can no 38

longer cancel the send (see Section 3.8). If a matching receive is never posted, then the 39

buﬀer cannot be freed. This runs somewhat counter to the stated goal of MPI_CANCEL 40

(always being able to free program space that was committed to the communication 41

subsystem). (End of advice to users.)

42

43

Advice to implementors. In a multithreaded environment, a call to MPI_WAIT should 44

block only the calling thread, allowing the thread scheduler to schedule another thread 45

for execution. (End of advice to implementors.)

46

47

48

54

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 MPI_TEST(request, ﬂag, status)

2
INOUT request
3

4

OUT

ﬂag

communication request (handle) true if operation completed (logical)

5

OUT status

6

status object (Status)

7 int MPI_Test(MPI_Request *request, int *flag, MPI_Status *status)
8

9 MPI_Test(request, flag, status, ierror) BIND(C)

10

TYPE(MPI_Request), INTENT(INOUT) :: request

11

LOGICAL, INTENT(OUT) :: flag

12

TYPE(MPI_Status) :: status

13

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

14
MPI_TEST(REQUEST, FLAG, STATUS, IERROR)
15
LOGICAL FLAG
16
INTEGER REQUEST, STATUS(MPI_STATUS_SIZE), IERROR
17

18

A call to MPI_TEST returns ﬂag = true if the operation identiﬁed by request is complete.

19 In such a case, the status object is set to contain information on the completed operation.

20 If the request is an active persistent request, it is marked as inactive. Any other type

21 of request is deallocated and the request handle is set to MPI_REQUEST_NULL. The call

22 returns ﬂag = false if the operation identiﬁed by request is not complete. In this case, the

23 value of the status object is undeﬁned. MPI_TEST is a local operation.

24

The return status object for a receive operation carries information that can be accessed

25 as described in Section 3.2.5. The status object for a send operation carries information

26 that can be accessed by a call to MPI_TEST_CANCELLED (see Section 3.8).

27

One is allowed to call MPI_TEST with a null or inactive request argument. In such a

28 case the operation returns with ﬂag = true and empty status.

29

The functions MPI_WAIT and MPI_TEST can be used to complete both sends and

30 receives.

31

32

Advice to users. The use of the nonblocking MPI_TEST call allows the user to

33

schedule alternative activities within a single thread of execution. An event-driven

34

thread scheduler can be emulated with periodic calls to MPI_TEST. (End of advice to

35

users.)

36

37
Example 3.11 Simple usage of nonblocking operations and MPI_WAIT.
38

39 CALL MPI_COMM_RANK(comm, rank, ierr)

40 IF (rank.EQ.0) THEN

41

CALL MPI_ISEND(a(1), 10, MPI_REAL, 1, tag, comm, request, ierr)

42

**** do some computation to mask latency ****

43

CALL MPI_WAIT(request, status, ierr)

44 ELSE IF (rank.EQ.1) THEN

45

CALL MPI_IRECV(a(1), 15, MPI_REAL, 0, tag, comm, request, ierr)

46

**** do some computation to mask latency ****

47

CALL MPI_WAIT(request, status, ierr)

48 END IF

3.7. NONBLOCKING COMMUNICATION

55

A request object can be deallocated without waiting for the associated communication 1

to complete, by using the following operation.

2

3

4

MPI_REQUEST_FREE(request)

5

INOUT request

communication request (handle)

6

7

8
int MPI_Request_free(MPI_Request *request)
9

MPI_Request_free(request, ierror) BIND(C)

10

TYPE(MPI_Request), INTENT(INOUT) :: request

11

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

12

13
MPI_REQUEST_FREE(REQUEST, IERROR) 14 INTEGER REQUEST, IERROR 15

Mark the request object for deallocation and set request to MPI_REQUEST_NULL. An 16

ongoing communication that is associated with the request will be allowed to complete. The 17

request will be deallocated only after its completion.

18

19

Rationale. The MPI_REQUEST_FREE mechanism is provided for reasons of perfor- 20

mance and convenience on the sending side. (End of rationale.)

21

22
Advice to users. Once a request is freed by a call to MPI_REQUEST_FREE, it is not 23 possible to check for the successful completion of the associated communication with 24 calls to MPI_WAIT or MPI_TEST. Also, if an error occurs subsequently during the 25 communication, an error code cannot be returned to the user — such an error must
26
be treated as fatal. An active receive request should never be freed as the receiver 27 will have no way to verify that the receive has completed and the receive buﬀer can 28 be reused. (End of advice to users.)
29

30

Example 3.12 An example using MPI_REQUEST_FREE.

31

32

CALL MPI_COMM_RANK(MPI_COMM_WORLD, rank, ierr)

33

IF (rank.EQ.0) THEN

34

DO i=1, n

35

CALL MPI_ISEND(outval, 1, MPI_REAL, 1, 0, MPI_COMM_WORLD, req, ierr)

36

CALL MPI_REQUEST_FREE(req, ierr)

37

CALL MPI_IRECV(inval, 1, MPI_REAL, 1, 0, MPI_COMM_WORLD, req, ierr)

38

CALL MPI_WAIT(req, status, ierr)

39

END DO

40

ELSE IF (rank.EQ.1) THEN

41

CALL MPI_IRECV(inval, 1, MPI_REAL, 0, 0, MPI_COMM_WORLD, req, ierr)

42

CALL MPI_WAIT(req, status, ierr)

43

DO I=1, n-1

44

CALL MPI_ISEND(outval, 1, MPI_REAL, 0, 0, MPI_COMM_WORLD, req, ierr) 45

CALL MPI_REQUEST_FREE(req, ierr)

46

CALL MPI_IRECV(inval, 1, MPI_REAL, 0, 0, MPI_COMM_WORLD, req, ierr)

47

CALL MPI_WAIT(req, status, ierr)

48

56

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

END DO

2

CALL MPI_ISEND(outval, 1, MPI_REAL, 0, 0, MPI_COMM_WORLD, req, ierr)

3

CALL MPI_WAIT(req, status, ierr)

4 END IF

5

6 3.7.4 Semantics of Nonblocking Communications
7
8 The semantics of nonblocking communication is deﬁned by suitably extending the deﬁnitions 9 in Section 3.5.

10
11 Order Nonblocking communication operations are ordered according to the execution order 12 of the calls that initiate the communication. The non-overtaking requirement of Section 3.5 13 is extended to nonblocking communication, with this deﬁnition of order being used.

14
15 Example 3.13 Message ordering for nonblocking operations.

16
CALL MPI_COMM_RANK(comm, rank, ierr)
17
IF (RANK.EQ.0) THEN
18
CALL MPI_ISEND(a, 1, MPI_REAL, 1, 0, comm, r1, ierr)
19
CALL MPI_ISEND(b, 1, MPI_REAL, 1, 0, comm, r2, ierr)
20
ELSE IF (rank.EQ.1) THEN
21
CALL MPI_IRECV(a, 1, MPI_REAL, 0, MPI_ANY_TAG, comm, r1, ierr)
22
CALL MPI_IRECV(b, 1, MPI_REAL, 0, 0, comm, r2, ierr)
23
END IF
24
CALL MPI_WAIT(r1, status, ierr)
25
CALL MPI_WAIT(r2, status, ierr)
26

27 The ﬁrst send of process zero will match the ﬁrst receive of process one, even if both messages 28 are sent before process one executes either receive.

29

30 Progress A call to MPI_WAIT that completes a receive will eventually terminate and return 31 if a matching send has been started, unless the send is satisﬁed by another receive. In 32 particular, if the matching send is nonblocking, then the receive should complete even if no 33 call is executed by the sender to complete the send. Similarly, a call to MPI_WAIT that 34 completes a send will eventually return if a matching receive has been started, unless the 35 receive is satisﬁed by another send, and even if no call is executed to complete the receive.
36

37 Example 3.14 An illustration of progress semantics.

38

39 CALL MPI_COMM_RANK(comm, rank, ierr)

40 IF (RANK.EQ.0) THEN

41

CALL MPI_SSEND(a, 1, MPI_REAL, 1, 0, comm, ierr)

42

CALL MPI_SEND(b, 1, MPI_REAL, 1, 1, comm, ierr)

43 ELSE IF (rank.EQ.1) THEN

44

CALL MPI_IRECV(a, 1, MPI_REAL, 0, 0, comm, r, ierr)

45

CALL MPI_RECV(b, 1, MPI_REAL, 0, 1, comm, status, ierr)

46

CALL MPI_WAIT(r, status, ierr)

47 END IF

48

3.7. NONBLOCKING COMMUNICATION

57

This code should not deadlock in a correct MPI implementation. The ﬁrst synchronous 1

send of process zero must complete after process one posts the matching (nonblocking) 2

receive even if process one has not yet reached the completing wait call. Thus, process zero 3

will continue and execute the second send, allowing process one to complete execution.

4

If an MPI_TEST that completes a receive is repeatedly called with the same arguments, 5

and a matching send has been started, then the call will eventually return ﬂag = true, unless 6

the send is satisﬁed by another receive. If an MPI_TEST that completes a send is repeatedly 7

called with the same arguments, and a matching receive has been started, then the call will 8

eventually return ﬂag = true, unless the receive is satisﬁed by another send.

9

10

3.7.5 Multiple Completions

11

12

It is convenient to be able to wait for the completion of any, some, or all the operations 13

in a list, rather than having to wait for a speciﬁc message. A call to MPI_WAITANY or 14

MPI_TESTANY can be used to wait for the completion of one out of several operations. A 15

call to MPI_WAITALL or MPI_TESTALL can be used to wait for all pending operations in 16

a list. A call to MPI_WAITSOME or MPI_TESTSOME can be used to complete all enabled 17

operations in a list.

18

19

20
MPI_WAITANY (count, array_of_requests, index, status) 21

IN

count

list length (non-negative integer)

22

INOUT OUT OUT

array_of_requests index status

array of requests (array of handles)

23

24
index of handle for operation that completed (integer) 25

status object (Status)

26

27

int MPI_Waitany(int count, MPI_Request array_of_requests[], int *index,

28

MPI_Status *status)

29

30
MPI_Waitany(count, array_of_requests, index, status, ierror) BIND(C) 31 INTEGER, INTENT(IN) :: count 32 TYPE(MPI_Request), INTENT(INOUT) :: array_of_requests(count) 33 INTEGER, INTENT(OUT) :: index 34 TYPE(MPI_Status) :: status 35 INTEGER, OPTIONAL, INTENT(OUT) :: ierror 36

MPI_WAITANY(COUNT, ARRAY_OF_REQUESTS, INDEX, STATUS, IERROR)

37

INTEGER COUNT, ARRAY_OF_REQUESTS(*), INDEX, STATUS(MPI_STATUS_SIZE),

38

IERROR

39

40

Blocks until one of the operations associated with the active requests in the array has 41

completed. If more than one operation is enabled and can terminate, one is arbitrarily 42

chosen. Returns in index the index of that request in the array and returns in status the 43

status of the completing operation. (The array is indexed from zero in C, and from one in 44

Fortran.) If the request is an active persistent request, it is marked inactive. Any other 45

type of request is deallocated and the request handle is set to MPI_REQUEST_NULL.

46

The array_of_requests list may contain null or inactive handles. If the list contains no 47

active handles (list has length zero or all entries are null or inactive), then the call returns 48

58

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 immediately with index = MPI_UNDEFINED, and an empty status.

2

The execution of MPI_WAITANY(count, array_of_requests, index, status) has the same

3 eﬀect as the execution of MPI_WAIT(&array_of_requests[i], status), where i is the value

4 returned by index (unless the value of index is MPI_UNDEFINED). MPI_WAITANY with an

5 array containing one active entry is equivalent to MPI_WAIT.

6

7
8 MPI_TESTANY(count, array_of_requests, index, ﬂag, status)

9

IN

count

10
INOUT array_of_requests
11

12

OUT

index

13

list length (non-negative integer) array of requests (array of handles) index of operation that completed, or MPI_UNDEFINED if none completed (integer)

14

OUT

ﬂag

15
OUT status
16

true if one of the operations is complete (logical) status object (Status)

17

18 int MPI_Testany(int count, MPI_Request array_of_requests[], int *index,

19

int *flag, MPI_Status *status)

20 MPI_Testany(count, array_of_requests, index, flag, status, ierror) BIND(C)

21

INTEGER, INTENT(IN) :: count

22

TYPE(MPI_Request), INTENT(INOUT) :: array_of_requests(count)

23

INTEGER, INTENT(OUT) :: index

24

LOGICAL, INTENT(OUT) :: flag

25

TYPE(MPI_Status) :: status

26

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

27

28 MPI_TESTANY(COUNT, ARRAY_OF_REQUESTS, INDEX, FLAG, STATUS, IERROR)

29

LOGICAL FLAG

30

INTEGER COUNT, ARRAY_OF_REQUESTS(*), INDEX, STATUS(MPI_STATUS_SIZE),

31

IERROR

32

Tests for completion of either one or none of the operations associated with active

33 handles. In the former case, it returns ﬂag = true, returns in index the index of this request

34 in the array, and returns in status the status of that operation. If the request is an active

35 persistent request, it is marked as inactive. Any other type of request is deallocated and

36 the handle is set to MPI_REQUEST_NULL. (The array is indexed from zero in C, and from

37 one in Fortran.) In the latter case (no operation completed), it returns ﬂag = false, returns

38 a value of MPI_UNDEFINED in index and status is undeﬁned.

39

The array may contain null or inactive handles. If the array contains no active handles

40 then the call returns immediately with ﬂag = true, index = MPI_UNDEFINED, and an empty

41 status.

42

If the array of requests contains active handles then the execution of

43 MPI_TESTANY(count, array_of_requests, index, status) has the same eﬀect as the execution

44 of MPI_TEST( &array_of_requests[i], ﬂag, status), for i=0, 1 ,..., count-1, in some arbitrary

45 order, until one call returns ﬂag = true, or all fail. In the former case, index is set to the

46 last value of i, and in the latter case, it is set to MPI_UNDEFINED. MPI_TESTANY with an

47 array containing one active entry is equivalent to MPI_TEST.

48

3.7. NONBLOCKING COMMUNICATION

59

MPI_WAITALL( count, array_of_requests, array_of_statuses)

1

IN

count

2
lists length (non-negative integer) 3

INOUT array_of_requests

array of requests (array of handles)

4

OUT array_of_statuses

array of status objects (array of Status)

5

6

int MPI_Waitall(int count, MPI_Request array_of_requests[],

7

MPI_Status array_of_statuses[])

8

9

MPI_Waitall(count, array_of_requests, array_of_statuses, ierror) BIND(C)

10

INTEGER, INTENT(IN) :: count

11

TYPE(MPI_Request), INTENT(INOUT) :: array_of_requests(count)

12

TYPE(MPI_Status) :: array_of_statuses(*)

13

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

14

15
MPI_WAITALL(COUNT, ARRAY_OF_REQUESTS, ARRAY_OF_STATUSES, IERROR) 16 INTEGER COUNT, ARRAY_OF_REQUESTS(*) 17 INTEGER ARRAY_OF_STATUSES(MPI_STATUS_SIZE,*), IERROR 18

Blocks until all communication operations associated with active handles in the list 19

complete, and return the status of all these operations (this includes the case where no 20

handle in the list is active). Both arrays have the same number of valid entries. The 21

i-th entry in array_of_statuses is set to the return status of the i-th operation. Active 22

persistent requests are marked inactive. Requests of any other type are deallocated and the 23

corresponding handles in the array are set to MPI_REQUEST_NULL. The list may contain 24

null or inactive handles. The call sets to empty the status of each such entry.

25

The error-free execution of MPI_WAITALL(count, array_of_requests, array_of_statuses) 26

has the same eﬀect as the execution of

27

MPI_WAIT(&array_of_request[i], &array_of_statuses[i]), for i=0 ,..., count-1, in some arbi- 28

trary order. MPI_WAITALL with an array of length one is equivalent to MPI_WAIT.

29

When one or more of the communications completed by a call to MPI_WAITALL fail, 30

it is desirable to return speciﬁc information on each communication. The function

31

MPI_WAITALL will return in such case the error code MPI_ERR_IN_STATUS and will set the 32

error ﬁeld of each status to a speciﬁc error code. This code will be MPI_SUCCESS, if the 33

speciﬁc communication completed; it will be another speciﬁc error code, if it failed; or it can 34

be MPI_ERR_PENDING if it has neither failed nor completed. The function MPI_WAITALL 35

will return MPI_SUCCESS if no request had an error, or will return another error code if it 36

failed for other reasons (such as invalid arguments). In such cases, it will not update the 37

error ﬁelds of the statuses.

38

39
Rationale. This design streamlines error handling in the application. The application 40 code need only test the (single) function result to determine if an error has occurred. It 41 needs to check each individual status only when an error occurred. (End of rationale.) 42

43

44

45

46

47

48

60

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 MPI_TESTALL(count, array_of_requests, ﬂag, array_of_statuses)

2
IN
3

count

lists length (non-negative integer)

4

INOUT array_of_requests

array of requests (array of handles)

5

OUT

ﬂag

6
OUT array_of_statuses
7

(logical) array of status objects (array of Status)

8

9 int MPI_Testall(int count, MPI_Request array_of_requests[], int *flag,

10

MPI_Status array_of_statuses[])

11 MPI_Testall(count, array_of_requests, flag, array_of_statuses, ierror)

12

BIND(C)

13

INTEGER, INTENT(IN) :: count

14

TYPE(MPI_Request), INTENT(INOUT) :: array_of_requests(count)

15

LOGICAL, INTENT(OUT) :: flag

16

TYPE(MPI_Status) :: array_of_statuses(*)

17

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

18

19 MPI_TESTALL(COUNT, ARRAY_OF_REQUESTS, FLAG, ARRAY_OF_STATUSES, IERROR)

20

LOGICAL FLAG

21

INTEGER COUNT, ARRAY_OF_REQUESTS(*),

22

ARRAY_OF_STATUSES(MPI_STATUS_SIZE,*), IERROR

23

Returns ﬂag = true if all communications associated with active handles in the array

24 have completed (this includes the case where no handle in the list is active). In this case, each

25 status entry that corresponds to an active request is set to the status of the corresponding

26 operation. Active persistent requests are marked inactive. Requests of any other type are

27 deallocated and the corresponding handles in the array are set to MPI_REQUEST_NULL.

28 Each status entry that corresponds to a null or inactive handle is set to empty.

29

Otherwise, ﬂag = false is returned, no request is modiﬁed and the values of the status

30 entries are undeﬁned. This is a local operation.

31

Errors that occurred during the execution of MPI_TESTALL are handled in the same

32 manner as errors in MPI_WAITALL.

33

34
35 MPI_WAITSOME(incount, array_of_requests, outcount, array_of_indices, array_of_statuses)

36

37

IN

incount

38

39

INOUT array_of_requests

length of array_of_requests (non-negative integer) array of requests (array of handles)

40

OUT outcount

number of completed requests (integer)

41

OUT array_of_indices

42

43

44

OUT array_of_statuses

45

array of indices of operations that completed (array of integers)
array of status objects for operations that completed (array of Status)

46

47 int MPI_Waitsome(int incount, MPI_Request array_of_requests[],

48

int *outcount, int array_of_indices[],

3.7. NONBLOCKING COMMUNICATION

61

MPI_Status array_of_statuses[])

1

2

MPI_Waitsome(incount, array_of_requests, outcount, array_of_indices,

3

array_of_statuses, ierror) BIND(C)

4

INTEGER, INTENT(IN) :: incount

5

TYPE(MPI_Request), INTENT(INOUT) :: array_of_requests(incount)

6

INTEGER, INTENT(OUT) :: outcount, array_of_indices(*)

7

TYPE(MPI_Status) :: array_of_statuses(*)

8

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

9

MPI_WAITSOME(INCOUNT, ARRAY_OF_REQUESTS, OUTCOUNT, ARRAY_OF_INDICES,

10

ARRAY_OF_STATUSES, IERROR)

11

INTEGER INCOUNT, ARRAY_OF_REQUESTS(*), OUTCOUNT, ARRAY_OF_INDICES(*),

12

ARRAY_OF_STATUSES(MPI_STATUS_SIZE,*), IERROR

13

14

Waits until at least one of the operations associated with active handles in the list have 15

completed. Returns in outcount the number of requests from the list array_of_requests that 16

have completed. Returns in the ﬁrst outcount locations of the array array_of_indices the 17

indices of these operations (index within the array array_of_requests; the array is indexed 18

from zero in C and from one in Fortran). Returns in the ﬁrst outcount locations of the array 19

array_of_status the status for these completed operations. Completed active persistent 20

requests are marked as inactive. Any other type or request that completed is deallocated, 21

and the associated handle is set to MPI_REQUEST_NULL.

22

If the list contains no active handles, then the call returns immediately with outcount 23

= MPI_UNDEFINED.

24

When one or more of the communications completed by MPI_WAITSOME fails, then 25

it is desirable to return speciﬁc information on each communication. The arguments

26

outcount, array_of_indices and array_of_statuses will be adjusted to indicate completion of 27

all communications that have succeeded or failed. The call will return the error code 28

MPI_ERR_IN_STATUS and the error ﬁeld of each status returned will be set to indicate 29

success or to indicate the speciﬁc error that occurred. The call will return MPI_SUCCESS 30

if no request resulted in an error, and will return another error code if it failed for other 31

reasons (such as invalid arguments). In such cases, it will not update the error ﬁelds of the 32

statuses.

33

34

MPI_TESTSOME(incount, array_of_requests, outcount, array_of_indices, array_of_statuses) 35
36

IN

incount

37

length of array_of_requests (non-negative integer)

38

INOUT OUT OUT

array_of_requests outcount array_of_indices

OUT array_of_statuses

array of requests (array of handles)

39

number of completed requests (integer)

40

41

array of indices of operations that completed (array of 42

integers)

43

array of status objects for operations that completed 44

(array of Status)

45

46

int MPI_Testsome(int incount, MPI_Request array_of_requests[],

47

int *outcount, int array_of_indices[],

48

62

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

MPI_Status array_of_statuses[])

2

3 MPI_Testsome(incount, array_of_requests, outcount, array_of_indices,

4

array_of_statuses, ierror) BIND(C)

5

INTEGER, INTENT(IN) :: incount

6

TYPE(MPI_Request), INTENT(INOUT) :: array_of_requests(incount)

7

INTEGER, INTENT(OUT) :: outcount, array_of_indices(*)

8

TYPE(MPI_Status) :: array_of_statuses(*)

9

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

10 MPI_TESTSOME(INCOUNT, ARRAY_OF_REQUESTS, OUTCOUNT, ARRAY_OF_INDICES,

11

ARRAY_OF_STATUSES, IERROR)

12

INTEGER INCOUNT, ARRAY_OF_REQUESTS(*), OUTCOUNT, ARRAY_OF_INDICES(*),

13

ARRAY_OF_STATUSES(MPI_STATUS_SIZE,*), IERROR

14

15

Behaves like MPI_WAITSOME, except that it returns immediately. If no operation has

16 completed it returns outcount = 0. If there is no active handle in the list it returns outcount

17 = MPI_UNDEFINED.

18

MPI_TESTSOME is a local operation, which returns immediately, whereas

19 MPI_WAITSOME will block until a communication completes, if it was passed a list that

20 contains at least one active handle. Both calls fulﬁll a fairness requirement: If a request for

21 a receive repeatedly appears in a list of requests passed to MPI_WAITSOME or

22 MPI_TESTSOME, and a matching send has been posted, then the receive will eventually

23 succeed, unless the send is satisﬁed by another receive; and similarly for send requests.

24

Errors that occur during the execution of MPI_TESTSOME are handled as for

25 MPI_WAITSOME.

26
Advice to users. The use of MPI_TESTSOME is likely to be more eﬃcient than the use
27
of MPI_TESTANY. The former returns information on all completed communications,
28
with the latter, a new call is required for each communication that completes.
29

30

A server with multiple clients can use MPI_WAITSOME so as not to starve any client.

31

Clients send messages to the server with service requests. The server calls

32

MPI_WAITSOME with one receive request for each client, and then handles all receives

33

that completed. If a call to MPI_WAITANY is used instead, then one client could starve

34

while requests from another client always sneak in ﬁrst. (End of advice to users.)

35

36

Advice to implementors. MPI_TESTSOME should complete as many pending com-

37

munications as possible. (End of advice to implementors.)

38

39
Example 3.15 Client-server code (starvation can occur).
40

41

42 CALL MPI_COMM_SIZE(comm, size, ierr)

43 CALL MPI_COMM_RANK(comm, rank, ierr)

44 IF(rank .GT. 0) THEN

! client code

45

DO WHILE(.TRUE.)

46

CALL MPI_ISEND(a, n, MPI_REAL, 0, tag, comm, request, ierr)

47

CALL MPI_WAIT(request, status, ierr)

48

END DO

3.7. NONBLOCKING COMMUNICATION

63

ELSE

! rank=0 -- server code

1

DO i=1, size-1

2

CALL MPI_IRECV(a(1,i), n, MPI_REAL, i, tag,

3

comm, request_list(i), ierr)

4

END DO

5

DO WHILE(.TRUE.)

6

CALL MPI_WAITANY(size-1, request_list, index, status, ierr)

7

CALL DO_SERVICE(a(1,index)) ! handle one message

8

CALL MPI_IRECV(a(1, index), n, MPI_REAL, index, tag,

9

comm, request_list(index), ierr)

10

END DO

11

END IF

12

13

14
Example 3.16 Same code, using MPI_WAITSOME. 15

16

CALL MPI_COMM_SIZE(comm, size, ierr)

17

CALL MPI_COMM_RANK(comm, rank, ierr)

18

IF(rank .GT. 0) THEN

! client code

19

DO WHILE(.TRUE.)

20

CALL MPI_ISEND(a, n, MPI_REAL, 0, tag, comm, request, ierr)

21

CALL MPI_WAIT(request, status, ierr)

22

END DO

23

ELSE

! rank=0 -- server code

24

DO i=1, size-1

25

CALL MPI_IRECV(a(1,i), n, MPI_REAL, i, tag,

26

comm, request_list(i), ierr)

27

END DO

28

DO WHILE(.TRUE.)

29

CALL MPI_WAITSOME(size, request_list, numdone,

30

indices, statuses, ierr)

31

DO i=1, numdone

32

CALL DO_SERVICE(a(1, indices(i)))

33

CALL MPI_IRECV(a(1, indices(i)), n, MPI_REAL, 0, tag,

34

comm, request_list(indices(i)), ierr)

35

END DO

36

END DO

37

END IF

38

39

3.7.6 Non-destructive Test of status

40

41

This call is useful for accessing the information associated with a request, without freeing 42

the request (in case the user is expected to access it later). It allows one to layer libraries 43

more conveniently, since multiple layers of software may access the same completed request 44

and extract from it the status information.

45

46

47

48

64

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 MPI_REQUEST_GET_STATUS( request, ﬂag, status )

2
IN
3

request

request (handle)

4

OUT

ﬂag

boolean ﬂag, same as from MPI_TEST (logical)

5

OUT status

6

status object if ﬂag is true (Status)

7 int MPI_Request_get_status(MPI_Request request, int *flag,

8

MPI_Status *status)

9

10 MPI_Request_get_status(request, flag, status, ierror) BIND(C)

11

TYPE(MPI_Request), INTENT(IN) :: request

12

LOGICAL, INTENT(OUT) :: flag

13

TYPE(MPI_Status) :: status

14

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

15
MPI_REQUEST_GET_STATUS( REQUEST, FLAG, STATUS, IERROR)
16
INTEGER REQUEST, STATUS(MPI_STATUS_SIZE), IERROR
17
LOGICAL FLAG
18

19

Sets ﬂag=true if the operation is complete, and, if so, returns in status the request

20 status. However, unlike test or wait, it does not deallocate or inactivate the request; a

21 subsequent call to test, wait or free should be executed with that request. It sets ﬂag=false

22 if the operation is not complete.

23

One is allowed to call MPI_REQUEST_GET_STATUS with a null or inactive request

24 argument. In such a case the operation returns with ﬂag=true and empty status.

25

26 3.8 Probe and Cancel
27

28 The MPI_PROBE, MPI_IPROBE, MPI_MPROBE, and MPI_IMPROBE operations allow in-

29 coming messages to be checked for, without actually receiving them. The user can then

30 decide how to receive them, based on the information returned by the probe (basically, the

31 information returned by status). In particular, the user may allocate memory for the receive

32 buﬀer, according to the length of the probed message.

33

The MPI_CANCEL operation allows pending communications to be cancelled. This is

34 required for cleanup. Posting a send or a receive ties up user resources (send or receive

35 buﬀers), and a cancel may be needed to free these resources gracefully.

36

37 3.8.1 Probe
38

39

40

MPI_IPROBE(source, tag, comm, ﬂag, status)

41

IN

source

rank of source or MPI_ANY_SOURCE (integer)

42

43

IN

tag

message tag or MPI_ANY_TAG (integer)

44

IN

comm

45

OUT

ﬂag

46

47

OUT status

communicator (handle) (logical) status object (Status)

48

3.8. PROBE AND CANCEL

65

int MPI_Iprobe(int source, int tag, MPI_Comm comm, int *flag,

1

MPI_Status *status)

2

3

MPI_Iprobe(source, tag, comm, flag, status, ierror) BIND(C)

4

INTEGER, INTENT(IN) :: source, tag

5

TYPE(MPI_Comm), INTENT(IN) :: comm

6

LOGICAL, INTENT(OUT) :: flag

7

TYPE(MPI_Status) :: status

8

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

9

MPI_IPROBE(SOURCE, TAG, COMM, FLAG, STATUS, IERROR)

10

LOGICAL FLAG

11

INTEGER SOURCE, TAG, COMM, STATUS(MPI_STATUS_SIZE), IERROR

12

13

MPI_IPROBE(source, tag, comm, ﬂag, status) returns ﬂag = true if there is a message 14

that can be received and that matches the pattern speciﬁed by the arguments source, tag, 15

and comm. The call matches the same message that would have been received by a call to 16

MPI_RECV(..., source, tag, comm, status) executed at the same point in the program, and 17

returns in status the same value that would have been returned by MPI_RECV(). Otherwise, 18

the call returns ﬂag = false, and leaves status undeﬁned.

19

If MPI_IPROBE returns ﬂag = true, then the content of the status object can be sub- 20

sequently accessed as described in Section 3.2.5 to ﬁnd the source, tag and length of the 21

probed message.

22

A subsequent receive executed with the same communicator, and the source and tag re- 23

turned in status by MPI_IPROBE will receive the message that was matched by the probe, if 24

no other intervening receive occurs after the probe, and the send is not successfully cancelled 25

before the receive. If the receiving process is multithreaded, it is the user’s responsibility 26

to ensure that the last condition holds.

27

The source argument of MPI_PROBE can be MPI_ANY_SOURCE, and the tag argument 28

can be MPI_ANY_TAG, so that one can probe for messages from an arbitrary source and/or 29

with an arbitrary tag. However, a speciﬁc communication context must be provided with 30

the comm argument.

31

It is not necessary to receive a message immediately after it has been probed for, and 32

the same message may be probed for several times before it is received.

33

A probe with MPI_PROC_NULL as source returns ﬂag = true, and the status object 34

returns source = MPI_PROC_NULL, tag = MPI_ANY_TAG, and count = 0; see Section 3.11 35

on page 81.

36

37

MPI_PROBE(source, tag, comm, status)

38

39

IN

source

rank of source or MPI_ANY_SOURCE (integer)

40

IN

tag

IN

comm

OUT status

message tag or MPI_ANY_TAG (integer)

41

42
communicator (handle)
43

status object (Status)

44

45

int MPI_Probe(int source, int tag, MPI_Comm comm, MPI_Status *status)

46

47
MPI_Probe(source, tag, comm, status, ierror) BIND(C) 48

66

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1

INTEGER, INTENT(IN) :: source, tag

2

TYPE(MPI_Comm), INTENT(IN) :: comm

3

TYPE(MPI_Status) :: status

4

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

5

6 MPI_PROBE(SOURCE, TAG, COMM, STATUS, IERROR)

7

INTEGER SOURCE, TAG, COMM, STATUS(MPI_STATUS_SIZE), IERROR

8

MPI_PROBE behaves like MPI_IPROBE except that it is a blocking call that returns

9 only after a matching message has been found.

10

The MPI implementation of MPI_PROBE and MPI_IPROBE needs to guarantee progress:

11 if a call to MPI_PROBE has been issued by a process, and a send that matches the probe

12 has been initiated by some process, then the call to MPI_PROBE will return, unless the

13 message is received by another concurrent receive operation (that is executed by another

14 thread at the probing process). Similarly, if a process busy waits with MPI_IPROBE and a

15 matching message has been issued, then the call to MPI_IPROBE will eventually return ﬂag

16 = true unless the message is received by another concurrent receive operation or matched

17 by a concurrent matched probe.

18

19 Example 3.17

20

Use blocking probe to wait for an incoming message.

21 22 23 24 25 26 27 28 29 30 31
32 100
33
34 200
35 36 37

CALL MPI_COMM_RANK(comm, rank, ierr) IF (rank.EQ.0) THEN
CALL MPI_SEND(i, 1, MPI_INTEGER, 2, 0, comm, ierr) ELSE IF (rank.EQ.1) THEN
CALL MPI_SEND(x, 1, MPI_REAL, 2, 0, comm, ierr) ELSE IF (rank.EQ.2) THEN
DO i=1, 2 CALL MPI_PROBE(MPI_ANY_SOURCE, 0, comm, status, ierr) IF (status(MPI_SOURCE) .EQ. 0) THEN CALL MPI_RECV(i, 1, MPI_INTEGER, 0, 0, comm, status, ierr) ELSE CALL MPI_RECV(x, 1, MPI_REAL, 1, 0, comm, status, ierr) END IF
END DO END IF

38
Each message is received with the right type.
39

40 Example 3.18 A similar program to the previous example, but now it has a problem.
41

42

CALL MPI_COMM_RANK(comm, rank, ierr)

43

IF (rank.EQ.0) THEN

44

CALL MPI_SEND(i, 1, MPI_INTEGER, 2, 0, comm, ierr)

45

ELSE IF (rank.EQ.1) THEN

46

CALL MPI_SEND(x, 1, MPI_REAL, 2, 0, comm, ierr)

47

ELSE IF (rank.EQ.2) THEN

48

DO i=1, 2

3.8. PROBE AND CANCEL

67

CALL MPI_PROBE(MPI_ANY_SOURCE, 0,

1

comm, status, ierr)

2

IF (status(MPI_SOURCE) .EQ. 0) THEN

3

100

CALL MPI_RECV(i, 1, MPI_INTEGER, MPI_ANY_SOURCE,

4

0, comm, status, ierr)

5

ELSE

6

200

CALL MPI_RECV(x, 1, MPI_REAL, MPI_ANY_SOURCE,

7

0, comm, status, ierr)

8

END IF

9

END DO

10

END IF

11

12

In Example 3.18, the two receive calls in statements labeled 100 and 200 in Example 3.17 13

slightly modiﬁed, using MPI_ANY_SOURCE as the source argument. The program is now 14

incorrect: the receive operation may receive a message that is distinct from the message 15

probed by the preceding call to MPI_PROBE.

16

17

Advice to users. In a multithreaded MPI program, MPI_PROBE and

18

MPI_IPROBE might need special care. If a thread probes for a message and then 19

immediately posts a matching receive, the receive may match a message other than 20

that found by the probe since another thread could concurrently receive that original 21

message [29]. MPI_MPROBE and MPI_IMPROBE solve this problem by matching the 22

incoming message so that it may only be received with MPI_MRECV or MPI_IMRECV 23

on the corresponding message handle. (End of advice to users.)

24

25

26
Advice to implementors. A call to MPI_PROBE(source, tag, comm, status) will match 27
the message that would have been received by a call to MPI_RECV(..., source, tag, 28
comm, status) executed at the same point. Suppose that this message has source s, 29
tag t and communicator c. If the tag argument in the probe call has value 30
MPI_ANY_TAG then the message probed will be the earliest pending message from 31 source s with communicator c and any tag; in any case, the message probed will be
32
the earliest pending message from source s with tag t and communicator c (this is the 33
message that would have been received, so as to preserve message order). This message 34
continues as the earliest pending message from source s with tag t and communicator 35
c, until it is received. A receive operation subsequent to the probe that uses the 36
same communicator as the probe and uses the tag and source values returned by 37
the probe, must receive this message, unless it has already been received by another 38
receive operation. (End of advice to implementors.) 39

40

41
3.8.2 Matching Probe 42

The function MPI_PROBE checks for incoming messages without receiving them. Since the 43

list of incoming messages is global among the threads of each MPI process, it can be hard 44

to use this functionality in threaded environments [29, 26].

45

Like MPI_PROBE and MPI_IPROBE, the MPI_MPROBE and MPI_IMPROBE opera- 46

tions allow incoming messages to be queried without actually receiving them, except that 47

MPI_MPROBE and MPI_IMPROBE provide a mechanism to receive the speciﬁc message 48

68

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 that was matched regardless of other intervening probe or receive operations. This gives 2 the application an opportunity to decide how to receive the message, based on the infor3 mation returned by the probe. In particular, the user may allocate memory for the receive 4 buﬀer, according to the length of the probed message.
5

6
7 MPI_IMPROBE(source, tag, comm, ﬂag, message, status)

8

IN

9
IN
10

11

IN

source tag comm

rank of source or MPI_ANY_SOURCE (integer) message tag or MPI_ANY_TAG (integer) communicator (handle)

12

OUT

ﬂag

13
OUT message
14

15

OUT status

ﬂag (logical) returned message (handle) status object (Status)

16

17 int MPI_Improbe(int source, int tag, MPI_Comm comm, int *flag,

18

MPI_Message *message, MPI_Status *status)

19
MPI_Improbe(source, tag, comm, flag, message, status, ierror) BIND(C)
20
INTEGER, INTENT(IN) :: source, tag
21
TYPE(MPI_Comm), INTENT(IN) :: comm
22
INTEGER, INTENT(OUT) :: flag
23
TYPE(MPI_Message), INTENT(OUT) :: message
24
TYPE(MPI_Status) :: status
25
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
26

27 MPI_IMPROBE(SOURCE, TAG, COMM, FLAG, MESSAGE, STATUS, IERROR)

28

INTEGER SOURCE, TAG, COMM, FLAG, MESSAGE, STATUS(MPI_STATUS_SIZE),

29

IERROR

30

31

MPI_IMPROBE(source, tag, comm, ﬂag, message, status) returns ﬂag = true if there is

a message that can be received and that matches the pattern speciﬁed by the arguments
32

source, tag, and comm. The call matches the same message that would have been received
33

34 by a call to MPI_RECV(..., source, tag, comm, status) executed at the same point in the

35 program and returns in status the same value that would have been returned by MPI_RECV.

In addition, it returns in message a handle to the matched message. Otherwise, the call
36

returns ﬂag = false, and leaves status and message undeﬁned.
37

38

A matched receive (MPI_MRECV or MPI_IMRECV) executed with the message han-

39 dle will receive the message that was matched by the probe. Unlike MPI_IPROBE, no

40 other probe or receive operation may match the message returned by MPI_IMPROBE.

41 Each message returned by MPI_IMPROBE must be received with either MPI_MRECV or

42 MPI_IMRECV.

43

The source argument of MPI_IMPROBE can be MPI_ANY_SOURCE, and the tag argu-

44 ment can be MPI_ANY_TAG, so that one can probe for messages from an arbitrary source

and/or with an arbitrary tag. However, a speciﬁc communication context must be provided
45

with the comm argument.
46

47

A synchronous send operation that is matched with MPI_IMPROBE or MPI_MPROBE

48 will complete successfully only if both a matching receive is posted with MPI_MRECV or

3.8. PROBE AND CANCEL

69

MPI_IMRECV, and the receive operation has started to receive the message sent by the 1

synchronous send.

2

There is a special predeﬁned message: MPI_MESSAGE_NO_PROC, which is a message 3

which has MPI_PROC_NULL as its source process. The predeﬁned constant

4

MPI_MESSAGE_NULL is the value used for invalid message handles.

5

A matching probe with MPI_PROC_NULL as source returns ﬂag = true, message = 6

MPI_MESSAGE_NO_PROC, and the status object returns source = MPI_PROC_NULL, tag 7

= MPI_ANY_TAG, and count = 0; see Section 3.11. It is not necessary to call MPI_MRECV 8

or MPI_IMRECV with MPI_MESSAGE_NO_PROC, but it is not erroneous to do so.

9

10

Rationale. MPI_MESSAGE_NO_PROC was chosen instead of

11

MPI_MESSAGE_PROC_NULL to avoid possible confusion as another null handle con- 12

stant. (End of rationale.)

13

14

15

16
MPI_MPROBE(source, tag, comm, message, status) 17

IN

source

rank of source or MPI_ANY_SOURCE (integer)

18

IN

tag

IN

comm

OUT message

message tag or MPI_ANY_TAG (integer)

19

20

communicator (handle) 21

returned message (handle)

22

OUT status

status object (Status)

23

24

25
int MPI_Mprobe(int source, int tag, MPI_Comm comm, MPI_Message *message, 26 MPI_Status *status) 27

MPI_Mprobe(source, tag, comm, message, status, ierror) BIND(C)

28

INTEGER, INTENT(IN) :: source, tag

29

TYPE(MPI_Comm), INTENT(IN) :: comm

30

TYPE(MPI_Message), INTENT(OUT) :: message

31

TYPE(MPI_Status) :: status

32

INTEGER, OPTIONAL, INTENT(OUT) :: ierror

33

34
MPI_MPROBE(SOURCE, TAG, COMM, MESSAGE, STATUS, IERROR) 35 INTEGER SOURCE, TAG, COMM, MESSAGE, STATUS(MPI_STATUS_SIZE), IERROR 36

MPI_MPROBE behaves like MPI_IMPROBE except that it is a blocking call that returns 37

only after a matching message has been found.

38

The implementation of MPI_MPROBE and MPI_IMPROBE needs to guarantee progress 39

in the same way as in the case of MPI_PROBE and MPI_IPROBE.

40

41

3.8.3 Matched Receives

42

43

The functions MPI_MRECV and MPI_IMRECV receive messages that have been previously 44

matched by a matching probe (Section 3.8.2).

45

46

47

48

70

CHAPTER 3. POINT-TO-POINT COMMUNICATION

1 MPI_MRECV(buf, count, datatype, message, status)

2

OUT

buf

3

initial address of receive buﬀer (choice)

4

IN

count

number of elements in receive buﬀer (non-negative in-

5

teger)

6

IN

datatype

7
INOUT message
8

9

OUT status

datatype of each receive buﬀer element (handle) message (handle) status object (Status)

10

11 int MPI_Mrecv(void* buf, int count, MPI_Datatype datatype,

12

MPI_Message *message, MPI_Status *status)

13
MPI_Mrecv(buf, count, datatype, message, status, ierror) BIND(C)
14
TYPE(*), DIMENSION(..) :: buf
15
INTEGER, INTENT(IN) :: count
16
TYPE(MPI_Datatype), INTENT(IN) :: datatype
17
TYPE(MPI_Message), INTENT(INOUT) :: message
18
TYPE(MPI_Status) :: status
19
INTEGER, OPTIONAL, INTENT(OUT) :: ierror
20

21 MPI_MRECV(BUF, COUNT, DATATYPE, MESSAGE, STATUS, IERROR)

22

<type> BUF(*)

23

INTEGER COUNT, DATATYPE, MESSAGE, STATUS(MPI_STATUS_SIZE), IERROR

24

This call receives a message matched by a matching probe operation (Section 3.8.2).
25

The receive buﬀer consists of the storage containing count consecutive elements of the
26

type speciﬁed by datatype, starting at address buf. The length of the received message must
27

be less than or equal to the length of the receive buﬀer. An overﬂow error occurs if all
28

incoming data does not ﬁt, without truncation, into the receive buﬀer.
29

If the message is shorter than the receive buﬀer, then only those locations corresponding
30

to the (shorter) message are modiﬁed.
31

32

On return from this function, the message handle is set to MPI_MESSAGE_NULL. All

errors that occur during the execution of this operation are handled according to the error
33

handler set for the communicator used in the matching probe call that produced the message
34

handle.
35

36

If MPI_MRECV is called with MPI_MESSAGE_NO_PROC as the message argument, the

37 call returns immediately with the status object set to source = MPI_PROC_NULL, tag =

38 MPI_ANY_TAG, and count = 0, as if a receive from MPI_PROC_NULL was issued (see Sec-

39 tion 3.11). A call to MPI_MRECV with MPI_MESSAGE_NULL is erroneous.

40

41

42

43

44

45

46

47

48

