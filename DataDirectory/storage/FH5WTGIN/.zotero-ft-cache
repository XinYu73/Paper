Statistical Mechanics
Third Edition

Statistical Mechanics
Third Edition
R. K. Pathria
Department of Physics University of California at San Diego
Paul D. Beale
Department of Physics University of Colorado at Boulder
AMSTERDAM • BOSTON • HEIDELBERG • LONDON NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO Butterworth-Heinemann is an imprint of Elsevier

Butterworth-Heinemann is an imprint of Elsevier The Boulevard, Langford Lane, Kidlington, Oxford, 0X5 1GB, UK 30 Corporate Drive, Suite 400, Burlington, MA 01803, USA
First published 1972; Second edition 1996

© 2011 Elsevier Ltd. All rights reserved

The right of R. K. Pathria to be identiﬁed as the author of this work has been asserted in accordance with the Copyright, Designs and Patents Act 1988.

No part of this publication may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or any information storage and retrieval system, without permission in writing from the publisher. Details on how to seek permission, further information about the Publisher’s permissions policies and our arrangements with organizations such as the Copyright Clearance Center and the Copyright Licensing Agency, can be found at our website: www.elsevier.com/permissions.
This book and the individual contributions contained in it are protected under copyright by the Publisher (other than as may be noted herein).

Notices Knowledge and best practice in this ﬁeld are constantly changing. As new research and experience broaden our understanding, changes in research methods, professional practices, or medical treatment may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating and using any information, methods, compounds, or experiments described herein. In using such information or methods they should be mindful of their own safety and the safety of others, including parties for whom they have a professional responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability for any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or from any use or operation of any methods, products, instructions, or ideas contained in the material herein.

British Library Cataloguing in Publication Data A catalogue of this book is available from the British Library.

Library of Congress Cataloging in Publication Data

Pathria, R. K.

Statistical mechanics–3rd ed. / R. K. Pathria, Paul D. Beale.

p. cm.

Includes bibliographical references and index.

ISBN 978-0-12-382188-1 (pbk.)

1. Statistical mechanics. I. Beale, Paul D. II. Title.

QC174.8.P38 2011

530.13–dc22

2010048955

Cover: The image was created using the opensource software CMBview (http://www.jportsmouth.com/code/CMBview/ cmbview.html) written by Jamie Portsmouth and used with permission. It was created using the WMAP seven-year Internal Linear Combination Map courtesy of the WMAP Science Team (http://lambda.gsfc.nasa.gov/product/map/dr4/ilc map get.cfm).

For information on all Butterworth-Heinemann publications, visit our Web site at www.elsevierdirect.com
Printed in the United States 11 12 13 14 15 10 9 8 7 6 5 4 3 2 1

Contents

Preface to the Third Edition

xiii

Preface to the Second Edition

xvii

Preface to the First Edition

xix

Historical Introduction

xxi

1. The Statistical Basis of Thermodynamics

1

1.1. The macroscopic and the microscopic states

1

1.2. Contact between statistics and thermodynamics:

physical signiﬁcance of the number (N, V , E)

3

1.3. Further contact between statistics and thermodynamics

6

1.4. The classical ideal gas

9

1.5. The entropy of mixing and the Gibbs paradox

16

1.6. The “correct” enumeration of the microstates

20

Problems

22

2. Elements of Ensemble Theory

25

2.1. Phase space of a classical system

25

2.2. Liouville’s theorem and its consequences

27

2.3. The microcanonical ensemble

30

2.4. Examples

32

2.5. Quantum states and the phase space

35

Problems

37

Traveling Wave Analysis of Partial Differential Equations

v

© 2011 Elsevier Ltd. All rights reserved.

vi Contents

3. The Canonical Ensemble

39

3.1. Equilibrium between a system and a heat reservoir

40

3.2. A system in the canonical ensemble

41

3.3. Physical signiﬁcance of the various statistical quantities

in the canonical ensemble

50

3.4. Alternative expressions for the partition function

52

3.5. The classical systems

54

3.6. Energy ﬂuctuations in the canonical ensemble:

correspondence with the microcanonical ensemble

58

3.7. Two theorems — the “equipartition” and the “virial”

61

3.8. A system of harmonic oscillators

65

3.9. The statistics of paramagnetism

70

3.10. Thermodynamics of magnetic systems:

negative temperatures

77

Problems

83

4. The Grand Canonical Ensemble

91

4.1. Equilibrium between a system and a particle-energy

reservoir

91

4.2. A system in the grand canonical ensemble

93

4.3. Physical signiﬁcance of the various statistical quantities 95

4.4. Examples

98

4.5. Density and energy ﬂuctuations in the grand canonical

ensemble: correspondence with other ensembles

103

4.6. Thermodynamic phase diagrams

105

4.7. Phase equilibrium and the Clausius–Clapeyron equation 109

Problems

111

5. Formulation of Quantum Statistics

115

5.1. Quantum-mechanical ensemble theory:

the density matrix

115

5.2. Statistics of the various ensembles

119

Contents vii

5.3. Examples

122

5.4. Systems composed of indistinguishable particles

128

5.5. The density matrix and the partition function of a

system of free particles

133

Problems

139

6. The Theory of Simple Gases

141

6.1. An ideal gas in a quantum-mechanical

microcanonical ensemble

141

6.2. An ideal gas in other quantum-mechanical ensembles

146

6.3. Statistics of the occupation numbers

149

6.4. Kinetic considerations

152

6.5. Gaseous systems composed of molecules with

internal motion

155

6.6. Chemical equilibrium

170

Problems

173

7. Ideal Bose Systems

179

7.1. Thermodynamic behavior of an ideal Bose gas

180

7.2. Bose–Einstein condensation in ultracold atomic gases

191

7.3. Thermodynamics of the blackbody radiation

200

7.4. The ﬁeld of sound waves

205

7.5. Inertial density of the sound ﬁeld

212

7.6. Elementary excitations in liquid helium II

215

Problems

223

8. Ideal Fermi Systems

231

8.1. Thermodynamic behavior of an ideal Fermi gas

231

8.2. Magnetic behavior of an ideal Fermi gas

238

8.3. The electron gas in metals

247

8.4. Ultracold atomic Fermi gases

258

viii Contents

8.5. Statistical equilibrium of white dwarf stars

259

8.6. Statistical model of the atom

264

Problems

269

9. Thermodynamics of the Early Universe

275

9.1. Observational evidence of the Big Bang

275

9.2. Evolution of the temperature of the universe

280

9.3. Relativistic electrons, positrons, and neutrinos

282

9.4. Neutron fraction

285

9.5. Annihilation of the positrons and electrons

287

9.6. Neutrino temperature

289

9.7. Primordial nucleosynthesis

290

9.8. Recombination

293

9.9. Epilogue

295

Problems

296

10. Statistical Mechanics of Interacting Systems:

The Method of Cluster Expansions

299

10.1. Cluster expansion for a classical gas

299

10.2. Virial expansion of the equation of state

307

10.3. Evaluation of the virial coefﬁcients

309

10.4. General remarks on cluster expansions

315

10.5. Exact treatment of the second virial coefﬁcient

320

10.6. Cluster expansion for a quantum-mechanical system

325

10.7. Correlations and scattering

331

Problems

340

11. Statistical Mechanics of Interacting Systems:

The Method of Quantized Fields

345

11.1. The formalism of second quantization

345

11.2. Low-temperature behavior of an imperfect Bose gas

355

Contents ix

11.3. Low-lying states of an imperfect Bose gas

361

11.4. Energy spectrum of a Bose liquid

366

11.5. States with quantized circulation

370

11.6. Quantized vortex rings and the breakdown

of superﬂuidity

376

11.7. Low-lying states of an imperfect Fermi gas

379

11.8. Energy spectrum of a Fermi liquid: Landau’s

phenomenological theory

385

11.9. Condensation in Fermi systems

392

Problems

394

12. Phase Transitions: Criticality, Universality, and Scaling

401

12.1. General remarks on the problem of condensation

402

12.2. Condensation of a van der Waals gas

407

12.3. A dynamical model of phase transitions

411

12.4. The lattice gas and the binary alloy

417

12.5. Ising model in the zeroth approximation

420

12.6. Ising model in the ﬁrst approximation

427

12.7. The critical exponents

435

12.8. Thermodynamic inequalities

438

12.9. Landau’s phenomenological theory

442

12.10. Scaling hypothesis for thermodynamic functions

446

12.11. The role of correlations and ﬂuctuations

449

12.12. The critical exponents ν and η

456

12.13. A ﬁnal look at the mean ﬁeld theory

460

Problems

463

13. Phase Transitions: Exact (or Almost Exact) Results

for Various Models

471

13.1. One-dimensional ﬂuid models

471

13.2. The Ising model in one dimension

476

x Contents

13.3. The n-vector models in one dimension

482

13.4. The Ising model in two dimensions

488

13.5. The spherical model in arbitrary dimensions

508

13.6. The ideal Bose gas in arbitrary dimensions

519

13.7. Other models

526

Problems

530

14. Phase Transitions: The Renormalization Group Approach 539

14.1. The conceptual basis of scaling

540

14.2. Some simple examples of renormalization

543

14.3. The renormalization group: general formulation

552

14.4. Applications of the renormalization group

559

14.5. Finite-size scaling

570

Problems

579

15. Fluctuations and Nonequilibrium Statistical Mechanics

583

15.1. Equilibrium thermodynamic ﬂuctuations

584

15.2. The Einstein–Smoluchowski theory of the

Brownian motion

587

15.3. The Langevin theory of the Brownian motion

593

15.4. Approach to equilibrium: the Fokker–Planck equation

603

15.5. Spectral analysis of ﬂuctuations: the

Wiener–Khintchine theorem

609

15.6. The ﬂuctuation–dissipation theorem

617

15.7. The Onsager relations

626

Problems

632

16. Computer Simulations

637

16.1. Introduction and statistics

637

16.2. Monte Carlo simulations

640

16.3. Molecular dynamics

643

16.4. Particle simulations

646

Contents xi

16.5. Computer simulation caveats

650

Problems

651

Appendices

653

A. Inﬂuence of boundary conditions on the

distribution of quantum states

653

B. Certain mathematical functions

655

C. “Volume” and “surface area” of an n-dimensional

sphere of radius R

662

D. On Bose–Einstein functions

664

E. On Fermi–Dirac functions

667

F. A rigorous analysis of the ideal Bose gas and the

onset of Bose–Einstein condensation

670

G. On Watson functions

675

H. Thermodynamic relationships

676

I. Pseudorandom numbers

683

Bibliography

687

Index

707

Preface to the Third Edition

The second edition of Statistical Mechanics was published in 1996. The new material added at that time focused on phase transitions, critical phenomena, and the renormalization group — topics that had undergone vast transformations during the years following the publication of the ﬁrst edition in 1972. In 2009, R. K. Pathria (R.K.P.) and the publishers agreed it was time for a third edition to incorporate the important changes that had occurred in the ﬁeld since the publication of the second edition and invited Paul B. Beale (P.D.B.) to join as coauthor. The two authors agreed on the scope of the additions and changes and P.D.B. wrote the ﬁrst draft of the new sections except for Appendix F which was written by R.K.P. Both authors worked very closely together editing the drafts and ﬁnalizing this third edition.
. The new topics added to this edition are: Bose–Einstein condensation and degenerate Fermi gas behavior in ultracold atomic gases: Sections 7.2, 8.4, 11.2.A, and 11.9. The creation of Bose–Einstein condensates in ultracold gases during the 1990s and in degenerate Fermi gases during the 2000s led to a revolution in atomic, molecular, and optical physics, and provided a valuable link to the quantum behavior of condensed matter systems. Several of P.D.B.’s friends and colleagues in physics
. and JILA at the University of Colorado have been leaders in this exciting new ﬁeld. Finite-size scaling behavior of Bose–Einstein condensates: Appendix F. We develop an analytical theory for the behavior of Bose–Einstein condensates in a ﬁnite system, which provides a rigorous justiﬁcation for singling out the ground state in the calculation of the
. properties of the Bose–Einstein condensate. Thermodynamics of the early universe: Chapter 9. The sequence of thermodynamic transitions that the universe went though shortly after the Big Bang left behind mileposts that astrophysicists have exploited to look back into the universe’s earliest moments. Major advances in astronomy over the past 20 years have provided a vast body of observational data about the early evolution of the universe. These include the Hubble Space Telescope’s deep space measurements of the expansion of the universe, the Cosmic Background Explorer’s precise measurements of the temperature of the cosmic microwave background, and the Wilkinson Microwave Anisotropy Probe’s mapping of the angular variations in the cosmic microwave background. These data sets have led to precise determinations of the age of the universe, its composition and early evolution. Coincidentally, P.D.B.’s faculty ofﬁce is located in the tower named after George Gamow, a member of the faculty at the University of Colorado in the 1950s and 1960s and a leader in the theory of nucleosynthesis
. in the early universe. Chemical equilibrium: Section 6.6. Chemical potentials determine the conditions necessary for chemical equilibrium. This is an important topic in its own right, but also plays a critical role in our discussion of the thermodynamics of the early universe in Chapter 9.

Traveling Wave Analysis of Partial Differential Equations © 2011 Elsevier Ltd. All rights reserved.

xiii

xiv Preface to the Third Edition
. Monte Carlo and molecular dynamics simulations: Chapter 16. Computer simulations have
become an important tool in modern statistical mechanics. We provide here a brief
. introduction to Monte Carlo and molecular dynamics techniques and algorithms. Correlation functions and scattering: Section 10.7. Correlation functions are central to the understanding of thermodynamic phases, phase transitions, and critical phenomena. The differences between thermodynamic phases are often most conspicuous in the behavior of correlation functions and the closely related static structure factors. We have collected
. discussions from the second edition into one place and added new material. Fluctuation–dissipation theorem and the dynamical structure factor: Sections 15.3.A., 15.6.A, and 15.6.B. The ﬂuctuation–dissipation theorem describes the relation between natural equilibrium thermodynamic ﬂuctuations in a system and the response of the system to small disturbances from equilibrium, and it is one of the cornerstones of nonequilibrium statistical mechanics. We have expanded the discussion of the ﬂuctuation–dissipation theorem to include a derivation of the key results from linear response theory, a discussion of the dynamical structure factor, and analysis of the
. Brownian motion of harmonic oscillators that provides useful practical examples. Phase equilibrium and the Clausius–Clapeyron equation: Sections 4.6 and 4.7. Much of the text is devoted to using statistical mechanics methods to determine the properties of thermodynamic phases and phase transitions. This brief overview of phase equilibrium
. and the structure of phase diagrams lays the groundwork for later discussions. Exact solutions of one-dimensional ﬂuid models: Section 13.1. One-dimensional ﬂuid models with short-range interactions do not exhibit phase transitions but they do display
. short-range correlations and other behaviors typical of dense ﬂuids. Exact solution of the two-dimensional Ising model on a ﬁnite lattice: Section 13.4.A. This solution entails an exact counting of the microstates of the microcanonical ensemble and provides analytical results for the energy distribution, internal energy, and heat capacity of the system. This solution also describes the ﬁnite-size scaling behavior of the Ising model near the transition point and provides an exact framework that can be
. used to test Monte Carlo methods. Summary of thermodynamic assemblies and associated statistical ensembles: Appendix H. We provide a summary of thermodynamic relations and their connections to statistical mechanical ensembles. Most of this information can be found elsewhere in the text, but we thought it would be helpful to provide a rundown of these important connections in one
. place. Pseudorandom number generators: Appendix I. Pseudorandom number generators are indispensable in computer simulations. We provide simple algorithms for generating
. uniform and Gaussian pseudorandom numbers and discuss their properties. Dozens of new homework problems.
The remainder of the text is largely unchanged. The completion of this task has left us indebted to many a friend and colleague. R.K.P. has
already expressed his indebtedness to a good number of people on two previous occasions — in 1972 and in 1996 — so, at this time, he will simply reiterate the many words of gratitude he has already written. In addition though, he would like to thank Paul Beale for his willingness to be a partner in this project and for his diligence in carrying out the task at hand both arduously and meticulously.
On his part, P.D.B. would like to thank his friends at the University of Colorado at Boulder for the many conversations he has had with them over the years about research and pedagogy of statistical mechanics, especially Noel Clark, Tom DeGrand, John Price, Chuck Rogers, Mike

Preface to the Third Edition xv
Dubson, and Leo Radzihovsky. He would also like to thank the faculty of the Department of Physics for according him the honor of serving as the chair of this outstanding department.
Special thanks are also due to many friends and colleagues who have read sections of the manuscript and have offered many valuable suggestions and corrections, especially Tom DeGrand, Michael Shull, David Nesbitt, Jamie Nagle, Matt Glaser, Murray Holland, Leo Radzihovsky, Victor Gurarie, Edmond Meyer, Matthew Grau, Andrew Sisler, Michael Foss-Feig, Allan Franklin, Shantha deAlwis, Dmitri Reznik, and Eric Cornell.
P.D.B. would like to take this opportunity to extend his thanks and best wishes to Professor Michael E. Fisher whose graduate statistical mechanics course at Cornell introduced him to this elegant ﬁeld. He would also like to express his gratitude to Raj Pathria for inviting him to be part of this project, and for the fun and engaging discussions they have had during the preparation of this new edition. Raj’s thoughtful counsel always proved to be valuable in improving the text.
P.D.B.’s greatest thanks go to Matthew, Melanie, and Erika for their love and support.
R.K.P. P.D.B.

Preface to the Second Edition
The ﬁrst edition of this book was prepared over the years 1966 to 1970 when the subject of phase transitions was undergoing a complete overhaul. The concepts of scaling and universality had just taken root but the renormalization group approach, which converted these concepts into a calculational tool, was still obscure. Not surprisingly, my text of that time could not do justice to these emerging developments. Over the intervening years I have felt increasingly conscious of this rather serious deﬁciency in the text; so when the time came to prepare a new edition, my major effort went toward correcting that deﬁciency.
Despite the aforementioned shortcoming, the ﬁrst edition of this book has continued to be popular over the last 20 years or so. I, therefore, decided not to tinker with it unnecessarily. Nevertheless, to make room for the new material, I had to remove some sections from the present text which, I felt, were not being used by the readers as much as the rest of the book was. This may turn out to be a disappointment to some individuals but I trust they will understand the logic behind it and, if need be, will go back to a copy of the ﬁrst edition for reference. I, on my part, hope that a good majority of the users will not be inconvenienced by these deletions. As for the material retained, I have conﬁned myself to making only editorial changes. The subject of phase transitions and critical phenomena, which has been my main focus of revision, has been treated in three new chapters that provide a respectable coverage of the subject and essentially bring the book up to date. These chapters, along with a collection of more than 60 homework problems, will, I believe, enhance the usefulness of the book for both students and instructors.
The completion of this task has left me indebted to many. First of all, as mentioned in the Preface to the ﬁrst edition, I owe a considerable debt to those who have written on this subject before and from whose writings I have beneﬁted greatly. It is difﬁcult to thank them all individually; the bibliography at the end of the book is an obvious tribute to them. As for deﬁnitive help, I am most grateful to Dr Surjit Singh who advised me expertly and assisted me generously in the selection of the material that comprises Chapters 11 to 13 of the new text; without his help, the ﬁnal product might not have been as coherent as it now appears to be. On the technical side, I am very thankful to Mrs. Debbie Guenther who typed the manuscript with exceptional skill and proof read it with extreme care; her task was clearly an arduous one but she performed it with good cheer — for which I admire her greatly.
Finally, I wish to express my heartfelt appreciation for my wife who let me devote myself fully to this task over a rather long period of time and waited for its completion ungrudgingly.
R.K.P.

Traveling Wave Analysis of Partial Differential Equations © 2011 Elsevier Ltd. All rights reserved.

xvii

Preface to the First Edition

This book has arisen out of the notes of lectures that I gave to the graduate students at the McMaster University (1964–1965), the University of Alberta (1965–1967), the University of Waterloo (1969–1971), and the University of Windsor (1970–1971). While the subject matter, in its ﬁner details, has changed considerably during the preparation of the manuscript, the style of presentation remains the same as followed in these lectures.
Statistical mechanics is an indispensable tool for studying physical properties of matter “in bulk” on the basis of the dynamical behavior of its “microscopic” constituents. Founded on the well-laid principles of mathematical statistics on one hand and Hamiltonian mechanics on the other, the formalism of statistical mechanics has proved to be of immense value to the physics of the last 100 years. In view of the universality of its appeal, a basic knowledge of this subject is considered essential for every student of physics, irrespective of the area(s) in which he/she may be planning to specialize. To provide this knowledge, in a manner that brings out the essence of the subject with due rigor but without undue pain, is the main purpose of this work.
The fact that the dynamics of a physical system is represented by a set of quantum states and the assertion that the thermodynamics of the system is determined by the multiplicity of these states constitute the basis of our treatment. The fundamental connection between the microscopic and the macroscopic descriptions of a system is uncovered by investigating the conditions for equilibrium between two physical systems in thermodynamic contact. This is best accomplished by working in the spirit of the quantum theory right from the beginning; the entropy and other thermodynamic variables of the system then follow in a most natural manner. After the formalism is developed, one may (if the situation permits) go over to the limit of the classical statistics. This message may not be new, but here I have tried to follow it as far as is reasonably possible in a textbook. In doing so, an attempt has been made to keep the level of presentation fairly uniform so that the reader does not encounter ﬂuctuations of too wild a character.
This text is conﬁned to the study of the equilibrium states of physical systems and is intended to be used for a graduate course in statistical mechanics. Within these bounds, the coverage is fairly wide and provides enough material for tailoring a good two-semester course. The ﬁnal choice always rests with the individual instructor; I, for one, regard Chapters 1 to 9 (minus a few sections from these chapters plus a few sections from Chapter 13) as the “essential part” of such a course. The contents of Chapters 10 to 12 are relatively advanced (not necessarily difﬁcult); the choice of material out of these chapters will depend entirely on the taste of the instructor. To facilitate the understanding of the subject, the text has been illustrated with a large number of graphs; to assess the understanding, a large number of problems have been included. I hope these features are found useful.

Traveling Wave Analysis of Partial Differential Equations © 2011 Elsevier Ltd. All rights reserved.

xix

xx Preface to the First Edition
I feel that one of the most essential aspects of teaching is to arouse the curiosity of the students in their subject, and one of the most effective ways of doing this is to discuss with them (in a reasonable measure, of course) the circumstances that led to the emergence of the subject. One would, therefore, like to stop occasionally to reﬂect upon the manner in which the various developments really came about; at the same time, one may not like the ﬂow of the text to be hampered by the discontinuities arising from an intermittent addition of historical material. Accordingly, I decided to include in this account a historical introduction to the subject which stands separate from the main text. I trust the readers, especially the instructors, will ﬁnd it of interest.
For those who wish to continue their study of statistical mechanics beyond the conﬁnes of this book, a fairly extensive bibliography is included. It contains a variety of references — old as well as new, experimental as well as theoretical, technical as well as pedagogical. I hope that this will make the book useful for a wider readership.
The completion of this task has left me indebted to many. Like most authors, I owe considerable debt to those who have written on the subject before. The bibliography at the end of the book is the most obvious tribute to them; nevertheless, I would like to mention, in particular, the works of the Ehrenfests, Fowler, Guggenheim, Schro¨dinger, Rushbrooke, ter Haar, Hill, Landau and Lifshitz, Huang, and Kubo, which have been my constant reference for several years and have inﬂuenced my understanding of the subject in a variety of ways. As for the preparation of the text, I am indebted to Robert Teshima who drew most of the graphs and checked most of the problems, to Ravindar Bansal, Vishwa Mittar, and Surjit Singh who went through the entire manuscript and made several suggestions that helped me unkink the exposition at a number of points, to Mary Annetts who typed the manuscript with exceptional patience, diligence and care, and to Fred Hetzel, Jim Briante, and Larry Kry who provided technical help during the preparation of the ﬁnal version.
As this work progressed I felt increasingly gratiﬁed toward Professors F. C. Auluck and D. S. Kothari of the University of Delhi with whom I started my career and who initiated me into the study of this subject, and toward Professor R. C. Majumdar who took keen interest in my work on this and every other project that I have undertaken from time to time. I am grateful to Dr. D. ter Haar of the University of Oxford who, as the general editor of this series, gave valuable advice on various aspects of the preparation of the manuscript and made several useful suggestions toward the improvement of the text. I am thankful to Professors J. W. Leech, J. Grindlay, and A. D. Singh Nagi of the University of Waterloo for their interest and hospitality that went a long way in making this task a pleasant one.
The ﬁnal tribute must go to my wife whose cooperation and understanding, at all stages of this project and against all odds, have been simply overwhelming.
R.K.P.

Historical Introduction

Statistical mechanics is a formalism that aims at explaining the physical properties of matter in bulk on the basis of the dynamical behavior of its microscopic constituents. The scope of the formalism is almost as unlimited as the very range of the natural phenomena, for in principle it is applicable to matter in any state whatsoever. It has, in fact, been applied, with considerable success, to the study of matter in the solid state, the liquid state, or the gaseous state, matter composed of several phases and/or several components, matter under extreme conditions of density and temperature, matter in equilibrium with radiation (as, for example, in astrophysics), matter in the form of a biological specimen, and so on. Furthermore, the formalism of statistical mechanics enables us to investigate the nonequilibrium states of matter as well as the equilibrium states; indeed, these investigations help us to understand the manner in which a physical system that happens to be “out of equilibrium” at a given time t approaches a “state of equilibrium” as time passes.
In contrast with the present status of its development, the success of its applications, and the breadth of its scope, the beginnings of statistical mechanics were rather modest. Barring certain primitive references, such as those of Gassendi, Hooke, and so on, the real work on this subject started with the contemplations of Bernoulli (1738), Herapath (1821), and Joule (1851) who, in their own individual ways, attempted to lay a foundation for the so-called kinetic theory of gases — a discipline that ﬁnally turned out to be the forerunner of statistical mechanics. The pioneering work of these investigators established the fact that the pressure of a gas arose from the motion of its molecules and could, therefore, be computed by considering the dynamical inﬂuence of the molecular bombardment on the walls of the container. Thus, Bernoulli and Herapath could show that, if temperature remained constant, the pressure P of an ordinary gas was inversely proportional to the volume V of the container (Boyle’s law), and that it was essentially independent of the shape of the container. This, of course, involved the explicit assumption that, at a given temperature T , the (mean) speed of the molecules was independent of both pressure and volume. Bernoulli even attempted to determine the (ﬁrst-order) correction to this law, arising from the ﬁnite size of the molecules, and showed that the volume V appearing in the statement of the law should be replaced by (V − b), where b is the “actual” volume of the molecules.1
Joule was the ﬁrst to show that the pressure P was directly proportional to the square of the molecular speed c, which he had initially assumed to be the same for all molecules. Kro¨nig (1856) went a step further. Introducing the “quasistatistical” assumption that, at any time t,
1As is well known, this “correction” was correctly evaluated, much later, by van der Waals (1873) who showed that,
for large V , b is four times the “actual” volume of the molecules; see Problem 1.4.

Traveling Wave Analysis of Partial Differential Equations © 2011 Elsevier Ltd. All rights reserved.

xxi

xxii Historical Introduction

one-sixth of the molecules could be assumed to be ﬂying in each of the six “independent” directions, namely +x, −x, +y, −y, +z, and −z, he derived the equation

P

=

1 n mc2, 3

(1)

where n is the number density of the molecules and m the molecular mass. Kro¨nig, too, assumed the molecular speed c to be the same for all molecules; so from (1), he inferred that the kinetic energy of the molecules should be directly proportional to the absolute temperature of the gas.
Kro¨nig justiﬁed his method in these words: “The path of each molecule must be so irregular that it will defy all attempts at calculation. However, according to the laws of probability, one could assume a completely regular motion in place of a completely irregular one!” It must, however, be noted that it is only because of the special form of the summations appearing in the calculation of the pressure that Kro¨nig’s argument leads to the same result as the one following from more reﬁned models. In other problems, such as the ones involving diffusion, viscosity, or heat conduction, this is no longer the case.
It was at this stage that Clausius entered the ﬁeld. First of all, in 1857, he derived the ideal-gas law under assumptions far less stringent than Kro¨nig’s. He discarded both leading assumptions of Kro¨nig and showed that equation (1) was still true; of course, c2 now became the mean square speed of the molecules. In a later paper (1859), Clausius introduced the concept of the mean free path and thus became the ﬁrst to analyze transport phenomena. It was in these studies that he introduced the famous “Stosszahlansatz” — the hypothesis on the number of collisions (among the molecules) — which, later on, played a prominent role in the monumental work of Boltzmann.2 With Clausius, the introduction of the microscopic and statistical points of view into the physical theory was deﬁnitive, rather than speculative. Accordingly, Maxwell, in a popular article entitled “Molecules,” written for the Encyclopedia Britannica, referred to Clausius as the “principal founder of the kinetic theory of gases,” while Gibbs, in his Clausius obituary notice, called him the “father of statistical mechanics.”3
The work of Clausius attracted Maxwell to the ﬁeld. He made his ﬁrst appearance with the memoir “Illustrations in the dynamical theory of gases” (1860), in which he went much farther than his predecessors by deriving his famous law of the “distribution of molecular speeds.” Maxwell’s derivation was based on elementary principles of probability and was clearly inspired by the Gaussian law of “distribution of random errors.” A derivation based on the requirement that “the equilibrium distribution of molecular speeds, once acquired, should remain invariant under molecular collisions” appeared in 1867. This led Maxwell to establish what is known as Maxwell’s transport equation which, if skilfully used, leads to the same results as one gets from the more fundamental equation due to Boltzmann.4
Maxwell’s contributions to the subject diminished considerably after his appointment, in 1871, as the Cavendish Professor at Cambridge. By that time Boltzmann had already made his ﬁrst strides. In the period 1868–1871 he generalized Maxwell’s distribution law to polyatomic gases, also taking into account the presence of external forces, if any; this gave rise to the famous Boltzmann factor exp(−βε), where ε denotes the total energy of a molecule. These investigations also led to the equipartition theorem. Boltzmann further showed that, just

2For an excellent review of this and related topics, see Ehrenfest and Ehrenfest (1912). 3For further details, refer to Montroll (1963) where an account is also given of the pioneering work of Waterston (1846,
1892). 4This equivalence has been demonstrated in Guggenheim (1960) where the coefﬁcients of viscosity, thermal
conductivity, and diffusion of a gas of hard spheres have been calculated on the basis of Maxwell’s transport equation.

Historical Introduction xxiii
like the original distribution of Maxwell, the generalized distribution (which we now call the Maxwell–Boltzmann distribution) is stationary with respect to molecular collisions.
In 1872 came the celebrated H-theorem, which provided a molecular basis for the natural tendency of physical systems to approach, and stay in, a state of equilibrium. This established a connection between the microscopic approach (which characterizes statistical mechanics) and the phenomenological approach (which characterized thermodynamics) much more transparently than ever before; it also provided a direct method for computing the entropy of a given physical system from purely microscopic considerations. As a corollary to the Htheorem, Boltzmann showed that the Maxwell–Boltzmann distribution is the only distribution that stays invariant under molecular collisions and that any other distribution, under the inﬂuence of molecular collisions, will ultimately go over to a Maxwell–Boltzmann distribution. In 1876 Boltzmann derived his famous transport equation, which, in the hands of Chapman and Enskog (1916–1917), has proved to be an extremely powerful tool for investigating macroscopic properties of systems in nonequilibrium states.
Things, however, proved quite harsh for Boltzmann. His H-theorem, and the consequent irreversible behavior of physical systems, came under heavy attack, mainly from Loschmidt (1876–1877) and Zermelo (1896). While Loschmidt wondered how the consequences of this theorem could be reconciled with the reversible character of the basic equations of motion of the molecules, Zermelo wondered how these consequences could be made to ﬁt with the quasiperiodic behavior of closed systems (which arose in view of the so-called Poincare´ cycles). Boltzmann defended himself against these attacks with all his might but, unfortunately, could not convince his opponents of the correctness of his viewpoint. At the same time, the energeticists, led by Mach and Ostwald, were criticizing the very (molecular) basis of the kinetic theory,5 while Kelvin was emphasizing the “nineteenth-century clouds hovering over the dynamical theory of light and heat.”6
All this left Boltzmann in a state of despair and induced in him a persecution complex.7 He wrote in the introduction to the second volume of his treatise Vorlesungen u¨ ber Gastheorie (1898):8
I am convinced that the attacks (on the kinetic theory) rest on misunderstandings and that the role of the kinetic theory is not yet played out. In my opinion it would be a blow to science if contemporary opposition were to cause kinetic theory to sink into the oblivion which was the fate suffered by the wave theory of light through the authority of Newton. I am aware of the weakness of one individual against the prevailing currents of opinion. In order to insure that not too much will have to be rediscovered when people return to the study of kinetic theory I will present the most difﬁcult and misunderstood parts of the subject in as clear a manner as I can.
We shall not dwell any further on the kinetic theory; we would rather move on to the development of the more sophisticated approach known as the ensemble theory, which may in fact be regarded as the statistical mechanics proper.9 In this approach, the dynamical state of a
5These critics were silenced by Einstein whose work on the Brownian motion (1905b) established atomic theory once and for all.
6The ﬁrst of these clouds was concerned with the mysteries of the “aether,” and was dispelled by the theory of relativity. The second was concerned with the inadequacy of the “equipartition theorem,” and was dispelled by the quantum theory.
7Some people attribute Boltzmann’s suicide on September 5, 1906 to this cause. 8Quotation from Montroll (1963). 9For a review of the historical development of kinetic theory leading to statistical mechanics, see Brush (1957, 1958, 1961a,b, 1965–1966).

xxiv Historical Introduction
given system, as characterized by the generalized coordinates qi and the generalized momenta pi, is represented by a phase point G(qi, pi) in a phase space of appropriate dimensionality. The evolution of the dynamical state in time is depicted by the trajectory of the G-point in the phase space, the “geometry” of the trajectory being governed by the equations of motion of the system and by the nature of the physical constraints imposed on it. To develop an appropriate formalism, one considers the given system along with an inﬁnitely large number of “mental copies” thereof; that is, an ensemble of similar systems under identical physical constraints (though, at any time t, the various systems in the ensemble would differ widely in respect of their dynamical states). In the phase space, then, one has a swarm of inﬁnitely many G-points (which, at any time t, are widely dispersed and, with time, move along their respective trajectories). The ﬁction of a host of inﬁnitely many, identical but independent, systems allows one to replace certain dubious assumptions of the kinetic theory of gases by readily acceptable statements of statistical mechanics. The explicit formulation of these statements was ﬁrst given by Maxwell (1879) who on this occasion used the word “statistico-mechanical” to describe the study of ensembles (of gaseous systems) — though, eight years earlier, Boltzmann (1871) had already worked with essentially the same kind of ensembles.
The most important quantity in the ensemble theory is the density function, ρ(qi, pi; t), of the G-points in the phase space; a stationary distribution (∂ρ/∂t = 0) characterizes a stationary ensemble, which in tum represents a system in equilibrium. Maxwell and Boltzmann conﬁned their study to ensembles for which the function ρ depended solely on the energy E of the system. This included the special case of ergodic systems, which were so deﬁned that “the undisturbed motion of such a system, if pursued for an unlimited time, would ultimately traverse (the neighborhood of ) each and every phase point compatible with the ﬁxed value E of the energy.” Consequently, the ensemble average, f , of a physical quantity f , taken at any given
time t, would be the same as the long-time average, f , pertaining to any given member of the
ensemble. Now, f is the value we expect to obtain for the quantity in question when we make an appropriate measurement on the system; the result of this measurement should, therefore, agree with the theoretical estimate f . We thus acquire a recipe to bring about a direct contact between theory and experiment. At the same time, we lay down a rational basis for a microscopic theory of matter as an alternative to the empirical approach of thermodynamics!
A signiﬁcant advance in this direction was made by Gibbs who, with his Elementary Principles of Statistical Mechanics (1902), turned ensemble theory into a most efﬁcient tool for the theorist. He emphasized the use of “generalized” ensembles and developed schemes which, in principle, enabled one to compute a complete set of thermodynamic quantities of a given system from purely mechanical properties of its microscopic constituents.10 In its methods and results, the work of Gibbs turned out to be much more general than any preceding treatment of the subject; it applied to any physical system that met the simple-minded requirements that (i) it was mechanical in structure and (ii) it obeyed Lagrange’s and Hamilton’s equations of motion. In this respect, Gibbs’s work may be considered to have accomplished for thermodynamics as much as Maxwell’s had accomplished for electrodynamics.
These developments almost coincided with the great revolution that Planck’s work of 1900 brought into physics. As is well known, Planck’s quantum hypothesis successfully resolved the essential mysteries of the black-body radiation — a subject where the three best-established disciplines of the nineteenth century, namely mechanics, electrodynamics, and thermodynamics, were all focused. At the same time, it uncovered both the strengths and the weaknesses of these disciplines. It would have been surprising if statistical mechanics, which linked thermodynamics with mechanics, could have escaped the repercussions of this revolution.
10In much the same way as Gibbs, but quite independently of him, Einstein (1902, 1903) also developed the theory of ensembles.

Historical Introduction xxv
The subsequent work of Einstein (1905a) on the photoelectric effect and of Compton (1923a,b) on the scattering of x-rays established, so to say, the “existence” of the quantum of radiation, or the photon as we now call it.11 It was then natural for someone to derive Planck’s radiation formula by treating black-body radiation as a gas of photons in the same way as Maxwell had derived his law of distribution of molecular speeds for a gas of conventional molecules. But, then, does a gas of photons differ so radically from a gas of conventional molecules that the two laws of distribution should be so different from one another?
The answer to this question was provided by the manner in which Planck’s formula was derived by Bose. In his historic paper of 1924, Bose treated black-body radiation as a gas of photons; however, instead of considering the allocation of the “individual” photons to the various energy states of the system, he ﬁxed his attention on the number of states that contained “a particular number” of photons. Einstein, who seems to have translated Bose’s paper into German from an English manuscript sent to him by the author, at once recognized the importance of this approach and added the following note to his translation: “Bose’s derivation of Planck’s formula is in my opinion an important step forward. The method employed here would also yield the quantum theory of an ideal gas, which I propose to demonstrate elsewhere.”
Implicit in Bose’s approach was the fact that in the case of photons what really mattered was “the set of numbers of photons in various energy states of the system” and not the speciﬁcation as to “which photon was in which state”; in other words, photons were mutually indistinguishable. Einstein argued that what Bose had implied for photons should be true for material particles as well (for the property of indistinguishability arose essentially from the wave character of these entities and, according to de Broglie, material particles also possessed that character).12 In two papers, which appeared soon after, Einstein (1924, 1925) applied Bose’s method to the study of an ideal gas and thereby developed what we now call Bose–Einstein statistics. In the second of these papers, the fundamental difference between the new statistics and the classical Maxwell–Boltzmann statistics comes out so transparently in terms of the indistinguishability of the molecules.13 In the same paper, Einstein discovered the phenomenon of Bose–Einstein condensation which, 13 years later, was adopted by London (1938a,b) as the basis for a microscopic understanding of the curious properties of liquid He4 at low temperatures.
Following the enunciation of Pauli’s exclusion principle (1925), Fermi (1926) showed that certain physical systems would obey a different kind of statistics, namely the Fermi–Dirac statistics, in which not more than one particle could occupy the same energy state (ni = 0, 1). It seems important to mention here that Bose’s method of 1924 leads to the Fermi–Dirac distribution as well, provided that one limits the occupancy of an energy state to at most one particle.14
11Strictly speaking, it might be somewhat misleading to cite Einstein’s work on the photoelectric effect as a proof of the existence of photons. In fact, many of the effects (including the photoelectric effect), for which it seems necessary to invoke photons, can be explained away on the basis of a wave theory of radiation. The only phenomena for which photons seem indispensable are the ones involving ﬂuctuations, such as the Hanbury Brown–Twiss effect or the Lamb shift. For the relevance of ﬂuctuations to the problem of radiation, see ter Haar (1967, 1968).
12Of course, in the case of material particles, the total number N (of the particles) will also have to be conserved; this had not to be done in the case of photons. For details, see Section 6.1.
13It is here that one encounters the correct method of counting “the number of distinct ways in which gi energy states can accommodate ni particles,” depending on whether the particles are (i) distinguishable or (ii) indistinguishable. The occupancy of the individual states was, in each case, unrestricted, that is, ni = 0, 1, 2, . . ..
14Dirac, who was the ﬁrst to investigate the connection between statistics and wave mechanics, showed, in 1926, that the wave functions describing a system of identical particles obeying Bose–Einstein (or Fermi–Dirac) statistics must be symmetric (or antisymmetric) with respect to an interchange of two particles.

xxvi Historical Introduction
Soon after its appearance, the Fermi–Dirac statistics were applied by Fowler (1926) to discuss the equilibrium states of white dwarf stars and by Pauli (1927) to explain the weak, temperature-independent paramagnetism of alkali metals; in each case, one had to deal with a “highly degenerate” gas of electrons that obey Fermi–Dirac statistics. In the wake of this, Sommerfeld produced his monumental work of 1928 that not only put the electron theory of metals on a physically secure foundation but also gave it a fresh start in the right direction. Thus, Sommerfeld could explain practically all the major properties of metals that arose from conduction electrons and, in each case, obtained results that showed much better agreement with experiment than the ones following from the classical theories of Riecke (1898), Drude (1900), and Lorentz (1904–1905). Around the same time, Thomas (1927) and Fermi (1928) investigated the electron distribution in heavier atoms and obtained theoretical estimates for the relevant binding energies; these investigations led to the development of the so-called Thomas–Fermi model of the atom, which was later extended so that it could be applied to molecules, solids, and nuclei as well.15
Thus, the whole structure of statistical mechanics was overhauled by the introduction of the concept of indistinguishability of (identical) particles.16 The statistical aspect of the problem, which was already there in view of the large number of particles present, was now augmented by another statistical aspect that arose from the probabilistic nature of the wave mechanical description. One had, therefore, to carry out a two-fold averaging of the dynamical variables over the states of the given system in order to obtain the relevant expectation values. That sort of a situation was bound to necessitate a reformulation of the ensemble theory itself, which was carried out step by step. First, Landau (1927) and von Neumann (1927) introduced the so-called density matrix, which was the quantum-mechanical analogue of the density function of the classical phase space; this was elaborated, both from statistical and quantummechanical points of view, by Dirac (1929–1931). Guided by the classical ensemble theory, these authors considered both microcanonical and canonical ensembles; the introduction of grand canonical ensembles in quantum statistics was made by Pauli (1927).17
The important question as to which particles would obey Bose–Einstein statistics and which Fermi–Dirac remained theoretically unsettled until Belinfante (1939) and Pauli (1940) discovered the vital connection between spin and statistics.18 It turns out that those particles whose spin is an integral multiple of obey Bose–Einstein statistics while those whose spin is a half-odd integral multiple of obey Fermi–Dirac statistics. To date, no third category of particles has been discovered.
Apart from the foregoing milestones, several notable contributions toward the development of statistical mechanics have been made from time to time; however, most of those contributions were concerned with the development or perfection of mathematical techniques that make application of the basic formalism to actual physical problems more fruitful. A review of these developments is out of place here; they will be discussed at their appropriate place in the text.
15For an excellent review of this model, see March (1957). 16Of course, in many a situation where the wave nature of the particles is not so important, classical statistics continue to apply. 17A detailed treatment of this development has been given by Kramers (1938). 18See also Lu¨ ders and Zumino (1958).

1
The Statistical Basis of Thermodynamics

In the annals of thermal physics, the 1850s mark a very deﬁnite epoch. By that time the science of thermodynamics, which grew essentially out of an experimental study of the macroscopic behavior of physical systems, had become, through the work of Carnot, Joule, Clausius, and Kelvin, a secure and stable discipline of physics. The theoretical conclusions following from the ﬁrst two laws of thermodynamics were found to be in very good agreement with the corresponding experimental results.1 At the same time, the kinetic theory of gases, which aimed at explaining the macroscopic behavior of gaseous systems in terms of the motion of their molecules and had so far thrived more on speculation than calculation, began to emerge as a real, mathematical theory. Its initial successes were glaring; however, a real contact with thermodynamics could not be made until about 1872 when Boltzmann developed his H-theorem and thereby established a direct connection between entropy on one hand and molecular dynamics on the other. Almost simultaneously, the conventional (kinetic) theory began giving way to its more sophisticated successor — the ensemble theory. The power of the techniques that ﬁnally emerged reduced thermodynamics to the status of an “essential” consequence of the get-together of the statistics and the mechanics of the molecules constituting a given physical system. It was then natural to give the resulting formalism the name Statistical Mechanics.
As a preparation toward the development of the formal theory, we start with a few general considerations regarding the statistical nature of a macroscopic system. These considerations will provide ground for a statistical interpretation of thermodynamics. It may be mentioned here that, unless a statement is made to the contrary, the system under study is supposed to be in one of its equilibrium states.

1.1 The macroscopic and the microscopic states
We consider a physical system composed of N identical particles conﬁned to a space of volume V . In a typical case, N would be an extremely large number — generally, of order 1023. In view of this, it is customary to carry out analysis in the so-called thermodynamic limit, namely N → ∞, V → ∞ (such that the ratio N/V , which represents the particle density n, stays ﬁxed at a preassigned value). In this limit, the extensive properties of the system
1The third law, which is also known as Nernst’s heat theorem, did not arrive until about 1906. For a general discussion of this law, see Simon (1930) and Wilks (1961); these references also provide an extensive bibliography on this subject.

. Statistical Mechanics DOI: 10.1016/B978-0-12-382188-1.00001-3

1

© 2011 Elsevier Ltd. All rights reserved.

. 2 Chapter 1 The Statistical Basis of Thermodynamics

become directly proportional to the size of the system (i.e., proportional to N or to V ), while the intensive properties become independent thereof; the particle density, of course, remains an important parameter for all physical properties of the system.
Next we consider the total energy E of the system. If the particles comprising the system could be regarded as noninteracting, the total energy E would be equal to the sum of the energies εi of the individual particles:

E = niεi,

(1)

i

where ni denotes the number of particles each with energy εi. Clearly,

N = ni.

(2)

i

According to quantum mechanics, the single-particle energies εi are discrete and their values depend crucially on the volume V to which the particles are conﬁned. Accordingly, the possible values of the total energy E are also discrete. However, for large V , the spacing of the different energy values is so small in comparison with the total energy of the system that the parameter E might well be regarded as a continuous variable. This would be true even if the particles were mutually interacting; of course, in that case the total energy E cannot be written in the form (1).
The speciﬁcation of the actual values of the parameters N, V , and E then deﬁnes a macrostate of the given system.
At the molecular level, however, a large number of possibilities still exist because at that level there will in general be a large number of different ways in which the macrostate (N, V , E) of the given system can be realized. In the case of a noninteracting system, since the total energy E consists of a simple sum of the N single-particle energies εi, there will obviously be a large number of different ways in which the individual εi can be chosen so as to make the total energy equal to E. In other words, there will be a large number of different ways in which the total energy E of the system can be distributed among the N particles constituting it. Each of these (different) ways speciﬁes a microstate, or complexion, of the given system. In general, the various microstates, or complexions, of a given system can be identiﬁed with the independent solutions ψ(r1, . . . , rN ) of the Schro¨dinger equation of the system, corresponding to the eigenvalue E of the relevant Hamiltonian. In any case, to a given macrostate of the system there does in general correspond a large number of microstates and it seems natural to assume, when there are no other constraints, that at any time t the system is equally likely to be in any one of these microstates. This assumption forms the backbone of our formalism and is generally referred to as the postulate of “equal a priori probabilities” for all microstates consistent with a given macrostate.
The actual number of all possible microstates will, of course, be a function of N, V , and E and may be denoted by the symbol (N, V , E); the dependence on V comes in because the possible values εi of the single-particle energy ε are themselves a function

1.2 Contact between statistics and thermodynamics 3
of this parameter.2 Remarkably enough, it is from the magnitude of the number , and from its dependence on the parameters N, V , and E, that complete thermodynamics of the given system can be derived!
We shall not stop here to discuss the ways in which the number (N, V , E) can be computed; we shall do that only after we have developed our considerations sufﬁciently so that we can carry out further derivations from it. First we have to discover the manner in which this number is related to any of the leading thermodynamic quantities. To do this, we consider the problem of “thermal contact” between two given physical systems, in the hope that this consideration will bring out the true nature of the number .

1.2 Contact between statistics and thermodynamics: physical signiﬁcance of the number (N, V , E)
We consider two physical systems, A1 and A2, which are separately in equilibrium; see Figure 1.1. Let the macrostate of A1 be represented by the parameters N1, V1, and E1 so that it has 1(N1, V1, E1) possible microstates, and the macrostate of A2 be represented by the parameters N2, V2, and E2 so that it has 2(N2, V2, E2) possible microstates. The mathematical form of the function 1 may not be the same as that of the function 2, because that ultimately depends on the nature of the system. We do, of course, believe that all thermodynamic properties of the systems A1 and A2 can be derived from the functions
1(N1, V1, E1) and 2(N2, V2, E2), respectively. We now bring the two systems into thermal contact with each other, thus allowing the
possibility of exchange of energy between the two; this can be done by sliding in a conducting wall and removing the impervious one. For simplicity, the two systems are still separated by a rigid, impenetrable wall, so that the respective volumes V1 and V2 and the respective particle numbers N1 and N2 remain ﬁxed. The energies E1 and E2, however, become variable and the only condition that restricts their variation is

E(0) = E1 + E2 = const.

(1)

A1 (N1, V1, E1)

A2 (N2, V2, E2)

FIGURE 1.1 Two physical systems being brought into thermal contact.
2It may be noted that the manner in which the εi depend on V is itself determined by the nature of the system. For instance, it is not the same for relativistic systems as it is for nonrelativistic ones; compare, for instance, the cases dealt with in Section 1.4 and in Problem 1.7. We should also note that, in principle, the dependence of on V arises from the fact that it is the physical dimensions of the container that appear in the boundary conditions imposed on the wave functions of the system.

. 4 Chapter 1 The Statistical Basis of Thermodynamics

Here, E(0) denotes the energy of the composite system A(0)(≡ A1 + A2); the energy of inter-
action between A1 and A2, if any, is being neglected. Now, at any time t, the subsystem A1 is equally likely to be in any one of the 1(E1) microstates while the subsystem A2 is equally likely to be in any one of the 2(E2) microstates; therefore, the composite system A(0) is
equally likely to be in any one of the

1(E1) 2(E2) = 1(E1) 2(E(0) − E1) = (0)(E(0), E1)

(2)

microstates.3 Clearly, the number (0) itself varies with E1. The question now arises: at what value of E1 will the composite system be in equilibrium? In other words, how far will the energy exchange go in order to bring the subsystems A1 and A2 into mutual equilibrium?
We assert that this will happen at that value of E1 which maximizes the number (0)(E(0), E1). The philosophy behind this assertion is that a physical system, left to itself, proceeds naturally in a direction that enables it to assume an ever-increasing number of microstates until it ﬁnally settles down in a macrostate that affords the largest possible number of microstates. Statistically speaking, we regard a macrostate with a larger number of microstates as a more probable state, and the one with the largest number of microstates as the most probable one. Detailed studies show that, for a typical system, the number of microstates pertaining to any macrostate that departs even slightly from the most probable one is “orders of magnitude” smaller than the number pertaining to the latter. Thus, the most probable state of a system is the macrostate in which the system spends an “overwhelmingly” large fraction of its time. It is then natural to identify this state with the equilibrium state of the system.
Denoting the equilibrium value of E1 by E1 (and that of E2 by E2), we obtain, on maximizing (0),

∂ 1(E1)

∂ E1

E1 =E 1

2(E2) +

1(E1)

∂ 2(E2) ∂ E2

E2 =E 2

·

∂ E2 ∂ E1

= 0.

Since ∂E2/∂E1 = −1, see equation (1), the foregoing condition can be written as

∂ ln 1(E1)

∂ E1

=
E1 =E 1

∂ ln 2(E2) ∂ E2

.
E2 =E 2

Thus, our condition for equilibrium reduces to the equality of the parameters β1 and β2 of the subsystems A1 and A2, respectively, where β is deﬁned by

β≡

∂ ln

(N, V , E)

∂E

.
N,V , E=E

(3)

3It is obvious that the macrostate of the composite system A(0) has to be deﬁned by two energies, namely E1 and E2 (or else E(0) and E1).

1.2 Contact between statistics and thermodynamics 5

We thus ﬁnd that when two physical systems are brought into thermal contact, which allows an exchange of energy between them, this exchange continues until the equilibrium values E1 and E2 of the variables E1 and E2 are reached. Once these values are reached, there is no more net exchange of energy between the two systems; the systems are then said to have attained a state of thermal equilibrium. According to our analysis, this happens only when the respective values of the parameter β, namely β1 and β2, become equal.4 It is then natural to expect that the parameter β is somehow related to the thermodynamic temperature T of a given system. To determine this relationship, we recall the thermodynamic formula

∂S

1

∂E N,V = T ,

(4)

where S is the entropy of the system in question. Comparing equations (3) and (4), we conclude that an intimate relationship exists between the thermodynamic quantity S and the statistical quantity ; we may, in fact, write for any physical system

S

1

(ln ) = βT = const.

(5)

This correspondence was ﬁrst established by Boltzmann who also believed that, since the relationship between the thermodynamic approach and the statistical approach seems to be of a fundamental character, the constant appearing in (5) must be a universal constant. It was Planck who ﬁrst wrote the explicit formula

S = k ln ,

(6)

without any additive constant S0. As it stands, formula (6) determines the absolute value of the entropy of a given physical system in terms of the total number of microstates accessible to it in conformity with the given macrostate. The zero of entropy then corresponds to the special state for which only one microstate is accessible ( = 1) — the so-called “unique conﬁguration”; the statistical approach thus provides a theoretical basis for the third law of thermodynamics as well. Formula (6) is of fundamental importance in physics; it provides a bridge between the microscopic and the macroscopic.
Now, in the study of the second law of thermodynamics we are told that the law of increase of entropy is related to the fact that the energy content of the universe, in its natural course, is becoming less and less available for conversion into work; accordingly, the entropy of a given system may be regarded as a measure of the so-called disorder or chaos prevailing in the system. Formula (6) tells us how disorder arises microscopically. Clearly, disorder is a manifestation of the largeness of the number of microstates the system can have. The larger the choice of microstates, the lesser the degree of predictability and hence the increased level of disorder in the system. Complete order prevails when and

4This result may be compared with the so-called “zeroth law of thermodynamics,” which stipulates the existence of a common parameter T for two or more physical systems in thermal equilibrium.

. 6 Chapter 1 The Statistical Basis of Thermodynamics

only when the system has no other choice but to be in a unique state ( = 1); this, in turn, corresponds to a state of vanishing entropy.
By equations (5) and (6), we also have

β

=

1 kT

.

(7)

The universal constant k is generally referred to as the Boltzmann constant. In Section 1.4
we shall discover how k is related to the gas constant R and the Avogadro number NA; see equation (1.4.3).5

1.3 Further contact between statistics and thermodynamics
In continuation of the preceding considerations, we now examine a more elaborate
exchange between the subsystems A1 and A2. If we assume that the wall separating the two subsystems is movable as well as conducting, then the respective volumes V1 and V2 (of subsystems A1 and A2) also become variable; indeed, the total volume V (0)(= V1 + V2) remains constant, so that effectively we have only one more independent variable. Of
course, the wall is still assumed to be impenetrable to particles, so the numbers N1 and N2 remain ﬁxed. Arguing as before, the state of equilibrium for the composite system A(0) will obtain when the number (0)(V (0), E(0); V1, E1) attains its largest value; that is, when not only

∂ ln 1 ∂ E1

=
N1,V1; E1=E1

∂ ln 2 ∂ E2

,
N2,V2; E2=E2

(1a)

but also

∂ ln 1 ∂ V1

=
N1,E1; V1=V 1

∂ ln 2 ∂ V2

.
N2,E2; V2=V 2

(1b)

Our conditions for equilibrium now take the form of an equality between the pair of parameters (β1, η1) of the subsystem A1 and the parameters (β2, η2) of the subsystem A2 where, by deﬁnition,

η≡

∂ ln

(N, V , E)

∂V

.
N,E,V =V

(2)

Similarly, if A1 and A2 came into contact through a wall that allowed an exchange of particles as well, the conditions for equilibrium would be further augmented by the equality

5We follow the notation whereby equation (1.4.3) means equation (3) of Section 1.4. However, while referring to an equation in the same section, we will omit the mention of the section number.

1.3 Further contact between statistics and thermodynamics 7

of the parameter ζ1 of subsystem A1 and the parameter ζ2 of subsystem A2 where, by deﬁnition,

ζ≡

∂ ln

(N, V , E)

.

∂N

V ,E,N=N

(3)

To determine the physical meaning of the parameters η and ζ , we make use of equation (1.2.6) and the basic formula of thermodynamics, namely

dE = T dS − P dV + µ dN,

(4)

where P is the thermodynamic pressure and µ the chemical potential of the given system. It follows that

η

=

P kT

and

ζ

=

−

µ kT

.

(5)

From a physical point of view, these results are completely satisfactory because, thermodynamically as well, the conditions of equilibrium between two systems A1 and A2, if the wall separating them is both conducting and movable (thus making their respective energies and volumes variable), are indeed the same as the ones contained in equations (1a) and (1b), namely

T1 = T2 and P1 = P2.

(6)

On the other hand, if the two systems can exchange particles as well as energy but have their volumes ﬁxed, the conditions of equilibrium, obtained thermodynamically, are indeed

T1 = T2 and µ1 = µ2.

(7)

And ﬁnally, if the exchange is such that all three (macroscopic) parameters become variable, then the conditions of equilibrium become

T1 = T2, P1 = P2, and µ1 = µ2.

(8)6

It is gratifying that these conclusions are identical to the ones following from statistical considerations.
Combining the results of the foregoing discussion, we arrive at the following recipe for deriving thermodynamics from a statistical beginning: determine, for the macrostate (N, V , E) of the given system, the number of all possible microstates accessible to the system; call this number (N, V , E). Then, the entropy of the system in that state follows from
6It may be noted that the same would be true for any two parts of a single thermodynamic system; consequently, in equilibrium, the parameters T , P, and µ would be constant throughout the system.

. 8 Chapter 1 The Statistical Basis of Thermodynamics

the fundamental formula

S(N, V , E) = k ln (N, V , E),

(9)

while the leading intensive ﬁelds, namely temperature, pressure, and chemical potential, are given by

∂S

1

∂S

P ∂S

µ

∂E N,V = T ;

∂V N,E = T ;

∂N V ,E = − T .

(10)

Alternatively, we can write7

∂S

∂S

∂E

P = ∂V N,E

∂E

=−
N ,V

∂V

N ,S

(11)

and

µ=−

∂S ∂N

V ,E

∂S

∂E

∂E

=
N ,V

∂N

,
V ,S

(12)

while

∂E

T=

∂S

.
N ,V

(13)

Formulae (11) through (13) follow equally well from equation (4). The evaluation of P, µ, and T from these formulae indeed requires that the energy E be expressed as a function of the quantities N, V , and S; this should, in principle, be possible once S is known as a function of N, V , and E.
The rest of the thermodynamics follows straightforwardly; see Appendix H. For instance, the Helmholtz free energy A, the Gibbs free energy G, and the enthalpy H are given by

A = E − TS, G = A + PV = E − TS + PV
= µN

(14) (15)8

7In writing these formulae, we have made use of the well-known relationship in partial differential calculus, namely that “if three variables x, y, and z are mutually related, then (see Appendix H)

∂x ∂y z

∂y ∂z x

∂z ∂x

= −1.”
y

8The relation E − TS + PV = µN follows directly from (4). For this, all we have to do is to regard the given system as having grown to its present size in a gradual manner, such that the intensive parameters, T , P, and µ stayed constant throughout the process while the extensive parameters N, V , and E (and hence S) grew proportionately with one another.

1.4 The classical ideal gas 9

and

H = E + PV = G + TS.

(16)

The speciﬁc heat at constant volume, CV , and the one at constant pressure, CP, would be given by

∂S

∂E

CV ≡ T

∂T

=
N ,V

∂T

N ,V

(17)

and

CP ≡ T

∂S ∂T

=
N ,P

∂(E + PV )

∂T

=
N ,P

∂H ∂T

.
N ,P

(18)

1.4 The classical ideal gas

To illustrate the approach developed in the preceding sections, we shall now derive the various thermodynamic properties of a classical ideal gas composed of monatomic molecules. The main reason why we choose this highly specialized system for consideration is that it affords an explicit, though asymptotic, evaluation of the number (N, V , E). This example becomes all the more instructive when we ﬁnd that its study enables us, in a most straightforward manner, to identify the Boltzmann constant k in terms of other physical constants; see equation (3). Moreover, the behavior of this system serves as a useful reference with which the behavior of other physical systems, especially real gases (with or without quantum effects), can be compared. And, indeed, in the limit of high temperatures and low densities the ideal-gas behavior becomes typical of most real systems.
Before undertaking a detailed study of this case it appears worthwhile to make a remark that applies to all classical systems composed of noninteracting particles, irrespective of the internal structure of the particles. This remark is related to the explicit dependence of the number (N, V , E) on V and hence to the equation of state of these systems. Now, if there do not exist any spatial correlations among the particles, that is, if the probability of any one of them being found in a particular region of the available space is completely independent of the location of the other particles,9 then the total number of ways in which the N particles can be spatially distributed in the system will be simply equal to the product of the numbers of ways in which the individual particles can be accommodated in the same space independently of one another. With N and E ﬁxed, each of these numbers will be directly proportional to V , the volume of the container; accordingly, the total number of ways will be directly proportional to the Nth power of V :

(N, E, V ) ∝ V N .

(1)

9This will be true if (i) the mutual interactions among particles are negligible, and (ii) the wave packets of individual particles do not signiﬁcantly overlap (or, in other words, the quantum effects are also negligible).

. 10 Chapter 1 The Statistical Basis of Thermodynamics

Combined with equations (1.3.9) and (1.3.10), this gives

P

∂ ln (N, E, V )

N

T =k

∂V

N,E = k V .

(2)

If the system contains n moles of the gas, then N = nNA, where NA is the Avogadro number. Equation (2) then becomes

PV = NkT = nRT (R = kNA),

(3)

which is the famous ideal-gas law, R being the gas constant per mole. Thus, for any classical system composed of noninteracting particles the ideal-gas law holds.
For deriving other thermodynamic properties of this system, we require a detailed knowledge of the way depends on the parameters N, V , and E. The problem essentially reduces to determining the total number of ways in which equations (1.1.1) and (1.1.2) can be mutually satisﬁed. In other words, we have to determine the total number of (independent) ways of satisfying the equation

3N

εr = E,

(4)

r=1

where εr are the energies associated with the various degrees of freedom of the N particles. The reason why this number should depend on the parameters N and E is quite
obvious. Nevertheless, this number also depends on the “spectrum of values” that the variables εr can assume; it is through this spectrum that the dependence on V comes in. Now, the energy eigenvalues for a free, nonrelativistic particle conﬁned to a cubical box of side L (V = L3), under the condition that the wave function ψ(r) vanishes everywhere on the boundary, are given by

ε(nx, ny, nz)

=

h2 8mL2

(n2x

+ n2y

+

n2z );

nx, ny, nz = 1, 2, 3, . . . ,

(5)

where h is Planck’s constant and m the mass of the particle. The number of distinct eigenfunctions (or microstates) for a particle of energy ε would, therefore, be equal to the number of independent, positive-integral solutions of the equation

n2x + n2y + n2z

=

8mV 2/3ε h2

=

ε∗.

(6)

We may denote this number by (1, ε, V ). Extending the argument, it follows that the desired number (N, E, V ) would be equal to the number of independent, positiveintegral solutions of the equation

3N
n2r
r=1

=

8mV 2/3E h2

=

E∗,

say.

(7)

1.4 The classical ideal gas 11

An important result follows straightforwardly from equation (7), even before the number (N, E, V ) is explicitly evaluated. From the nature of the expression appearing on the right
side of this equation, we conclude that the volume V and the energy E of the system enter into the expression for in the form of the combination (V 2/3E). Consequently,

S(N, V , E) ≡ S N, V 2/3E .

(8)

Hence, for the constancy of S and N, which deﬁnes a reversible adiabatic process,

V 2/3E = const.

(9)

Equation (1.3.11) then gives

∂E

2E

P = − ∂V N,S = 3 V ,

(10)

that is, the pressure of a system of nonrelativistic, noninteracting particles is precisely equal to two-thirds of its energy density.10 It should be noted here that, since an explicit computation of the number has not yet been done, results (9) and (10) hold for quantum as well as classical statistics; equally general is the result obtained by combining these, namely

PV 5/3 = const.,

(11)

which tells us how P varies with V during a reversible adiabatic process. We shall now attempt to evaluate the number . In this evaluation we shall explicitly
assume the particles to be distinguishable, so that if a particle in state i gets interchanged with a particle in state j the resulting microstate is counted as distinct. Consequently, the number (N, V , E), or better N (E∗) (see equation (7)), is equal to the number of positiveintegral lattice points lying on the surface of a 3N-dimensional sphere of radius √E∗.11 Clearly, this number will be an extremely irregular function of E∗, in that for two given values of E∗ that may be very close to one another, the values of this number could be very different. In contrast, the number N (E∗), which denotes the number of positive-integral lattice points lying on or within the surface of a 3N-dimensional sphere of radius √E∗, will be much less irregular. In terms of our physical problem, this would correspond to the number, (N, V , E), of microstates of the given system consistent with all macrostates characterized by the speciﬁed values of the parameters N and V but having energy less

10 Combining

(10)

with

(2),

we

obtain

for

the

classical

ideal

gas:

E

=

3 2

NkT

.

Accordingly,

equation

(9)

reduces

to

the

well-known

thermodynamic

relationship:

V γ −1T

=

const.,

which

holds

during

a

reversible

adiabatic

process,

with

γ

=

5 3

.

11If the particles are regarded as indistinguishable, the evaluation of the number by counting lattice points becomes

quite intricate. The problem is then solved by having recourse to the theory of “partitions of numbers”; see Auluck and

Kothari (1946).

. 12 Chapter 1 The Statistical Basis of Thermodynamics

than or equal to E; that is,

(N, V , E) =

(N, V , E )

(12)

E ≤E

or

N (E∗) =

N (E∗ ).

(13)

E∗ ≤E∗

Of course, the number will also be somewhat irregular; however, we expect that its asymptotic behavior, as E∗ → ∞, will be a lot smoother than that of . We shall see in the sequel that the thermodynamics of the system follows equally well from the number as from .
To appreciate the point made here, let us digress a little to examine the behavior of the numbers 1(ε∗) and 1(ε∗), which correspond to the case of a single particle conﬁned to the given volume V . The exact values of these numbers, for ε∗ ≤ 10,000, can be extracted from a table compiled by Gupta (1947). The wild irregularities of the number
1(ε∗) can hardly be missed. The number 1(ε∗), on the other hand, exhibits a much smoother asymptotic behavior. From the geometry of the problem, we note that, asymptotically, 1(ε∗) should be equal to the volume of an octant of a three-dimensional sphere of radius √ε∗, that is,

lim
ε∗ →∞

1(ε∗) (π/6)ε∗3/2

=

1.

(14)

A more detailed analysis shows that, to the next approximation (see Pathria, 1966),

1(ε∗) ≈

π 6

ε∗3/2

−

3π 8

ε∗;

(15)

the correction term arises from the fact that the volume of an octant somewhat overestimates the number of desired lattice points, for it includes, partly though, some points with one or more coordinates equal to zero. Figure 1.2 shows a histogram of the actual values of 1(ε∗) for ε∗ lying between 200 and 300; the theoretical estimate (15) is also shown. In the ﬁgure, we have also included a histogram of the actual values of the corresponding number of microstates, 1(ε∗), when the quantum numbers nx, ny, and nz can assume the value zero as well. In the latter case, the volume of an octant somewhat underestimates the number of desired lattice points; we now have

1(ε∗) ≈

π 6

ε∗3/2

+

3π 8

ε∗.

(16)

Asymptotically, however, the number 1(ε∗) also satisﬁes equation (14). Returning to the N-particle problem, the number N (E∗) should be asymptotically
equal to the “volume” of the “positive compartment” of a 3N-dimensional sphere of

1.4 The classical ideal gas 13
3200

2800

␲ 6

(´*)3/2 1

3␲ ´* 8

2400

⌺1(´*) ␲ 6 (´*) 3/2

2000

1600

␲ (´*)3/2 2 3␲ ´*

6

8

1200 200 220 240 260 280 300 ´*
FIGURE 1.2 Histograms showing the actual number of microstates available to a particle in a cubical enclosure; the lower histogram corresponds to the so-called Dirichlet boundary conditions, while the upper one corresponds to the Neumann boundary conditions (see Appendix A). The corresponding theoretical estimates, (15) and (16), are shown by dashed lines; the customary estimate, equation (14), is shown by a solid line.

radius √E∗. Referring to equation (C.7a) of Appendix C, we obtain

N (E∗) ≈

1 3N 2

π 3N/2 (3N /2)!

E

∗3N

/2

which, on substitution for E∗, gives

(N, V , E) ≈

V h3

N (2π mE)3N/2 .
(3N /2)!

(17)

Taking logarithms and applying Stirling’s formula, (B.29) in Appendix B,

ln(n! ) ≈ n ln n − n (n 1),

(18)

. 14 Chapter 1 The Statistical Basis of Thermodynamics

we get

ln

(N, V , E) ≈ N ln

V h3

4π mE 3N

3/2

3 + 2N.

(19)

For deriving the thermodynamic properties of the given system we must somehow ﬁx

the precise value of, or limits for, the energy of the system. In view of the extremely irreg-

ular nature of the function (N, V , E), the speciﬁcation of a precise value for the energy

of the system cannot be justiﬁed on physical grounds, for that would never yield well-

behaved expressions for the thermodynamic functions of the system. From a practical

point of view, too, an absolutely isolated system is too much of an idealization. In the real

world, almost every system has some contact with its surroundings, however little it may

be; as a result, its energy cannot be deﬁned sharply.12 Of course, the effective width of the

range over which the energy may vary would, in general, be small in comparison with the

mean value of the energy. Let us specify this range by the limits

E

−

1 2

and

E

+

1 2

where, by assumption,

E; typically, /E = O(1/√N). The corresponding number of

microstates, (N, V , E; ), is then given by

which gives

(N, V , E; )

∂ (N, V , E) ∂E

3N ≈2E

(N, V , E),

(17a)

ln (N, V , E;

) ≈ N ln

V h3

4π mE 3/2 3

3N

+ 2 N + ln

3N 2

+ ln

E

.

(19a)

Now, for N 1, the ﬁrst term in the curly bracket is negligible in comparison with any
of the terms outside this bracket, for lim (ln N)/N = 0. Furthermore, for any reasonable
N →∞
value of /E, the same is true of the second term in this bracket.13 Hence, for all practical
purposes,

V 4π mE 3/2 3

ln ≈ ln ≈ N ln h3 3N

+ 2N.

(20)

We thus arrive at the bafﬂing result that, for all practical purposes, the actual width of the

range allowed for the energy of the system does not make much difference; the energy

could lie between

E

−

1 2

and

E

+

1 2

or equally well between 0 and E. The reason

underlying this situation is that the rate at which the number of microstates of the system

12Actually, the very act of making measurements on a system brings about, inevitably, a contact between the system
and its surroundings. 13It should be clear that, while /E is much less than 1, it must not tend to 0, for that would make → 0 and ln →
−∞. A situation of that kind would be too artiﬁcial and would have nothing to do with reality. Actually, in most physical systems, /E = O(N−1/2), whereby ln( /E) becomes of order ln N, which again is negligible in comparison with the
terms outside the curly bracket.

1.4 The classical ideal gas 15

increases with energy is so fantastic, see equation (17), that even if we allow all values of energy between zero and a particular value E, it is only the “immediate neighborhood” of E that makes an overwhelmingly dominant contribution to this number! And since we are ﬁnally concerned only with the logarithm of this number, even the “width” of that neighborhood is inconsequential!
The stage is now set for deriving the thermodynamics of our system. First of all, we have

S(N, V , E) = k ln

V = Nk ln h3

4π mE 3/2 3

3N

+ 2 Nk,

(21)14

which can be inverted to give

E(S, V , N)

=

3h2N 4π mV 2/3

exp

2S 3Nk

−

1

.

(22)

The temperature of the gas then follows with the help of formula (1.3.10) or (1.3.13), which leads to the energy–temperature relationship

3

3

E = N 2 kT = n 2 RT ,

(23)

where n is the number of moles of the gas. The speciﬁc heat at constant volume now follows with the help of formula (1.3.17):

∂E

3

3

CV =

∂T

N ,V

=

Nk 2

=

nR. 2

(24)

For the equation of state, we obtain

∂E

2E

P = − ∂V N,S = 3 V ,

(25)

which agrees with our earlier result (10). Combined with (23), this gives

NkT

P = V or PV = nRT ,

(26)

which is the same as (3). The speciﬁc heat at constant pressure is given by, see (1.3.18),

CP =

∂(E + PV ) ∂T

5

N ,P

=

nR, 2

(27)

14Henceforth, we shall replace the sign ≈, which characterizes the asymptotic character of a relationship, by the sign of equality because for most physical systems the asymptotic results are as good as exact.

. 16 Chapter 1 The Statistical Basis of Thermodynamics

so that, for the ratio of the two speciﬁc heats, we have

γ

=

CP /CV

=

5. 3

(28)

Now, suppose that the gas undergoes an isothermal change of state (T = const. and N = const.); then, according to (23), the total energy of the gas would remain constant while, according to (26), its pressure would vary inversely with volume (Boyle’s law). The change in the entropy of the gas, between the initial state i and the ﬁnal state f , would then be, see equation (21),

Sf − Si = Nk ln(Vf /Vi).

(29)

On the other hand, if the gas undergoes a reversible adiabatic change of state (S = const. and N = const.), then, according to (22) and (23), both E and T would vary as V −2/3; so, according to (25) or (26), P would vary as V −5/3. These results agree with the conventional
thermodynamic ones, namely

PV γ = const. and TV γ −1 = const.,

(30)

with

γ

=

5 3

.

It

may

be

noted

that,

thermodynamically,

the

change

in

E

during

an

adiabatic

process arises solely from the external work done by the gas on the surroundings or vice

versa:

(dE)adiab

=

−PdV

=

2E − 3V

dV ;

(31)

see equations (1.3.4) and (25). The dependence of E on V follows readily from this relationship.
The considerations of this section have clearly demonstrated the manner in which the thermodynamics of a macroscopic system can be derived from the multiplicity of its microstates (as represented by the number or or ). The whole problem then hinges on an asymptotic enumeration of these numbers, which unfortunately is tractable only in a few idealized cases, such as the one considered in this section; see also Problems 1.7 and 1.8. Even in an idealized case like this, there remains an inadequacy that could not be detected in the derivations made so far; this relates to the explicit dependence of S on N. The discussion of the next section is intended not only to bring out this inadequacy but also to provide the necessary remedy for it.

1.5 The entropy of mixing and the Gibbs paradox
One thing we readily observe from expression (1.4.21) is that, contrary to what is logically desired, the entropy of an ideal gas, as given by this expression, is not an extensive

1.5 The entropy of mixing and the Gibbs paradox 17

(N1, V1; T ) (N2, V2; T )

FIGURE 1.3 The mixing together of two ideal gases 1 and 2.

property of the system! That is, if we increase the size of the system by a factor α, keeping the intensive variables unchanged,15 then the entropy of the system, which should also increase by the same factor α, does not do so; the presence of the ln V term in the expression affects the result adversely. This in a way means that the entropy of this system is different from the sum of the entropies of its parts, which is quite unphysical. A more common way of looking at this problem is to consider the so-called Gibbs paradox.
Gibbs visualized the mixing of two ideal gases 1 and 2, both being initially at the same temperature T ; see Figure 1.3. Clearly, the temperature of the mixture would also be the same. Now, before the mixing took place, the respective entropies of the two gases were, see equations (1.4.21) and (1.4.23),

3 Si = Nik ln Vi + 2 Nik

1 + ln

2π mikT h2

; i = 1, 2.

(1)

After the mixing has taken place, the total entropy would be

2
ST =

3 Nik ln V + 2 Nik

1 + ln

2π mikT h2

,

(2)

i=1

where V = V1 + V2. Thus, the net increase in the value of S, which may be called the entropy of mixing, is given by

(

2
S) = ST − Si = k
i=1

N1 ln

V1 + V2 V1

+ N2

ln

V1 + V2 V2

;

(3)

the quantity S is indeed positive, as it must be for an irreversible process like mixing. Now, in the special case when the initial particle densities of the two gases (and, hence, the particle density of the mixture) are also the same, equation (3) becomes

(

S)∗ = k

N1 ln

N1 + N2 N1

+ N2 ln

N1 + N2 N2

,

(4)

which is again positive.
15This means an increase of the parameters N, V , and E to αN, αV , and αE, so that the energy per particle and the volume per particle remain unchanged.

. 18 Chapter 1 The Statistical Basis of Thermodynamics

So far, it seems all right. However, a paradoxical situation arises if we consider the mixing of two samples of the same gas. Once again, the entropies of the individual samples will be given by (1); of course, now m1 = m2 = m, say. And the entropy after mixing will be given by

3

2π mkT

ST

=

Nk

ln V

+

Nk 2

1 + ln

h2

,

(2a)

where N = N1 + N2; note that this expression is numerically the same as (2), with mi = m. Therefore, the entropy of mixing in this case will also be given by expression (3) and, if N1/V1 = N2/V2 = (N1 + N2)/(V1 + V2), by expression (4). The last conclusion, however, is unacceptable because the mixing of two samples of the same gas, with a common initial temperature T and a common initial particle density n, is clearly a reversible process, for we can simply reinsert the partitioning wall into the system and obtain a situation that is in no way different from the one we had before mixing. Of course, we tacitly imply that in dealing with a system of identical particles we cannot track them down individually; all we can reckon with is their numbers. When two dissimilar gases, even with a common initial temperature T , and a common initial particle density n, mixed together the process was irreversible, for by reinserting the partitioning wall one would obtain two samples of the mixture and not the two gases that were originally present; to that case, expression (4) would indeed apply. However, in the present case, the corresponding result should be

( S)∗1≡2 = 0.

(4a)16

The foregoing result would also be consistent with the requirement that the entropy of a given system is equal to the sum of the entropies of its parts. Of course, we had already noticed that this is not ensured by expression (1.4.21). Thus, once again we are led to believe that there is something basically wrong with that expression.
To see how the above paradoxical situation can be avoided, we recall that, for the entropy of mixing of two samples of the same gas, with a common T and a common n, we were led to result (4), which can also be written as

( S)∗ = ST − (S1 + S2) ≈ k[ln{(N1 + N2)! } − ln(N1! ) − ln(N2! )],

(4)

instead of the logical result (4a). A closer look at this expression shows that we would indeed obtain the correct result if our original expression for S were diminished by an ad hoc term, k ln(N! ), for that would diminish S1 by k ln(N1! ), S2 by k ln(N2! ) and ST by k ln {(N1 + N2)! }, with the result that ( S)∗ would turn out to be zero instead of the expression appearing in (4). Clearly, this would amount to an ad hoc reduction of the statistical numbers and by a factor N!. This is precisely the remedy proposed by Gibbs to avoid the paradox in question.
16In view of this, we fear that expression (3) may also be inapplicable to this case.

1.5 The entropy of mixing and the Gibbs paradox 19

If we agree with the foregoing suggestion, then the modiﬁed expression for the entropy of a classical ideal gas would be

S(N, V , E) = Nk ln

V Nh3

4π mE 3/2 5

3N

+ 2 Nk

V3 5

2π mkT

= Nk ln N + 2 Nk 3 + ln h2

,

(1.4.21a) (1a)

which indeed is truly extensive! If we now mix two samples of the same gas at a common initial temperature T , the entropy of mixing would be

( S)1≡2 = k (N1 + N2) ln

V1 + V2 N1 + N2

− N1 ln

V1 N1

− N2 ln

V2 N2

(3a)

and, if the initial particle densities of the samples were also equal, the result would be

( S)∗1≡2 = 0.

(4a)

It may be noted that for the mixing of two dissimilar gases, the original expressions (3) and (4) would continue to hold even when (1.4.21) is replaced by (1.4.21a).17 The paradox of Gibbs is thereby resolved.
Equation (1a) is generally referred to as the Sackur–Tetrode equation. We reiterate the fact that, by this equation, the entropy of the system does indeed become a truly extensive quantity. Thus, the very root of the trouble has been eliminated by the recipe of Gibbs. We shall discuss the physical implications of this recipe in Section 1.6; here, let us jot down some of its immediate consequences.
First of all, we note that the expression for the energy E of the gas, written as a function of N, V , and S, is also modiﬁed. We now have

E(N, V , S)

=

3h2N 5/3 4π mV 2/3

exp

2S 5 3Nk − 3

,

(1.4.22a)

which, unlike its predecessor (1.4.22), makes energy too a truly extensive quantity. Of course, the thermodynamic results (1.4.23) through (1.4.31), derived in the previous section, remain unchanged. However, there are some that were intentionally left out, for they would come out correct only from the modiﬁed expression for S(N, V , E) or E(S, V , N). The most important of these is the chemical potential of the gas, for which we obtain

µ≡

∂E ∂N

5 2S

=E
V ,S

3N − 3N2k

.

(5)

17Because, in this case, the entropy ST of the mixture would be diminished by k ln(N1! N2! ), rather than by k ln{(N1 + N2)! }.

. 20 Chapter 1 The Statistical Basis of Thermodynamics

In view of equations (1.4.23) and (1.4.25), this becomes

µ

=

1 N

[E

+

PV

−

TS]

≡

G N

,

(6)

where G is the Gibbs free energy of the system. In terms of the variables N, V , and T , expression (5) takes the form



µ(N

,

V

,

T

)

=

kT

ln

 

N V

h2

3/2 

.

2π mkT 

(7)

Another quantity of importance is the Helmholtz free energy:

 N

h2

3/2  

A

=

E

−

TS

=

G

−

PV

=

NkT

ln 

V

2π mkT

− 1. 

(8)

It will be noted that, while A is an extensive property of the system, µ is intensive.

1.6 The “correct” enumeration of the microstates
In the preceding section we saw that an ad hoc diminution in the entropy of an N-particle system by an amount k ln(N!), which implies an ad hoc reduction in the number of microstates accessible to the system by a factor (N!), was able to correct the unphysical features of some of our former expressions. It is now natural to ask: why, in principle, should the number of microstates, computed in Section 1.4, be reduced in this manner? The physical reason for doing so is that the particles constituting the given system are not only identical but also indistinguishable; accordingly, it is unphysical to label them as No. 1, No. 2, No. 3, and so on and to speak of their being individually in the various single-particle states εi. All we can sensibly speak of is their distribution over the states εi by numbers, that is, n1 particles being in the state ε1, n2 in the state ε2, and so on. Thus, the correct way of specifying a microstate of the system is through the distribution numbers {nj}, and not through the statement as to “which particle is in which state.” To elaborate the point, we may say that if we consider two microstates that differ from one another merely in an interchange of two particles in different energy states, then according to our original mode of counting we would regard these microstates as distinct; in view of the indistinguishability of the particles, however, these microstates are not distinct (for, physically, there exists no way whatsoever of distinguishing between them).18
18Of course, if an interchange took place among particles in the same energy state, then even our original mode of counting did not regard the two microstates as distinct.

1.6 The “correct” enumeration of the microstates 21

Now, the total number of permutations that can be effected among N particles, distributed according to the set {ni}, is

N!

,

(1)

n1! n2! . . .

where the ni must be consistent with the basic constraints (1.1.1) and (1.1.2).19 If our particles were distinguishable, then all these permutations would lead to “distinct” microstates. However, in view of the indistinguishability of the particles, these permutations must be regarded as leading to one and the same thing; consequently, for any distribution set {ni}, we have one, and only one, distinct microstate. As a result, the total number of distinct microstates accessible to the system, consistent with a given macrostate (N, V , E), would be severely cut down. However, since factor (1) itself depends on the numbers ni constituting a particular distribution set and for a given macrostate there will be many such sets, there is no straightforward way to “correct down” the number of microstates computed on the basis of the classical concept of “distinguishability” of the particles.
The recipe of Gibbs clearly amounts to disregarding the details of the numbers ni and slashing the whole sequence of microstates by a common factor N!; this is correct for situations in which all N particles happen to be in different energy states but is certainly wrong for other situations. We must keep in mind that by adopting this recipe we are still using a spurious weight factor,

1

w{ni} = n1! n2! . . . ,

(2)

for the distribution set {ni} whereas in principle we should use a factor of unity, irrespective of the values of the numbers ni.20 Nonetheless, the recipe of Gibbs does correct the situation in a gross manner, though in matters of detail it is still inadequate. In fact, it is only by taking w{ni} to be equal to unity (or zero) that we obtain true quantum statistics!
We thus see that the recipe of Gibbs corrects the enumeration of the microstates, as necessitated by the indistinguishability of the particles, only in a gross manner. Numerically, this would approach closer and closer to reality as the probability of the ni being greater than 1 becomes less and less. This in turn happens when the given system is at a sufﬁciently high temperature (so that many more energy states become accessible) and has a sufﬁciently low density (so that there are not as many particles to accommodate). It follows that the “corrected” classical statistics represents truth more closely if the expectation values of the occupation numbers ni are much less than unity:

ni 1,

(3)

19The presence of the factors (ni! ) in the denominator is related to the comment made in the preceding note. 20Or a factor of zero if the distribution set {ni} is disallowed on certain physical grounds, such as the Pauli exclusion
principle.

. 22 Chapter 1 The Statistical Basis of Thermodynamics

that is, if the numbers ni are generally 0, occasionally 1, and rarely greater than 1. Condition (3) in a way deﬁnes the classical limit. We must, however, remember that it is because of the application of the correction factor 1/N!, which replaces (1) by (2), that our results agree with reality at least in the classical limit.
In Section 5.5 we shall demonstrate, in an independent manner, that the factor by which the number of microstates, as computed for the “labeled” molecules, be reduced so that the formalism of classical statistical mechanics becomes a true limit of the formalism of quantum statistical mechanics is indeed N!.

Problems
1.1. (a) Show that, for two large systems in thermal contact, the number (0)(E(0), E1) of Section 1.2 can be expressed as a Gaussian in the variable E1. Determine the root-mean-square deviation of E1 from the mean value E1 in terms of other quantities pertaining to the problem.
(b) Make an explicit evaluation of the root-mean-square deviation of E1 in the special case when the systems A1 and A2 are ideal classical gases.
1.2. Assuming that the entropy S and the statistical number of a physical system are related through an arbitrary functional form

S = f ( ),

show that the additive character of S and the multiplicative character of necessarily require that the function f ( ) be of the form (1.2.6). 1.3. Two systems A and B, of identical composition, are brought together and allowed to exchange both energy and particles, keeping volumes VA and VB constant. Show that the minimum value of the quantity (dEA/dNA) is given by
µATB − µBTA , TB − TA
where the µ’s and the T ’s are the respective chemical potentials and temperatures. 1.4. In a classical gas of hard spheres (of diameter D), the spatial distribution of the particles is no
longer uncorrelated. Roughly speaking, the presence of n particles in the system leaves only a volume (V − nv0) available for the (n + 1)th particle; clearly, v0 would be proportional to D3. Assuming that Nv0 V , determine the dependence of (N, V , E) on V (compare to equation (1.4.1)) and show that, as a result of this, V in the ideal-gas law (1.4.3) gets replaced by (V − b), where b is four times the actual volume occupied by the particles. 1.5. Read Appendix A and establish formulae (1.4.15) and (1.4.16). Estimate the importance of the linear term in these formulae, relative to the main term (π/6)ε∗3/2, for an oxygen molecule conﬁned to a cube of side 10 cm; take ε = 0.05 eV. 1.6. A cylindrical vessel 1 m long and 0.1 m in diameter is ﬁlled with a monatomic gas at P = 1 atm and T = 300 K. The gas is heated by an electrical discharge, along the axis of the vessel, which releases an energy of 104 joules. What will the temperature of the gas be immediately after the discharge? 1.7. Study the statistical mechanics of an extreme relativisitic gas characterized by the single-particle energy states

ε(nx, ny, nz)

=

hc 2L

n2x + n2y + n2z

1/2
,

instead of (1.4.5), along the lines followed in Section 1.4. Show that the ratio CP/CV in this case is 4/3, instead of 5/3.
1.8. Consider a system of quasiparticles whose energy eigenvalues are given by

ε(n) = nhν; n = 0, 1, 2, . . . .

Problems 23

Obtain an asymptotic expression for the number of this system for a given number N of the quasiparticles and a given total energy E. Determine the temperature T of the system as a function of E/N and hν, and examine the situation for which E/(Nhν) 1. 1.9. Making use of the fact that the entropy S(N, V , E) of a thermodynamic system is an extensive quantity, show that

∂S

∂S

∂S

N

∂N

+V
V ,E

∂V

+E
N ,E

∂E

= S.
N ,V

Note that this result implies that (−Nµ + PV + E)/T = S, that is, Nµ = E + PV − TS. 1.10. A mole of argon and a mole of helium are contained in vessels of equal volume. If argon is at 300 K,
what should the temperature of helium be so that the two have the same entropy? 1.11. Four moles of nitrogen and one mole of oxygen at P = 1 atm and T = 300 K are mixed together to
form air at the same pressure and temperature. Calculate the entropy of mixing per mole of the air formed. 1.12. Show that the various expressions for the entropy of mixing, derived in Section 1.5, satisfy the following relations: (a) For all N1, V1, N2, and V2,

( S)1≡2 = {( S) − ( S)∗} ≥ 0,

the equality holding when and only when N1/V1 = N2/V2. (b) For a given value of (N1 + N2),

( S)∗ ≤ (N1 + N2)k ln 2,

the equality holding when and only when N1 = N2. 1.13. If the two gases considered in the mixing process of Section 1.5 were initially at different
temperatures, say T1 and T2, what would the entropy of mixing be in that case? Would the contribution arising from this cause depend on whether the two gases were different or identical? 1.14. Show that for an ideal gas composed of monatomic molecules the entropy change, between any two temperatures, when the pressure is kept constant is 5/3 times the corresponding entropy change when the volume is kept constant. Verify this result numerically by calculating the actual values of ( S)P and ( S)V per mole of an ideal gas whose temperature is raised from 300 K to 400 K. 1.15. We have seen that the (P, V )-relationship during a reversible adiabatic process in an ideal gas is governed by the exponent γ , such that

PV γ = const.

Consider a mixture of two ideal gases, with mole fractions f1 and f2 and respective exponents γ1 and γ2. Show that the effective exponent γ for the mixture is given by

γ

1 −1

=

f1 γ1 − 1

+

f2 γ2 −

1

.

1.16. Establish thermodynamically the formulae

∂P

∂P

V

∂T

=S
µ

and

V

∂µ

= N.
T

Express the pressure P of an ideal classical gas in terms of the variables µ and T , and verify the above formulae.

2
Elements of Ensemble Theory

In the preceding chapter we noted that, for a given macrostate (N, V , E), a statistical system, at any time t, is equally likely to be in any one of an extremely large number of distinct microstates. As time passes, the system continually switches from one microstate to another, with the result that, over a reasonable span of time, all one observes is a behavior “averaged” over the variety of microstates through which the system passes. It may, therefore, make sense if we consider, at a single instant of time, a rather large number of systems — all being some sort of “mental copies” of the given system — which are characterized by the same macrostate as the original system but are, naturally enough, in all sorts of possible microstates. Then, under ordinary circumstances, we may expect that the average behavior of any system in this collection, which we call an ensemble, would be identical to the time-averaged behavior of the given system. It is on the basis of this expectation that we proceed to develop the so-called ensemble theory.
For classical systems, the most appropriate framework for developing the desired formalism is provided by the phase space. Accordingly, we begin our study of the various ensembles with an analysis of the basic features of this space.

2.1 Phase space of a classical system

The microstate of a given classical system, at any time t, may be deﬁned by specifying the
instantaneous positions and momenta of all the particles constituting the system. Thus,
if N is the number of particles in the system, the deﬁnition of a microstate requires the speciﬁcation of 3N position coordinates q1, q2, . . . , q3N and 3N momentum coordinates p1, p2, . . . , p3N . Geometrically, the set of coordinates (qi, pi), where i = 1, 2, . . . , 3N, may be regarded as a point in a space of 6N dimensions. We refer to this space as the phase space, and the phase point (qi, pi) as a representative point, of the given system.
Of course, the coordinates qi and pi are functions of the time t; the precise manner in which they vary with t is determined by the canonical equations of motion,

q˙ i p˙ i

= =

∂H(qi, pi) ∂ pi
∂H(qi, pi) − ∂qi


   
i
   

=

1,

2,

.

.

.

,

3N ,

(1)

where H(qi, pi) is the Hamiltonian of the system. Now, as time passes, the set of coordinates (qi, pi), which also deﬁnes the microstate of the system, undergoes a continual change. Correspondingly, our representative point in the phase space carves out a

. Statistical Mechanics DOI: 10.1016/B978-0-12-382188-1.00002-5

25

© 2011 Elsevier Ltd. All rights reserved.

. 26 Chapter 2 Elements of Ensemble Theory

trajectory whose direction, at any time t, is determined by the velocity vector v ≡ (q˙ i, p˙ i), which in turn is given by the equations of motion (1). It is not difﬁcult to see that the
trajectory of the representative point must remain within a limited region of the phase
space; this is so because a ﬁnite volume V directly limits the values of the coordinates qi, while a ﬁnite energy E limits the values of both the qi and the pi [through the Hamiltonian H(qi, pi)]. In particular, if the total energy of the system is known to have a precise value, say E, the corresponding trajectory will be restricted to the “hypersurface”

H(qi, pi) = E

(2)

of the phase space; on the other hand, if the total energy may lie anywhere in the range

E

−

1 2

,

E

+

1 2

, the corresponding trajectory will be restricted to the “hypershell”

deﬁned by these limits.

Now, if we consider an ensemble of systems (i.e., the given system, along with a large

number of mental copies of it) then, at any time t, the various members of the ensem-

ble will be in all sorts of possible microstates; indeed, each one of these microstates must

be consistent with the given macrostate that is supposed to be common to all members

of the ensemble. In the phase space, the corresponding picture will consist of a swarm of

representative points, one for each member of the ensemble, all lying within the “allowed”

region of this space. As time passes, every member of the ensemble undergoes a continual

change of microstates; correspondingly, the representative points constituting the swarm

continually move along their respective trajectories. The overall picture of this movement

possesses some important features that are best illustrated in terms of what we call a

density function ρ(q, p; t).1 This function is such that, at any time t, the number of representative points in the “volume element” (d3N q d3N p) around the point (q, p) of the phase space is given by the product ρ(q, p; t)d3N q d3N p. Clearly, the density function ρ(q, p; t)

symbolizes the manner in which the members of the ensemble are distributed over all

possible microstates at different instants of time. Accordingly, the ensemble average f of a

given physical quantity f (q, p), which may be different for systems in different microstates,

would be given by

f (q, p)ρ(q, p; t)d3N q d3N p

f=

ρ(q, p; t)d3N q d3N p .

(3)

The integrations in (3) extend over the whole of the phase space; however, it is only the populated regions of the phase space (ρ = 0) that really contribute. We note that, in general, the ensemble average f may itself be a function of time.
An ensemble is said to be stationary if ρ does not depend explicitly on time, that is, at all times

∂ρ

∂t = 0.

(4)

1Note that (q, p) is an abbreviation of (qi, pi) ≡ (q1, . . . , q3N , p1, . . . , p3N ).

2.2 Liouville’s theorem and its consequences 27

Clearly, for such an ensemble the average value f of any physical quantity f (q, p) will be independent of time. Naturally, a stationary ensemble qualiﬁes to represent a system in equilibrium. To determine the circumstances under which equation (4) may hold, we have to make a rather detailed study of the movement of the representative points in the phase space.

2.2 Liouville’s theorem and its consequences

Consider an arbitrary “volume” ω in the relevant region of the phase space and let the “surface” enclosing this volume be denoted by σ ; see Figure 2.1. Then, the rate at which the number of representative points in this volume increases with time is written as

∂ ∂t

ρ dω,

(1)

ω

where dω ≡ d3N q d3N p . On the other hand, the net rate at which the representative points “ﬂow” out of ω (across the bounding surface σ ) is given by

ρ v · nˆ dσ ;

(2)

σ

here, v is the velocity vector of the representative points in the region of the surface element dσ while nˆ is the (outward) unit vector normal to this element. By the divergence theorem, (2) can be written as

div(ρv)dω;

(3)

ω

of course, the operation of divergence here means

3N ∂

∂

div(ρv) ≡
i=1

∂qi (ρq˙ i) + ∂pi (ρp˙ i) .

(4)

v

n^ d␴

␻

vdt

␴ FIGURE 2.1 The “hydrodynamics” of the representative points in the phase space.

. 28 Chapter 2 Elements of Ensemble Theory

In view of the fact that there are no “sources” or “sinks” in the phase space and hence the total number of representative points remains conserved,2 we have, by (1) and (3),

∂

∂t ρ dω = − div(ρv)dω,

(5)

ω

ω

that is,

∂ρ ∂t

+

div(ρv)

dω = 0.

(6)

ω

Now, the necessary and sufﬁcient condition that integral (6) vanish for all arbitrary volumes ω is that the integrand itself vanish everywhere in the relevant region of the phase space. Thus, we must have

∂ρ ∂t

+

div(ρv)

=

0,

(7)

which is the equation of continuity for the swarm of the representative points. Combining (4) and (7), we obtain

∂ρ 3N ∂t +
i=1

∂ρ

∂ρ

∂qi q˙ i + ∂pi p˙ i

3N
+ρ
i=1

∂q˙ i ∂p˙ i ∂qi + ∂pi

= 0.

(8)

The last group of terms vanishes identically because, by the equations of motion, we have, for all i,

∂q˙ i ∂ qi

=

∂2H(qi, pi) ∂ qi ∂ pi

≡

∂2H(qi, pi) ∂ pi ∂ qi

=

−

∂ ∂

p˙ i pi

.

(9)

Further, since ρ ≡ ρ (q, p; t), the remaining terms in (8) may be combined to form the “total” time derivative of ρ, with the result that

dρ dt

=

∂ρ ∂t

+ [ρ, H] = 0.

(10)3

Equation (10) embodies Liouville’s theorem (1838). According to this theorem, the “local” density of the representative points, as viewed by an observer moving with a representative point, stays constant in time. Thus, the swarm of the representative points moves in

2This means that in the ensemble under consideration neither are any new members being added nor are any old ones being removed.
3We recall that the Poisson bracket [ρ, H] stands for the sum
3N ∂ρ ∂H ∂ρ ∂H i=1 ∂qi ∂pi − ∂pi ∂qi ,
which is identical to the group of terms in the middle of (8).

2.2 Liouville’s theorem and its consequences 29

the phase space in essentially the same manner as an incompressible ﬂuid moves in the physical space!
A distinction must be made, however, between equation (10) on one hand and equation (2.1.4) on the other. While the former derives from the basic mechanics of the particles and is therefore quite generally true, the latter is only a requirement for equilibrium which, in a given case, may or may not be satisﬁed. The condition that ensures simultaneous validity of the two equations is clearly

3N ∂ρ

∂ρ

[ρ, H] =
i=1

∂qi q˙ i + ∂pi p˙ i

= 0.

(11)

Now, one possible way of satisfying (11) is to assume that ρ, which is already assumed to have no explicit dependence on time, is independent of the coordinates (q, p) as well, that is,

ρ(q, p) = const.

(12)

over the relevant region of the phase space (and, of course, is zero everywhere else). Physically, this choice corresponds to an ensemble of systems that at all times are uniformly distributed over all possible microstates. The ensemble average (2.1.3) then reduces to

1 f =ω

f (q, p)dω;

(13)

ω

here, ω denotes the total “volume” of the relevant region of the phase space. Clearly, in this case, any member of the ensemble is equally likely to be in any one of the various possible microstates, inasmuch as any representative point in the swarm is equally likely to be in the neighborhood of any phase point in the allowed region of the phase space. This statement is usually referred to as the postulate of “equal a priori probabilities” for the various possible microstates (or for the various volume elements in the allowed region of the phase space); the resulting ensemble is referred to as the microcanonical ensemble.
A more general way of satisfying (11) is to assume that the dependence of ρ on (q, p) comes only through an explicit dependence on the Hamiltonian H(q, p), that is,

ρ(q, p) = ρ[H(q, p)];

(14)

condition (11) is then identically satisﬁed. Equation (14) provides a class of density functions for which the corresponding ensemble is stationary. In Chapter 3 we shall see that the most natural choice in this class of ensembles is the one for which

ρ(q, p) ∝ exp[−H(q, p)/kT ].

(15)

The ensemble so deﬁned is referred to as the canonical ensemble.

. 30 Chapter 2 Elements of Ensemble Theory

2.3 The microcanonical ensemble

In this ensemble the macrostate of a system is deﬁned by the number of molecules N,

the volume V , and the energy E. However, in view of the considerations expressed in

Section 1.4, we may prefer to specify a range of energy values, say from

E

−

1 2

to

E

+

1 2

, rather than a sharply deﬁned value E. With the macrostate speciﬁed, a choice

still remains for the systems of the ensemble to be in any one of a large number of pos-

sible microstates. In the phase space, correspondingly, the representative points of the

ensemble have a choice to lie anywhere within a “hypershell” deﬁned by the condition

1 E− 2

≤ H(q, p) ≤

1 E+ 2

.

(1)

The volume of the phase space enclosed within this shell is given by

ω = dω ≡ d3N q d3N p ,

(2)

where the primed integration extends only over that part of the phase space which conforms to condition (1). It is clear that ω will be a function of the parameters N, V , E, and .
Now, the microcanonical ensemble is a collection of systems for which the density function ρ is, at all times, given by

ρ(q, p) = const.

if

E

−

1 2

≤ H(q, p) ≤

E

+

1 2

 

.

(3)

0

otherwise



Accordingly, the expectation value of the number of representative points lying in a volume element dω of the relevant hypershell is simply proportional to dω. In other words, the a priori probability of ﬁnding a representative point in a given volume element dω is the same as that of ﬁnding a representative point in an equivalent volume element dω located anywhere in the hypershell. In our original parlance, this means an equal a priori probability for a given member of the ensemble to be in any one of the various possible microstates. In view of these considerations, the ensemble average f , as given by equation (2.2.13), acquires a simple physical meaning. To see this, we proceed as follows.
Since the ensemble under study is a stationary one, the ensemble average of any physical quantity f will be independent of time; accordingly, taking a time average thereof will not produce any new result. Thus

f ≡ the ensemble average of f = the time average of (the ensemble average of f ).

2.3 The microcanonical ensemble 31

Now, the processes of time averaging and ensemble averaging are completely independent, so the order in which they are performed may be reversed without causing any change in the value of f . Thus

f = the ensemble average of (the time average of f ).
Now, the time average of any physical quantity, taken over a sufﬁciently long interval of time, must be the same for every member of the ensemble, for after all we are dealing with only mental copies of a given system.4 Therefore, taking an ensemble average thereof should be inconsequential, and we may write

f = the long-time average of f ,
where the latter may be taken over any member of the ensemble. Furthermore, the longtime average of a physical quantity is all one obtains by making a measurement of that quantity on the given system; therefore, it may be identiﬁed with the value one expects to obtain through experiment. Thus, we ﬁnally have

f = fexp.

(4)

This brings us to the most important result: the ensemble average of any physical quantity f is identical to the value one expects to obtain on making an appropriate measurement on the given system.
The next thing we look for is the establishment of a connection between the mechanics of the microcanonical ensemble and the thermodynamics of the member systems. To do this, we observe that there exists a direct correspondence between the various microstates of the given system and the various locations in the phase space. The volume ω (of the allowed region of the phase space) is, therefore, a direct measure of the multiplicity of the microstates accessible to the system. To establish a numerical correspondence between ω

4To provide a rigorous justiﬁcation for this assertion is not trivial. One can readily see that if, for any particular member of the ensemble, the quantity f is averaged only over a short span of time, the result is bound to depend on the relevant “subset of microstates” through which the system passes during that time. In the phase space, this will mean an averaging over only a “part of the allowed region.” However, if we employ instead a sufﬁciently long interval of time, the system may be expected to pass through almost all possible microstates “without fear or favor”; consequently, the result of the averaging process would depend only on the macrostate of the system, and not on a subset of microstates. Correspondingly, the averaging in the phase space would go over practically all parts of the allowed region, again “without fear or favor.” In other words, the representative point of our system will have traversed each and every part of the allowed region almost uniformly. This statement embodies the so-called ergodic theorem or ergodic hypothesis, which was ﬁrst introduced by Boltzmann (1871). According to this hypothesis, the trajectory of a representative point passes, in the course of time, through each and every point of the relevant region of the phase space. A little reﬂection, however, shows that the statement as such requires a qualiﬁcation; we better replace it by the so-called quasiergodic hypothesis, according to which the trajectory of a representative point traverses, in the course of time, any neighborhood of any point of the relevant region. For further details, see ter Haar (1954, 1955), Farquhar (1964).
Now, when we consider an ensemble of systems, the foregoing statement should hold for every member of the ensemble; thus, irrespective of the initial (and ﬁnal ) states of the various systems, the long-time average of any physical quantity f should be the same for every member system.

. 32 Chapter 2 Elements of Ensemble Theory

and , we need to discover a fundamental volume ω0 that could be regarded as “equivalent to one microstate.” Once this is done, we may say that, asymptotically,

= ω/ω0.

(5)

The thermodynamics of the system would then follow in the same way as in Sections 1.2– 1.4, namely through the relationship

S = k ln = k ln(ω/ω0), etc.

(6)

The basic problem then consists in determining ω0. From dimensional considerations, see (2), ω0 must be in the nature of an “angular momentum raised to the power 3N.” To determine it exactly, we consider certain simpliﬁed systems, both from the point of view
of the phase space and from the point of view of the distribution of quantum states.

2.4 Examples
We consider, ﬁrst of all, the problem of a classical ideal gas composed of monatomic particles; see Section 1.4. In the microcanonical ensemble, the volume ω of the phase space accessible to the representative points of the (member) systems is given by

ω = . . . d3N q d3N p ,

(1)

where the integrations are restricted by the conditions that (i) the particles of the system

are conﬁned in physical space to volume V , and (ii) the total energy of the system lies

between the limits

E

−

1 2

and

E

+

1 2

. Since the Hamiltonian in this case is a function

of the pi alone, integrations over the qi can be carried out straightforwardly; these give a

factor of V N . The remaining integral is

E

−

1 2

...

3N ≤

p2i /2m ≤

E

+

1 2

i=1

d3N p =

2m

E−

1 2

...

3N ≤

yi2 ≤2m

E+

1 2

i=1

d3N y,

which is equal to the volume of a 3N-dimensional hypershell, bounded by hyperspheres of radii

1 2m E + 2

1

and

2m E − 2

.

For

E, this is given by the thickness of the shell, which is almost equal to (m/2E)1/2,

multiplied by the surface area of a 3N-dimensional hypersphere of radius √(2mE). By

2.4 Examples 33

equation (7) of Appendix C, we obtain for this integral

m 1/2 2E

2π 3N [(3N /2)

/2
−

1]!

(2mE

)(3N

−1)/2

,

which gives

ω

E

V

N

(2π mE)3N/2 [(3N/2) − 1]!

.

(2)

Comparing (2) with (1.4.17 and 1.4.17a), we obtain the desired correspondence, namely

(ω/ )asymp ≡ ω0 = h3N ;

see also Problem 2.9. Quite generally, if the system under study has N degrees of freedom, the desired conversion factor is

ω0 = hN .

(3)

In the case of a single particle, N = 3; accordingly, the number of microstates available would asymptotically be equal to the volume of the allowed region of the phase space divided by h3. Let (P) denote the number of microstates available to a free particle conﬁned to volume V of the physical space, its momentum p being less than or equal to a speciﬁed value P. Then

(P)

≈

1 h3

...

d3q d3p

=

V h3

4π 3

P3,

(4)

p≤P

from which we obtain for the number of microstates with momentum lying between p and p + dp

g (p)dp

=

d

(p) dp
dp

≈

V h3

4π p2dp.

(5)

Expressed in terms of the particle energy, these expressions assume the form

(E)

≈

V h3

4π 3

(2mE)3/2

(6)

and

a(ε)dε

=

d

(ε) dε

dε

≈

V h3

2π (2m)3/2 ε1/2 dε.

(7)

The next case we consider here is that of a one-dimensional simple harmonic oscillator. The classical expression for the Hamiltonian of this system is

H(q, p)

=

1 kq2 2

+

1 2m

p2,

(8)

. 34 Chapter 2 Elements of Ensemble Theory

where k is the spring constant and m the mass of the oscillating particle. The space coordinate q and the momentum coordinate p of the system are given by

q = A cos(ωt + φ), p = mq˙ = −mωA sin(ωt + φ),

(9)

A being the amplitude and ω the (angular) frequency of vibration:

ω = √(k/m).

(10)

The energy of the oscillator is a constant of the motion, and is given by

E

=

1 mω2A2. 2

(11)

The phase-space trajectory of the representative point (q, p) of this system is determined by eliminating t between expressions (9) for q(t) and p(t); we obtain

q2

p2

(2E/mω2) + (2mE) = 1,

(12)

which is an ellipse, with axes proportional to √E and hence area proportional to E; to be

precise, the area of this ellipse is 2πE/ω. Now, if we restrict the oscillator energy to the

interval

E

−

1 2

,E

+

1 2

, its representative point in the phase space will be conﬁned to

the region bounded by elliptical trajectories corresponding to the energy values

E

+

1 2

and

E

−

1 2

. The “volume” (in this case, the area) of this region will be

...

2π (dq dp) =

E

+

1 2

ω

2π

E

−

1 2

−

ω

2π = ω.

(13)

E−

1 2

≤H (q,p)≤

E

+

1 2

According to quantum mechanics, the energy eigenvalues of the harmonic oscillator are given by

1 En = n + 2

ω; n = 0, 1, 2, . . .

(14)

In terms of phase space, one could say that the representative point of the system must

move along one of the “chosen” trajectories, as shown in Figure 2.2; the area of the phase

space between two consecutive trajectories, for which = ω, is simply 2π .5 For arbitrary

values of E and , such that E

ω, the number of eigenstates within the allowed

5Strictly speaking, the very concept of phase space is invalid in quantum mechanics because there, in principle, it is wrong to assign to a particle the coordinates q and p simultaneously. Nevertheless, the ideas discussed here are tenable in the correspondence limit.

2.5 Quantum states and the phase space 35

Ϫ2q0

p 2p0
p0

nϭ3 nϭ2 nϭ1
nϭ0

Ϫq0

q0

Ϫp0

q 2q0

Ϫ2p0

FIGURE 2.2 Eigenstates of a linear harmonic oscillator, in relation to its phase space.

energy interval is very nearly equal to / ω. Hence, the area of the phase space equivalent to one eigenstate is, asymptotically, given by

ω0 = (2π /ω)/( / ω) = 2π = h.

(15)

If, on the other hand, we consider a system of N harmonic oscillators along the same lines as above, we arrive at the result: ω0 = hN (see Problem 2.7). Thus, our ﬁndings in these cases are consistent with our earlier result (3).

2.5 Quantum states and the phase space
At this stage we would like to say a few words on the central role played here by the Planck constant h. The best way to appreciate this role is to recall the implications of the Heisenberg uncertainty principle, according to which we cannot specify simultaneously both the position and the momentum of a particle exactly. An element of uncertainty is inherently present and can be expressed as follows: assuming that all conceivable uncertainties of measurement are eliminated, even then, by the very nature of things, the product of the uncertainties q and p in the simultaneous measurement of the canonically conjugate coordinates q and p would be of order :

( q p)min ∼ .

(1)

Thus, it is impossible to deﬁne the position of a representative point in the phase space of the given system more accurately than is allowed by condition (1). In other words, around any point (q, p) in the (two-dimensional) phase space, there exists an area of order within

. 36 Chapter 2 Elements of Ensemble Theory

which the position of the representative point cannot be pinpointed. In a phase space of 2N dimensions, the corresponding “volume of uncertainty” around any point would be of order N . Therefore, it seems reasonable to regard the phase space as made up of elementary cells, of volume ∼ N , and to consider the various positions within such a cell as nondistinct. These cells could then be put into one-to-one correspondence with the quantum-mechanical states of the system.
It is, however, obvious that considerations of uncertainty alone cannot give us the exact value of the conversion factor ω0. This could only be done by an actual counting of microstates on one hand and a computation of volume of the relevant region of the phase space on the other, as was done in the examples of the previous section. Clearly, a procedure along these lines could not be possible until after the work of Schro¨dinger and others. Historically, however, the ﬁrst to establish the result (2.4.3) was Tetrode (1912) who, in his well-known work on the chemical constant and the entropy of a monatomic gas, assumed that

ω0 = (zh)N ,

(2)

where z was supposed to be an unknown numerical factor. Comparing theoretical results with the experimental data on mercury, Tetrode found that z was very nearly equal to unity; from this he concluded that “it seems rather plausible that z is exactly equal to unity, as has already been taken by O. Sackur (1911).”6
In the extreme relativistic limit, the same result was established by Bose (1924). In his famous treatment of the photon gas, Bose made use of Einstein’s relationship between the momentum of a photon and the frequency of the associated radiation, namely

p

=

hν c

,

(3)

and observed that, for a photon conﬁned to a three-dimensional cavity of volume V , the relevant “volume” of the phase space,

(d3q d3p) = V 4π p2dp = V (4π h3ν2/c3)dν,

(4)

would correspond exactly to the Rayleigh expression,

V (4π ν2/c3)dν,

(5)

for the number of normal modes of a radiation oscillator, provided that we divide phase space into elementary cells of volume h3 and put these cells into one-to-one correspondence with the vibrational modes of Rayleigh. It may, however, be added that a two-fold multiplicity of these states (g = 2) arises from the spin orientations of the photon
6For a more satisfactory proof of this result, see Section 5.5, especially equation (5.5.22).

Problems 37

(or from the states of polarization of the vibrational modes); this requires a multiplication of both expressions (4) and (5) by a factor of 2, leaving the conversion factor h3
unchanged.

Problems

2.1. Show that the volume element

3N
dω = (dqi dpi)
i=1

of the phase space remains invariant under a canonical transformation of the (generalized)

coordinates (q, p) to any other set of (generalized) coordinates (Q, P).

[Hint: Before considering the most general transformation of this kind, which is referred to as a

contact transformation, it may be helpful to consider a point transformation — one in which the

new coordinates Qi and the old coordinates qi transform only among themselves.] 2.2. (a) Verify explicitly the invariance of the volume element dω of the phase space of a single particle

under transformation from the Cartesian coordinates x, y, z, px, py, pz to the spherical polar coordinates (r, θ , φ, pr, pθ , pφ).

(b) The foregoing result seems to contradict the intuitive notion of “equal weights for equal solid

angles,” because the factor sin θ is invisible in the expression for dω. Show that if we average

out any physical quantity, whose dependence on pθ and pφ comes only through the kinetic

energy of the particle, then as a result of integration over these variables we do indeed recover

the factor sin θ to appear with the subelement (dθ dφ).

2.3. Starting with the line of zero energy and working in the (two-dimensional) phase space of a classical

rotator, draw lines of constant energy that divide phase space into cells of “volume” h. Calculate the

energies of these states and compare them with the energy eigenvalues of the corresponding

quantum-mechanical rotator.

2.4. By evaluating the “volume” of the relevant region of its phase space, show that the number of

microstates available to a rigid rotator with angular momentum ≤ M is (M/ )2. Hence determine

the number of microstates that may be associated with the quantized angular momentum

Mj

=

√ {

j(

j

+

1)}

,

where

j

=

0, 1, 2, . . .

or

1 2

,

3 2

,

5 2

,

.

.

..

Interpret

the

result

physically.

[Hint: It simpliﬁes to consider motion in the variables θ and ϕ, with M2 = p2θ + (pφ/ sin θ )2.]

2.5. Consider a particle of energy E moving in a one-dimensional potential well V (q), such that

dV m
dq

{m(E − V )}3/2.

Show that the allowed values of the momentum p of the particle are such that

1 p dq = n + 2 h,

where n is an integer.

2.6. The generalized coordinates of a simple pendulum are the angular displacement θ and the angular momentum ml2θ˙. Study, both mathematically and graphically, the nature of the corresponding

trajectories in the phase space of the system, and show that the area A enclosed by a trajectory is

equal to the product of the total energy E and the time period τ of the pendulum.

2.7. Derive (i) an asymptotic expression for the number of ways in which a given energy E can be

distributed among a set of N one-dimensional harmonic oscillators, the energy eigenvalues of the

oscillators being

n

+

1 2

ω; n = 0, 1, 2, . . ., and (ii) the corresponding expression for the “volume” of

the relevant region of the phase space of this system. Establish the correspondence between the two results, showing that the conversion factor ω0 is precisely hN .

. 38 Chapter 2 Elements of Ensemble Theory

2.8. Following the method of Appendix C, replacing equation (C.4) by the integral

∞
e−r r2dr = 2,

0

show that

N

V3N = . . .

N

i=1

0≤ ri≤R

i=1

4π ri2dri

= (8π R3)N /(3N)! .

Using this result, compute the “volume” of the relevant region of the phase space of an extreme relativistic gas (ε = pc) of N particles moving in three dimensions. Hence, derive expressions for the various thermodynamic properties of this system and compare your results with those of Problem 1.7. 2.9. (a) Solve the integral

. . . (dx1 . . . dx3N )
3N
0≤ |xi|≤R
i=1
and use it to determine the “volume” of the relevant region of the phase space of an extreme relativistic gas (ε = pc) of 3N particles moving in one dimension. Determine, as well, the number of ways of distributing a given energy E among this system of particles and show that, asymptotically, ω0 = h3N . (b) Compare the thermodynamics of this system with that of the system considered in Problem 2.8.

3
The Canonical Ensemble

In the preceding chapter we established the basis of ensemble theory and made

a somewhat detailed study of the microcanonical ensemble. In that ensemble the

macrostate of the systems was deﬁned through a ﬁxed number of particles N, a ﬁxed vol-

ume

V,

and

a

ﬁxed

energy

E

[or,

preferably,

a

ﬁxed

energy

range

(E

−

1 2

,

E

+

1 2

)]. The

basic problem then consisted in determining the number (N, V , E), or (N, V , E; ), of

distinct microstates accessible to the system. From the asymptotic expressions of these

numbers, complete thermodynamics of the system could be derived in a straightforward

manner. However, for most physical systems, the mathematical problem of determin-

ing these numbers is quite formidable. For this reason alone, a search for an alternative

approach within the framework of the ensemble theory seems necessary.

Practically, too, the concept of a ﬁxed energy (or even an energy range) for a system

belonging to the real world does not appear satisfactory. For one thing, the total energy

E of a system is hardly ever measured; for another, it is hardly possible to keep its value

under strict physical control. A far better alternative appears to be to speak of a ﬁxed tem-

perature T of the system — a parameter that is not only directly observable (by placing a

“thermometer” in contact with the system) but also controllable (by keeping the system

in contact with an appropriate “heat reservoir”). For most purposes, the precise nature of

the reservoir is not very relevant; all one needs is that it should have an inﬁnitely large

heat capacity, so that, irrespective of energy exchange between the system and the reser-

voir, an overall constant temperature can be maintained. Now, if the reservoir consists of

an inﬁnitely large number of mental copies of the given system we have once again an

ensemble of systems — this time, however, it is an ensemble in which the macrostate of

the systems is deﬁned through the parameters N, V , and T . Such an ensemble is referred

to as a canonical ensemble.

In the canonical ensemble, the energy E of a system is variable; in principle, it can

take values anywhere between zero and inﬁnity. The question then arises: what is the

probability that, at any time t, a system in the ensemble is found to be in one of the states

characterized by the energy value Er?1 We denote this probability by the symbol Pr. Clearly,

there are two ways in which the dependence of Pr on Er can be determined. One consists

of regarding the system as in equilibrium with a heat reservoir at a common temperature T

and studying the statistics of the energy exchange between the two. The other consists of

regarding the system as a member of a canonical ensemble (N, V , T ), in which an energy

E is being shared by N identical systems constituting the ensemble, and studying the

1In what follows, the energy levels Er appear as purely mechanical quantities — independent of the temperature of the system. For a treatment involving “temperature-dependent energy levels,” see Elcock and Landsberg (1957).

. Statistical Mechanics DOI: 10.1016/B978-0-12-382188-1.00003-7

39

© 2011 Elsevier Ltd. All rights reserved.

. 40 Chapter 3 The Canonical Ensemble

statistics of this sharing process. We expect that in the thermodynamic limit the ﬁnal result in either case would be the same. Once Pr is determined, the rest follows without difﬁculty.

3.1 Equilibrium between a system and a heat reservoir

We consider the given system A, immersed in a very large heat reservoir A ; see Figure 3.1.
On attaining a state of mutual equilibrium, the system and the reservoir would have a
common temperature, say T. Their energies, however, would be variable and, in principle, could have, at any time t, values lying anywhere between 0 and E(0), where E(0) denotes the energy of the composite system A(0)(≡ A + A ). If, at any particular instant of time, the
system A happens to be in a state characterized by the energy value Er, then the reservoir would have an energy Er, such that

Er + Er = E(0) = const.

(1)

Of course, since the reservoir is supposed to be much larger than the given system, any practical value of Er would be a very small fraction of E(0); therefore, for all practical
purposes,

Er E(0) =

1

−

Er E (0)

1.

(2)

With the state of the system A having been speciﬁed, the reservoir A can still be in any
one of a large number of states compatible with the energy value Er. Let the number of these states be denoted by (Er). The prime on the symbol emphasizes the fact that its functional form will depend on the nature of the reservoir; of course, the details of
this dependence are not going to be of any particular relevance to our ﬁnal results. Now,
the larger the number of states available to the reservoir, the larger the probability of the
reservoir assuming that particular energy value Er (and, hence, of the system A assuming the corresponding energy value Er). Moreover, since the various possible states (with a given energy value) are equally likely to occur, the relevant probability would be directly
proportional to this number; thus,

Pr ∝ (Er) ≡ (E(0) − Er).

(3)

A9 (Er9;T )

A (Er ;T )

FIGURE 3.1 A given system A immersed in a heat reservoir A ; in equilibrium, the two have a common temperature T .

3.2 A system in the canonical ensemble 41

In view of (2), we may carry out an expansion of (3) around the value Er = E(0), that is, around Er = 0. However, for reasons of convergence, it is essential to effect the expansion
of its logarithm instead:

ln

(Er) = ln

(E(0)) +

∂ ln ∂E

(Er − E(0)) + · · ·
E =E(0)

const − β Er,

(4)

where use has been made of formula (1.2.3), whereby

∂ ln ∂E

≡ β;
N ,V

(5)

note that, in equilibrium, β = β = 1/kT . From (3) and (4), we obtain the desired result:

Pr ∝ exp(−βEr).

(6)

Normalizing (6), we get

Pr =

exp(−βEr) , exp(−β Er )

(7)

r

where the summation in the denominator goes over all states accessible to the system A. We note that our ﬁnal result (7) bears no relation whatsoever to the physical nature of the reservoir A .
We now examine the same problem from the ensemble point of view.

3.2 A system in the canonical ensemble
We consider an ensemble of N identical systems (which may be labelled as 1, 2, . . . , N ), sharing a total energy E ; let Er(r = 0, 1, 2, . . .) denote the energy eigenvalues of the systems. If nr denotes the number of systems which, at any time t, have the energy value Er, then the set of numbers {nr} must satisfy the obvious conditions

nr = N

 



r

(1)

nr Er

=

E

=

N

U , 

r

where U(= E /N ) denotes the average energy per system in the ensemble. Any set {nr} that satisﬁes the restrictive conditions (1) represents a possible mode of distribution of the total energy E among the N members of the ensemble. Furthermore, any such mode can be realized in a number of ways, for we may effect a reshufﬂe among those members of the ensemble for which the energy values are different and thereby obtain a state of the

. 42 Chapter 3 The Canonical Ensemble

ensemble that is distinct from the original one. Denoting the number of different ways of doing so by the symbol W {nr}, we have

N!

W {nr} = n0! n1! n2! . . . .

(2)

In view of the fact that all possible states of the ensemble, which are compatible with conditions (1), are equally likely to occur, the frequency with which the distribution set {nr} may appear will be directly proportional to the number W {nr}. Accordingly, the “most probable” mode of distribution will be the one for which the number W is a maximum. We denote the corresponding distribution set by {n∗r }; clearly, the set {n∗r } must also satisfy conditions (1). As will be seen in the sequel, the probability of appearance of other modes of distribution, however little they may differ from the most probable mode, is extremely low! Therefore, for all practical purposes, the most probable distribution set {n∗r } is the only one we have to contend with.
However, unless this has been mathematically demonstrated, one must take into account all possible modes of distribution, as characterized by the various distribution sets {nr}, along with their respective weight factors W {nr}. Accordingly, the expectation values, or mean values, nr of the numbers nr would be given by

nrW {nr}

nr

{nr }
=

W {nr}

,

(3)

{nr }

where the primed summations go over all distribution sets that conform to conditions (1).
In principle, the mean value nr , as a fraction of the total number N , should be a natural analog of the probability Pr evaluated in the preceding section. In practice, however, the fraction n∗r /N also turns out to be the same.
We now proceed to derive expressions for the numbers n∗r and nr , and to show that, in the limit N → ∞, they are identical.

The method of most probable values Our aim here is to determine that distribution set which, while satisfying conditions (1), maximizes the weight factor (2). For simplicity, we work with ln W instead:

ln W = ln(N ! ) − ln(nr! ).

(4)

r

Since, in the end, we propose to resort to the limit N → ∞, the values of nr (which are going to be of any practical signiﬁcance) would also, in that limit, tend to inﬁnity. It is, therefore, justiﬁed to apply the Stirling formula, ln(n! ) ≈ n ln n − n, to (4) and write

ln W = N ln N − nr ln nr.

(5)

r

3.2 A system in the canonical ensemble 43

If we shift from the set {nr} to a slightly different set {nr + δnr}, then expression (5) would change by an amount

δ(ln W ) = − (ln nr + 1)δnr.

(6)

r

Now, if the set {nr} is maximal, the variation δ(ln W ) should vanish. At the same time, in view of the restrictive conditions (1), the variations δnr themselves must satisfy the conditions

δnr = 0

 



r

(7)

Er δnr

=

0. 

r

The desired set {n∗r } is then determined by the method of Lagrange multipliers,2 by which the condition determining this set becomes

{−(ln n∗r + 1) − α − βEr}δnr = 0,

(8)

r

where α and β are the Lagrangian undetermined multipliers that take care of the restrictive conditions (7). In (8), the variations δnr become completely arbitrary; accordingly, the only way to satisfy this condition is that all its coefﬁcients must vanish identically, that is, for all r,

which gives

ln n∗r = −(α + 1) − βEr,

n∗r = C exp(−βEr),

(9)

where C is again an undetermined parameter. To determine C and β, we subject (9) to conditions (1), with the result that

n∗r N=

exp(−βEr) , exp(−β Er )

(10)

r

the parameter β being a solution of the equation

E N

=U =

Er exp(−βEr)

r

.

exp(−β Er )

r

2For the method of Lagrange multipliers, see ter Haar and Wergeland (1966, Appendix C.1).

(11)

. 44 Chapter 3 The Canonical Ensemble

Combining statistical considerations with thermodynamic ones, see Section 3.3, we can show that the parameter β here is exactly the same as the one appearing in Section 3.1, that is, β = 1/kT .

The method of mean values Here we attempt to evaluate expression (3) for nr , taking into account the weight factors (2) and the restrictive conditions (1). To do this, we replace (2) by

W˜ {nr}

=

N ! ω0n0 ω1n1 ω2n2 . . . , n0! n1! n2! . . .

(12)

with the understanding that in the end all ωr will be set equal to unity, and introduce a function

(N , U) = W˜ {nr},

(13)

{nr }

where the primed summation, as before, goes over all distribution sets that conform to conditions (1). Expression (3) can then be written as

nr

∂ = ωr ∂ωr (ln

)

.

all ωr =1

(14)

Thus, all we need to know here is the dependence of the quantity ln on the parameters ωr. Now,

(N , U) = N !
{nr }

ω0n0 ω1n1 ω2n2 n0! · n1! · n2! · · ·

(15)

but the summation appearing here cannot be evaluated explicitly because it is restricted to those sets only that conform to the pair of conditions (1). If our distribution sets were restricted by the condition r nr = N alone, then the evaluation of (15) would have been trivial; by the multinomial theorem, (N ) would have been simply (ω0 + ω1 + · · · )N . The added restriction r nrEr = N U, however, permits the inclusion of only a “limited” number of terms in the sum — and that constitutes the real difﬁculty of the problem. Nevertheless, we can still hope to make some progress because, from a physical point of view, we do not require anything more than an asymptotic result — one that holds in the limit N → ∞. The method commonly used for this purpose is the one developed by Darwin and Fowler (1922a,b, 1923), which itself makes use of the saddle-point method of integration or the so-called method of steepest descent.
We construct a generating function G(N , z) for the quantity (N , U):

∞

G(N , z) =

(N , U)zN U

(16)

U =0

3.2 A system in the canonical ensemble 45

which, in view of equation (15) and the second of the restrictive conditions (1), may be written as





G(N , z)

=

∞

U=0 {nr }

N! n0! n1! . . .

ω0zE0

n0

ω1zE1

n1 . . . .

(17)

It is easy to see that the summation over doubly restricted sets {nr}, followed by a summation over all possible values of U, is equivalent to a summation over singly restricted sets
{nr}, namely the ones that satisfy only one condition: r nr = N . Expression (17) can be evaluated with the help of the multinomial theorem, with the result

G(N , z) = ω0zE0 + ω1zE1 + · · · N

= [ f (z)]N , say.

(18)

Now, if we suppose that the Er (and hence the total energy value E = N U) are all integers, then, by (16), the quantity (N , U) is simply the coefﬁcient of zN U in the expansion of the function G(N , z) as a power series in z. It can, therefore, be evaluated by the method of
residues in the complex z-plane.
To make this plan work, we assume to have chosen, right at the outset, a unit of energy
so small that, to any desired degree of accuracy, we can regard the energies Er (and the prescribed total energy N U) as integral multiples of this unit. In terms of this unit, any energy
value we come across will be an integer. We further assume, without loss of generality, that the sequence E0, E1, . . . is a nondecreasing sequence, with no common divisor;3 also, for the sake of simplicity, we assume that E0 = 0.4 The solution now is

(N

,U)

=

1 2π i

[ f (z)]N zN U+1 dz,

(19)

where the integration is carried along any closed contour around the origin; of course, we must stay within the circle of convergence of the function f (z), so that a need for analytic
continuation does not arise.
First of all, we examine the behavior of the integrand as we proceed from the origin along the real positive axis, remembering that all our ωr are virtually equal to unity and that 0 = E0 ≤ E1 ≤ E2 · · · . We ﬁnd that the factor [ f (z)]N starts from the value 1 at z = 0, increases monotonically and tends to inﬁnity as z approaches the circle of convergence of f (z), wherever that may be. The factor z−(N U+1), on the other hand, starts from a positive,
inﬁnite value at z = 0 and decreases monotonically as z increases. Moreover, the relative rate of increase of the factor [ f (z)]N itself increases monotonically while the relative rate

3Actually, this is not a serious restriction at all, for a common divisor, if any, can be removed by selecting the unit of
energy correspondingly larger. 4This too is not serious, for by doing so we are merely shifting the zero of the energy scale; the mean energy U then
becomes U − E0, but we can agree to call it U again.

. 46 Chapter 3 The Canonical Ensemble

of decrease of the factor z−(N U+1) decreases monotonically. Under these circumstances,
the integrand must exhibit a minimum (and no other extremum) at some value of z, say
x0, within the circle of convergence. And, in view of the largeness of the numbers N and N U, this minimum may indeed be very steep!
Thus, at z = x0 the ﬁrst derivative of the integrand must vanish, while the second derivative must be positive and, hopefully, very large. Accordingly, if we proceed through
the point z = x0 in a direction orthogonal to the real axis, the integrand must exhibit an equally steep maximum.5 Thus, in the complex z-plane, as we move along the real axis
our integrand shows a minimum at z = x0, whereas if we move along a path parallel to the imaginary axis but passing through the point z = x0, the integrand shows a maximum there. It is natural to call the point x0 a saddle point; see Figure 3.2. For the contour of integration we choose a circle, with center at z = 0 and radius equal to x0, hoping that on integration along this contour only the immediate neighborhood of the sharp maximum at the point x0 will make the most dominant contribution to the value of the integral.6
To carry out the integration we ﬁrst locate the point x0. For this we write our integrand as

[ f (z)]N zN U+1

= exp[N g(z)],

(20)

where

g(z) = ln f (z) −

1 U+N

ln z,

(21)

exp{ g(z)}

Saddle point

I m z

0

x0

Re z

Contour of integration

FIGURE 3.2 The saddle point.
5This can be seen by noting that (i) an analytic function must possess a unique derivative everywhere (so, in our case, it must be zero, irrespective of the direction in which we pass through the point x0), and (ii) by the Cauchy–Riemann conditions of analyticity, the second derivative of the function with respect to y must be equal and opposite to the second derivative with respect to x.
6It is indeed true that, for large N , the contribution from the rest of the circle is negligible. The intuitive reason for this is that the terms (ωr zEr ), which constitute the function f (z), “reinforce” one another only at the point z = x0; elsewhere, there is bound to be disagreement among their phases, so that at all other points along the circle, |f (z)| < f (x0). Now, the factor that actually governs the relative contributions is [|f (z)|/f (x0)]N ; for N 1, this will clearly be negligible. For a rigorous demonstration of this point, see Schro¨dinger (1960, pp. 31–33).

3.2 A system in the canonical ensemble 47

while

f (z) = ωrzEr .

(22)

r

The number x0 is then determined by the equation

g

(x0)

=

f (x0) f (x0)

−

NU +1 N x0

=

0,

(23)

which, in view of the fact that N U 1, can be written as

U

≈

x0

f f

(x0) (x0)

=

r

ωr Er x0Er ωr x0Er .

(24)

r

We further have

g (x0) =

f (x0) [ f (x0)]2 f (x0) − [ f (x0)]2

NU +1 + N x02

≈

f (x0) f (x0)

−

U2 −U x02

.

(25)

It will be noted here that, in the limit N → ∞ and E (≡ N U) → ∞, with U staying constant, the number x0 and the quantity g (x0) become independent of N .
Expanding g(z) about the point z = x0, along the direction of integration, that is, along the line z = x0 + iy, we have

g

(z)

=

g

(x0

)

−

1 2

g

(x0)y2 + · · · ;

accordingly, the integrand (20) might be approximated as

[ f (x0)]N x0N U+1

exp

N −2g

(x0)y2

.

(26)

Equation (19) then gives

(N , U)

1 [ f (x0)]N 2π i x0N U+1

∞

exp

N −2g

(x0)y2

i dy

−∞

=

[ f (x0)]N x0N U+1

1 · {2π N g (x0)}1/2 ,

(27)

which gives

1

1

1

ln N

(N , U) = {ln f (x0) − U ln x0} − N ln x0 − 2N ln{2π N g (x0)}.

(28)

. 48 Chapter 3 The Canonical Ensemble

In the limit N → ∞ (with U staying constant), the last two terms in this expression tend to zero, with the result

1

ln N

(N , U) = ln f (x0) − U ln x0.

(29)

Substituting for f (x0) and introducing a new variable β, deﬁned by the relationship

x0 ≡ exp(−β),

(30)

we get

1

ln N

(N , U) = ln

ωr exp(−βEr) + βU.
r

(31)

The expectation value of the number nr then follows from (14) and (31):

nr N

 =



ωr exp(−βEr) 

ωr

exp(−β Er )

+

− 

r

ωrEr exp(−βEr)





 ∂β

ωr exp(−βEr) + U ωr ∂ωr 

.

(32)

r

r

all ωr =1

The term inside the curly brackets vanishes identically because of (24) and (30). It has been included here to emphasize the fact that, for a ﬁxed value of U, the number β(≡ − ln x0) in fact depends on the choice of the ωr; see (24). We will appreciate the importance of this fact when we evaluate the mean square ﬂuctuation in the number nr; in the calculation of the expectation value of nr, this does not really matter. We thus obtain

nr N

=

exp(−βEr) , exp(−β Er )

(33)

r

which is identical to expression (10) for n∗r /N . The physical signiﬁcance of the parameter β is also the same as in that expression, for it is determined by equation (24), with all ωr = 1, that is, by equation (11) which ﬁts naturally with equation (33) because U is nothing but
the ensemble average of the variable Er:

1

U = r ErPr = N

Er nr .
r

(34)

Finally, we compute ﬂuctuations in the values of the numbers nr. We have, ﬁrst of all,

n2r

n2r W {nr }
{nr }

1

≡

=

W {nr}

ωr

∂

∂ ωr

ωr

∂

∂ ωr

;
all ωr =1

(35)

{nr }

see equations (12) to (14). It follows that

(

nr )2

≡

{nr −

nr

2
}

=

n2r

−

nr

2
=

∂ ωr ∂ωr

∂

ωr ∂ωr

ln

.
all ωr =1

(36)

3.2 A system in the canonical ensemble 49

Substituting from (31) and making use of (32), we get



( nr )2 N

=

ωr

∂ ∂ ωr



ωr exp(−βEr) ωr exp(−βEr)

r

  +− 

r

ωrEr exp(−βEr)





 ∂β

ωr exp(−βEr) + U ωr ∂ωr 

.

(37)

r

all ωr =1

We note that the term in the curly brackets would not make any contribution because it
is identically zero, whatever the choice of the ωr. However, in the differentiation of the ﬁrst term, we must not forget to take into account the implicit dependence of β on the ωr, which arises from the fact that unless the ωr are set equal to unity the relation determining β does contain ωr; see equations (24) and (30), whereby

ωrEr exp(−βEr)

U = r ωr exp(−βEr)

.

(38)

r

all ωr =1

A straightforward calculation gives

∂β ∂ ωr

=
U all ωr =1

Er − U Er2 − U2

nr N

.

(39)

We can now evaluate (37), with the result

( nr )2

nr

N =N −

nr N

2
+

nr N

(U − Er)

∂β ∂ ωr

U all ωr =1

nr =N

1−

nr N

nr −N

(Er − U)2 (Er − U)2

.

(40)

For the relative ﬂuctuation in nr, we get

nr 2

11

nr

= nr − N

1+

(Er − U)2 (Er − U)2

.

(41)

As N → ∞, nr also → ∞, with the result that the relative ﬂuctuations in nr tend to zero; accordingly, the canonical distribution becomes inﬁnitely sharp and with it the mean value, the most probable value — in fact, any values of nr that appear with nonvanishing probability — become essentially the same. And that is the reason why two wildly different methods of obtaining the canonical distribution followed in this section have led to identical results.

. 50 Chapter 3 The Canonical Ensemble

3.3 Physical signiﬁcance of the various statistical quantities in the canonical ensemble

We start with the canonical distribution

Pr ≡

nr N

=

exp(−β Er ) exp(−β Er

)

,

(1)

r

where β is determined by the equation

U=

r

Er exp(−βEr) exp(−β Er )

∂ = − ∂β ln

r

exp(−βEr) .
r

(2)

We now look for a general recipe to extract information about the various macroscopic properties of the given system on the basis of the foregoing statistical results. For this, we recall certain thermodynamic relationships involving the Helmholtz free energy A(= U − TS), namely

dA = dU − TdS − SdT = −SdT − PdV + µ dN,

(3)

∂A

S=−

∂T

,
N ,V

∂A

P=−

∂V

,
N ,T

µ=

∂A

∂N

,
V ,T

(4)

and

U = A + TS = A − T

∂A ∂T

= −T 2
N ,V

∂ ∂T

A T

∂(A/T )

=
N ,V

, ∂(1/T ) N,V

(5)

where the various symbols have their usual meanings. Comparing (5) with (2), we infer that there exists a close correspondence between the quantities entering through the statistical treatment and the ones coming from thermodynamics, namely

β

=

1 kT

,

ln

exp(−β Er )
r

A = − kT ,

(6)

where k is a universal constant yet to be determined; soon we shall see that k is indeed the Boltzmann constant.
The equations in (6) constitute the most fundamental result of the canonical ensemble theory. Customarily, we write it in the form

A(N, V , T ) = −kT ln QN (V , T ),

(7)

where

QN (V , T ) = exp(−Er/kT ).

(8)

r

3.3 Physical signiﬁcance of the various statistical quantities 51

The quantity QN (V , T ) is referred to as the partition function of the system; sometimes it is also called the “sum-over-states” (German: Zustandssumme). The dependence of Q on T is quite obvious. The dependence on N and V comes through the energy eigenvalues Er; in fact, any other parameters that might govern the values Er should also appear in the argument of Q. Moreover, for the quantity A(N, V , T ) to be an extensive property of the system, ln Q must also be an extensive quantity.
Once the Helmholtz free energy is known, the rest of the thermodynamic quantities follow straightforwardly. While the entropy, the pressure and the chemical potential are obtained from formulae (4), the speciﬁc heat at constant volume follows from

∂U

∂2A

CV =

∂T

= −T
N ,V

∂T2
N ,V

(9)

and the Gibbs free energy from

G = A + PV = A − V

∂A

∂V

=N
N ,T

∂A ∂N

= Nµ;
V ,T

(10)

see Problem 3.5. At this stage it appears worthwhile to make a few remarks on the foregoing results. First
of all, we note from equations (4) and (6) that the pressure P is given by

P=−

r

∂ Er ∂V

exp(−β Er )

exp(−βEr) ,

(11)

r

so that

PdV = − PrdEr = −dU.

(12)

r

The quantity on the right side of this equation is clearly the change in the average energy of a system (in the ensemble) during a process that alters the energy levels Er, leaving the probabilities Pr unchanged. The left side then tells us that the volume change dV provides an example of such a process, and the pressure P is the “force” accompanying that process. The quantity P, which was introduced here through the thermodynamic relationship (3), thus acquires a mechanical meaning as well.
The entropy of the system is determined as follows. Since Pr = Q−1 exp(−βEr),

ln Pr = − ln Q − β Er = β(A − U) = −S/k,

with the result that

S = −k ln Pr = −k Pr ln Pr.

(13)

r

. 52 Chapter 3 The Canonical Ensemble

This is an extremely interesting relationship, for it shows that the entropy of a physical system is solely and completely determined by the probability values Pr (of the system being in different dynamical states accessible to it)!
From the very look of it, equation (13) appears to be of fundamental importance; indeed, it reveals a number of interesting conclusions. One of these relates to a system in its ground state (T = 0 K). If the ground state is unique, then the system is sure to be found in this particular state and in no other; consequently, Pr is equal to 1 for this state and 0 for all others. Equation (13) then tells us that the entropy of the system is precisely zero, which is essentially the content of the Nernst heat theorem or the third law of thermodynamics.7 We also infer that vanishing entropy and perfect statistical order (which implies complete predictability about the system) go together. As the number of accessible states increases, more and more of the Pr become nonzero; the entropy of the system thereby increases. As the number of states becomes exceedingly large, most of the Pvalues become exceedingly small (and their logarithms assume large, negative values); the net result is that the entropy becomes exceedingly large. Thus, the largeness of entropy and the high degree of statistical disorder (or unpredictability) in the system also go hand in hand.
It is because of this fundamental connection between entropy on one hand and lack of information on the other that equation (13) became the starting point of the pioneering work of Shannon (1948, 1949) in the development of the theory of communication.
It may be pointed out that formula (13) applies in the microcanonical ensemble as well. There, for each member system of the ensemble, we have a group of states, all equally likely to occur. The value of Pr is, then, 1/ for each of these states and 0 for all others. Consequently,

11

S = −k

ln

= k ln ,

(14)

r=1

which is precisely the central result in the microcanonical ensemble theory; see equation (1.2.6) or (2.3.6).

3.4 Alternative expressions for the partition function
In most physical cases the energy levels accessible to a system are degenerate, that is, one has a group of states, gi in number, all belonging to the same energy value Ei. In such cases it is more useful to write the partition function (3.3.8) as

QN (V , T ) = gi exp(−βEi);

(1)

i

7Of course, if the ground state of the system is degenerate (with a multiplicity 0), then the ground-state entropy is nonzero and is given by the expression k ln 0; see equation (14).

3.4 Alternative expressions for the partition function 53

the corresponding expression for Pi, the probability that the system be in a state with energy Ei, would be

Pi =

gi exp(−βEi) . gi exp(−βEi)

(2)

i

Clearly, the gi states with a common energy Ei are all equally likely to occur. As a result, the probability of a system having energy Ei becomes proportional to the multiplicity gi of this level; gi thus plays the role of a “weight factor” for the level Ei. The actual probability is then determined by the weight factor gi as well as by the Boltzmann factor exp(−βEi) of the level, as we have in (2). The basic relations established in the preceding section, however, remain unaffected.
Now, in view of the largeness of the number of particles constituting a given system and the largeness of the volume to which these particles are conﬁned, the consecutive energy values Ei of the system are, in general, very close to one another. Accordingly, there lie, within any reasonable interval of energy (E, E + dE), a very large number of energy levels. One may then regard E as a continuous variable and write P(E)dE for the probability that the given system, as a member of the canonical ensemble, may have its energy in the range (E, E + dE). Clearly, this probability will be given by the product of the relevant single-state probability and the number of energy states lying in the speciﬁed range. Denoting the latter by g(E)dE, where g(E) denotes the density of states around the energy value E, we have

P(E)dE ∝ exp(−βE)g(E)dE

(3)

which, on normalization, becomes

P(E)dE =

exp(−β E )g (E )dE
∞

.

(4)

exp(−β E )g (E )dE

0

The denominator here is yet another expression for the partition function of the system:

∞

QN (V , T ) = e−βE g(E)dE.

(5)

0

The expression for f , the expectation value of a physical quantity f , may now be written as

f (Ei)gie−βEi

∞
f

(E)e−βE

g

(E

)dE

f

≡

i

fiPi =

i

gi e−β Ei
i

0
→

.

∞

e−βE g(E)dE

(6)

0

. 54 Chapter 3 The Canonical Ensemble

Before proceeding further, we take a closer look at equation (5) and note that, with β > 0, the partition function Q(β) is just the Laplace transform of the density of states g(E). We may, therefore, write g(E) as the inverse Laplace transform of Q(β):

β +i∞

g (E )

=

1 2π i

eβE Q(β)dβ (β > 0)

(7)

β −i∞

∞

1 = 2π

e(β +iβ )E Q(β + iβ )dβ ,

(8)

−∞

where β is now treated as a complex variable, β + iβ , while the path of integration runs parallel to, and to the right of, the imaginary axis, that is, along the straight line Re β = β > 0. Of course, the path may be continuously deformed so long as the integral converges.

3.5 The classical systems
The theory developed in the preceding sections is of very general applicability. It applies to systems in which quantum-mechanical effects are important as well as to those that can be treated classically. In the latter case, our formalism may be written in the language of the phase space; as a result, the summations over quantum states get replaced by integrations over phase space.
We recall the concepts developed in Sections 2.1 and 2.2, especially formula (2.1.3) for the ensemble average f of a physical quantity f (q, p), namely

f (q, p)ρ(q, p)d3N q d3N p

f=

ρ(q, p)d3N q d3N p ,

(1)

where ρ(q, p) denotes the density of the representative points (of the systems) in the phase space; we have omitted here the explicit dependence of the function ρ on time t because we are interested in the study of equilibrium situations only. Evidently, the function ρ(q, p) is a measure of the probability of ﬁnding a representative point in the vicinity of the phase point (q, p), which in turn depends on the corresponding value H(q, p) of the Hamiltonian of the system. In the canonical ensemble,

ρ(q, p) ∝ exp{−βH(q, p)};

(2)

compare to equation (3.1.6). The expression for f then takes the form

f=

f

(q, p) exp(−βH)dω exp(−β H )dω

,

(3)

3.5 The classical systems 55

where dω(≡ d3N q d3N p) denotes a volume element of the phase space. The denominator of this expression is directly related to the partition function of the system. However, to write the precise expression for the latter, we must take into account the relationship between a volume element in the phase space and the corresponding number of distinct quantum states of the system. This relationship was established in Sections 2.4 and 2.5, whereby an element of volume dω in the phase space corresponds to

dω

N ! h3N

(4)

distinct quantum states of the system.8 The appropriate expression for the partition function would, therefore, be

QN (V , T ) =

1 N ! h3N

e−β H (q,p) dω;

(5)

it is understood that the integration in (5) goes over the whole of the phase space. As our ﬁrst application of this formulation, we consider the example of an ideal gas.
Here, we have a system of N identical molecules, assumed to be monatomic (so there are no internal degrees of motion to be considered), conﬁned to a space of volume V and in equilibrium at temperature T . Since there are no intermolecular interactions to be taken into account, the energy of the system is wholly kinetic:

N

H(q, p) = (p2i /2m).

(6)

i=1

The partition function of the system would then be

QN (V , T

)

=

1 N ! h3N

N
e−(β/2m) ip2i (d3qid3pi).

(7)

i=1

Integrations over the space coordinates are rather trivial; they yield a factor of V N . Integrations over the momentum coordinates are also quite easy, once we note that integral (7) is simply a product of N identical integrals. Thus, we get

QN (V , T )

=

VN N ! h3N

 

∞
e−p2 /2mkT

N 4π p2dp 

(8)

0

1 = N!

V h3

(2π

mkT

)3/2

N
;

(9)

8Ample justiﬁcation has already been given for the factor h3N . The factor N! comes from the considerations of Sections 1.5 and 1.6; it arises essentially from the fact that the particles constituting the given system are not only identical but, in fact, indistinguishable. For a complete proof of this result, see Section 5.5.

. 56 Chapter 3 The Canonical Ensemble

here, use has been made of equation (B.13a). The Helmholtz free energy is then given by, using Stirling’s formula (B.29),



A(N

,

V

,

T

)

≡

−kT

ln

QN

(V

,

T

)

=

NkT

ln

 

N V

h2

3/2  

2π mkT

− 1. 

(10)

The foregoing result is identical to equation (1.5.8), which was obtained by following a very different procedure. The simplicity of the present approach is, however, striking. Needless to say, the complete thermodynamics of the ideal gas can be derived from equation (10) in a straightforward manner. For instance,

µ≡



∂A

N

∂N

V ,T

=

kT

ln V

h2

3/2 

2π mkT

, 

(11)

∂A

NkT

P≡−

∂V

=
N ,T

V

(12)

and

∂A

V 2π mkT 3/2 5

S≡−

∂T

= Nk
N ,V

ln

N

h2

+2 .

(13)

These results are identical to the ones derived previously, namely (1.5.7), (1.4.2), and (1.5.1a), respectively. In fact, the identiﬁcation of formula (12) with the ideal-gas law, PV = nRT , establishes the identity of the (hitherto undetermined) constant k as the Boltzmann constant; see equation (3.3.6). We further obtain

U ≡−

∂ ∂β

(ln

Q)

Er

≡ −T 2

∂ ∂T

A T

3

N ,V

≡ A + TS

=

NkT , 2

(14)

and so on. At this stage we have an important remark to make. Looking at the form of equation (8)
and the manner in which it came about, we may write

QN (V , T ) =

1 N

!

[Q1(V

,

T

)]N

,

(15)

where Q1(V , T ) may be regarded as the partition function of a single molecule in the system. A little reﬂection will show that this result obtains essentially from the fact that the basic constituents of our system are noninteracting (and hence the total energy of the system is simply the sum of their individual energies). Clearly, the situation will not be altered even if the molecules in the system had internal degrees of motion as well. What is essentially required for equation (15) to be valid is the absence of interactions among the basic constituents of the system (and, of course, the absence of quantum-mechanical correlations).

3.5 The classical systems 57

Going back to the ideal gas, we could as well have started with the density of states g(E). From equation (1.4.17), and in view of the Gibbs correction factor, we have

g(E) =

∂ ∂E

≈

1 N!

V h3

N

(2π m)3N/2 {(3N/2) − 1}!

E

(3N

/2)−1.

(16)

Substituting this into equation (3.4.5), and noting that the integral involved is equal to {(3N/2) − 1}! /β3N/2, we obtain

QN

(β )

=

1 N!

VN h3

2π m 3N/2 ,
β

(17)

which is identical to (9). It may also be noted that if one starts with the single-particle density of states (2.4.7), namely

a(ε)

≈

2π V h3

(2m)3/2ε1/2,

(18)

computes the single-particle partition function,

∞

Q1(β) =

e−β ε a(ε)dε

=

V h3

2π m 3/2

β

,

(19)

0

and then makes use of formula (15), one would arrive at the same result for QN (V , T ). Lastly, we consider the question of determining the density of states, g(E), from
the expression for the partition function, Q(β) — assuming that the latter is already known; indeed, expression (9) for Q(β) was derived without making use of any knowledge regarding the function g(E). According to equation (3.4.7) and (9), we have

g (E )

=

VN N!

2π m h2

3N /2

1 2π i

β

+i∞ eβE β 3N /2

dβ

(β > 0).

(20)

β −i∞

Noting that, for all positive n,

1 2π i

s

+i∞ esx sn+1

ds

=

 xn  n!
0

s −i∞

for for

x≥0 x ≤ 0,

equation (20) becomes

 

V

N



g(E) = N!

2π m 3N/2 E(3N/2)−1

h2

{(3N/2) − 1}!

for

E≥0

0

for E ≤ 0,

(21)9 (22)

9For the details of this evaluation, see Kubo (1965, pp. 165–168).

. 58 Chapter 3 The Canonical Ensemble

which is indeed the correct result for the density of states of an ideal gas; compare to equation (16). The foregoing derivation may not appear particularly valuable because in the present case we already knew the expression for g(E). However, cases do arise where the evaluation of the partition function of a given system and the consequent evaluation of its density of states turn out to be quite simple, whereas a direct evaluation of the density of states from ﬁrst principles is rather involved. In such cases, the method given here can indeed be useful; see, for example, Problem 3.15 in comparison with Problems 1.7 and 2.8.

3.6 Energy ﬂuctuations in the canonical ensemble: correspondence with the microcanonical ensemble
In the canonical ensemble, a system can have energy anywhere between zero and inﬁnity. On the other hand, the energy of a system in the microcanonical ensemble is restricted to a very narrow range. How, then, can we assert that the thermodynamic properties of a system derived through the formalism of the canonical ensemble would be the same as the ones derived through the formalism of the microcanonical ensemble? Of course, we do expect that the two formalisms yield identical results, for otherwise our whole scheme would be marred by internal inconsistency. And, indeed, in the case of an ideal classical gas the results obtained by following one approach were precisely the same as the ones obtained by following the other approach. What, then, is the underlying reason for this equivalence?
The answer to this question is obtained by examining the extent of the range over which the energies of the systems in the canonical ensemble have a signiﬁcant probability to spread; that will tell us the extent to which the canonical ensemble really differs from the microcanonical one. To explore this point, we write down the expression for the mean energy

Er exp(−βEr)

U ≡ E = r exp(−βEr)

(1)

r

and differentiate it with respect to the parameter β, holding the energy values Er constant. We obtain

∂U

Er2 exp(−βEr)
r

∂β = − exp(−βEr) +

r

2
Er exp(−βEr)
r 2
exp(−β Er )
r

= − E2 + E 2,

(2)

from which it follows that

(

E)2

≡

E2

−

E

2
=−

∂U ∂β

= kT 2

∂U ∂T

= kT 2CV .

(3)

3.6 Energy ﬂuctuations in the canonical ensemble 59

Note that we have here the speciﬁc heat at constant volume, because the partial differentiation in (2) was carried out with the Er kept constant! For the relative root-mean-square ﬂuctuation in E, equation (3) gives

√[

( E)2 E

]

=

√(kT 2CV ) , U

(4)

which is O(N−1/2), N being the number of particles in the system. Consequently, for large N (which is true for every statistical system) the relative r.m.s. ﬂuctuation in the values of E is quite negligible! Thus, for all practical purposes, a system in the canonical ensemble has an energy equal to, or almost equal to, the mean energy U; the situation in this ensemble is, therefore, practically the same as in the microcanonical ensemble. That explains why the two ensembles lead to practically identical results.
For further understanding of the situation, we consider the manner in which energy is distributed among the various members of the (canonical) ensemble. To do this, we treat E as a continuous variable and start with expression (3.4.3), namely

P(E)dE ∝ exp(−βE)g(E)dE.

(3.4.3)

The probability density P(E) is given by the product of two factors: (i) the Boltzmann factor, which monotonically decreases with E, and (ii) the density of states, which monotonically increases with E. The product, therefore, has an extremum at some value of E, say E∗.10 The value E∗ is determined by the condition

that is, by

∂ ∂E

{e−βE g(E)}

E=E∗

=

0,

∂ ln g(E) ∂E

E=E∗

= β.

(5)

Recalling that

S = k ln g and the foregoing condition implies that

∂ S(E ) ∂E

E=U

=

1 T

= kβ,

E∗ = U.

(6)

This is a very interesting result, for it shows that, irrespective of the physical nature of the given system, the most probable value of its energy is identical to its mean value. Accordingly, if it is advantageous, we may use one instead of the other.
10Subsequently we shall see that this extremum is actually a maximum — and an extremely sharp one at that.

. 60 Chapter 3 The Canonical Ensemble

We now expand the logarithm of the probability density P(E) around the value E∗ ≈ U; we get

ln e−βE g(E) =

−β

U

+

S k

1 ∂2 + 2 ∂E2 ln

e−βE g(E)

(E − U)2 + · · ·

E=U

=

−β (U

−

TS)

−

1 2kT 2CV

(E

−

U )2

+

·

·

·

,

(7)

from which we obtain

P(E) ∝ e−βE g(E)

e−β(U−TS) exp

(E − U)2 − 2kT 2CV

.

(8)

This is a Gaussian distribution in E, with mean value U and dispersion √(kT 2CV ); compare with equation (3). In terms of the reduced variable E/U, the distribution is again Gaussian, with mean value unity and dispersion √(kT 2CV )/U {which is O(N−1/2)}; thus, for N 1, we have an extremely sharp distribution which, as N → ∞, approaches a delta-function!

It would be instructive here to consider once again the case of a classical ideal gas. Here, g(E) is proportional to E(3N/2−1) and hence increases very fast with E; the factor e−βE, of course, decreases with E. The product g(E) exp(−βE) exhibits a maximum at E∗ = (3N/2 − 1)β−1, which is practically the same as the mean value U = (3N/2)β−1. For

values of E signiﬁcantly different from E∗, the product essentially vanishes (for smaller val-

ues of E, due to the relative paucity of the available energy states; for larger values of E, due

to the relative depletion caused by the Boltzmann factor). The overall picture is shown in

Figure 3.3 where we have displayed the actual behavior of these functions in the special

case

N

=

10.

The

most

probable

value

of

E

is

now

14 15

of

the

mean

value;

so,

the

distribution

is somewhat asymmetrical. The effective width can be readily calculated from (3) and

turns out to be (2/3N)1/2U, which, for N = 10, is about a quarter of U. We can see that,

as N becomes large, both E∗ and U increase (essentially linearly with N), the ratio E∗/U

approaches unity and the distribution tends to become symmetrical about E∗. At the same time, the width increases (but only as N1/2); considered in the relative sense, it tends to vanish (as N−1/2).

We ﬁnally look at the partition function QN (V , T ), as given by equation (3.4.5), with its integrand replaced by (8). We have

∞
QN (V , T ) e−β(U−TS) e−(E−U)2/2kT2CV dE
0
∞
e−β(U−TS)√(2kT 2CV ) e−x2 dx
−∞
= e−β(U−TS)√(2π kT 2CV ),

3.7 Two theorems — the “equipartition” and the “virial” 61

g (E )e–␤E

1.0 g (E )
0.5

e–␤E

0

E *U

⌬⌬

E

FIGURE 3.3 The actual behavior of the functions g(E), e−βE , and g(E)e−βE for an ideal gas, with N = 10. The numerical values of the functions have been expressed as fractions of their respective values at E = U.

so that

−kT ln QN (V , T ) ≡ A

(U

−

TS)

−

1 2

kT

ln(2π kT 2CV

).

(9)

The last term, being O(ln N), is negligible in comparison with the other terms, which are all O(N). Hence,

A ≈ U − TS.

(10)

Note that the quantity A in this formula has come through the formalism of the canonical ensemble, while the quantity S has come through a deﬁnition belonging to the microcanonical ensemble. The fact that we ﬁnally end up with a consistent thermodynamic relationship establishes beyond doubt that these two approaches are, for all practical purposes, identical.

3.7 Two theorems — the “equipartition” and the “virial”
To derive these theorems, we determine the expectation value of the quantity xi(∂H/∂xj), where H(q, p) is the Hamiltonian of the system (assumed classical) while xi

. 62 Chapter 3 The Canonical Ensemble

and xj are any two of the 6N generalized coordinates (q, p). In the canonical ensemble,

∂H xi ∂xj =

xi

∂ ∂

H xj

e−βH dω

e−βH dω

dω = d3N q d3N p .

(1)

Let us consider the integral in the numerator. Integrating over xj by parts, it becomes

−

1 β

xi

e−β

H

(xj )2 (xj )1

+

1 β

∂ xi ∂ xj

e−βH dxj dω(j);

here, (xj)1 and (xj)2 are the “extreme” values of the coordinate xj, while dω(j) denotes “dω devoid of dxj.” The integrated part here vanishes because whenever any of the coordinates takes an “extreme” value the Hamiltonian of the system becomes inﬁnite.11 In the integral
that remains, the factor ∂xi/∂xj, being equal to δij, comes out of the integral sign and we are left with

1 β

δij

e−βH dω.

Substituting this into (1), we arrive at the remarkable result:

∂H xi ∂xj

= δijkT ,

(2)

which is independent of the precise form of the function H. In the special case xi = xj = pi, equation (2) takes the form

∂H

pi ∂pi ≡ piq˙ i = kT ,

(3)

while for xi = xj = qi, it becomes

∂H

qi ∂qi ≡ − qip˙ i = kT .

(4)

Adding over all i, from i = 1 to 3N, we obtain

∂H

i pi ∂pi ≡

piq˙ i = 3NkT
i

(5)

11For instance, if xj is a space coordinate, then its extreme values will correspond to “locations at the walls of the container”; accordingly, the potential energy of the system would become inﬁnite. If, on the other hand, xj is a momentum coordinate, then its extreme values will themselves be ±∞, in which case the kinetic energy of the system would become inﬁnite.

3.7 Two theorems — the “equipartition” and the “virial” 63

and

∂H

i qi ∂qi ≡ −

qip˙ i = 3NkT .
i

(6)

Now, in many physical situations the Hamiltonian of the system happens to be a quadratic function of its coordinates; so, through a canonical transformation, it can be brought into the form

H = AjPj2 + BjQj2,

(7)

j

j

where Pj and Qj are the transformed, canonically conjugate, coordinates while Aj and Bj are certain constants of the problem. For such a system, we clearly have

∂H

∂H

j Pj ∂Pj + Qj ∂Qj = 2H;

(8)

accordingly, by equations (3) and (4),

1

H = 2 fkT ,

(9)

where f is the number of nonvanishing coefﬁcients in expression (7). We, therefore, con-

clude that each harmonic term in the (transformed) Hamiltonian makes a contribution

of

1 2

kT

toward

the

internal

energy

of

the

system

and,

hence,

a

contribution

of

1 2

k

toward

the speciﬁc heat CV . This result embodies the classical theorem of equipartition of energy

(among the various degrees of freedom of the system). It may be mentioned here that,

for the distribution of kinetic energy alone, the equipartition theorem was ﬁrst stated by

Boltzmann (1871).

In our subsequent study we shall ﬁnd that the equipartition theorem as stated here is

not always valid; it applies only when the relevant degrees of freedom can be freely excited.

At a given temperature T , there may be certain degrees of freedom which, due to the insuf-

ﬁciency of the energy available, are more or less “frozen” due to quantum mechanical

effects. Such degrees of freedom do not make a signiﬁcant contribution toward the inter-

nal energy of the system or toward its speciﬁc heat; see, for example, Sections 6.5, 7.4,

and 8.3. Of course, the higher the temperature of the system the better the validity of this

theorem.

We now consider the implications of formula (6). First of all, we note that this formula

embodies the so-called virial theorem of Clausius (1870) for the quantity i qip˙ i , which is the expectation value of the sum of the products of the coordinates of the various particles

in the system and the respective forces acting on them; this quantity is generally referred to

as the virial of the system and is denoted by the symbol V. The virial theorem then states

. 64 Chapter 3 The Canonical Ensemble

that

V = −3NkT .

(10)

The relationship between the virial and other physical quantities of the system is best understood by ﬁrst looking at a classical gas of noninteracting particles. In this case, the only forces that come into play are the ones arising from the walls of the container; these forces can be designated by an external pressure P that acts on the system by virtue of the fact that it is bounded by the walls of the container. Consequently, we have here a force −P dS associated with an element of area dS of the walls; the negative sign appears because the force is directed inward while the vector dS is directed outward. The virial of the gas is then given by

V0 =

qiFi = −P r · dS,

i

0

S

(11)12

where r is the position vector of a particle that happens to be in the (close) vicinity of the surface element dS; accordingly, r may be considered to be the position vector of the surface element itself. By the divergence theorem, equation (11) becomes

V0 = −P (div r)dV = −3PV .

(12)

V

Comparing (12) with (10), we obtain the well-known result:

PV = NkT .

(13)

The internal energy of the gas, which in this case is wholly kinetic, follows from the

equipartition

theorem

(9)

and

is

equal

to

3 2

NkT

,

3N

being

the

number

of

degrees

of

freedom. Comparing this result with (10), we obtain the classical relationship

V = −2K ,

(14)

where K denotes the average kinetic energy of the system. It is straightforward to apply this theorem to a system of particles interacting through
a two-body potential u(r). In the thermodynamic limit, the pressure of a d-dimensional system depends only on the virial terms arising from the forces between pairs of particles:

P

1

nkT = 1 + NdkT

F (rij) · rij

1 = 1 − NdkT

i<j

i<j

∂

u(rij ∂ rij

)

rij

.

(15)

12It will be noted that the summation over the various particles of the system, which appears in the deﬁnition of the virial, has been replaced by an integration over the surface of the container, for the simple reason that no contribution to the virial arises from the interior of the container.

3.8 A system of harmonic oscillators 65

Equation (15) is called the virial equation of state. This equation can also be written in terms of the pair correlation function, equation (10.7.11), and is also used in computer simulations to determine the pressure of the system; see Problem 3.14, Section 10.7, and Section 16.4.

3.8 A system of harmonic oscillators
We shall now examine a system of N, practically independent, harmonic oscillators. This study will not only provide an interesting illustration of the canonical ensemble formulation but will also serve as a basis for some of our subsequent studies in this text. Two important problems in this line are (i) the theory of the black-body radiation (or the “statistical mechanics of photons”) and (ii) the theory of lattice vibrations (or the “statistical mechanics of phonons”); see Sections 7.3 and 7.4 for details.
We start with the specialized situation when the oscillators can be treated classically. The Hamiltonian of any one of them (assumed to be one-dimensional) is then given by

H(qi, pi)

=

1 2

mω2qi2

+

1 2m

p2i

(i = 1, . . . , N).

(1)

For the single-oscillator partition function, we readily obtain

∞∞

Q1(β) =

exp −β

1 mω2q2 2

+

1 p2 2m

−∞ −∞

dqdp h

1 2π 1/2 2π m 1/2

1

kT

= h βmω2

β

=β ω=

, ω

(2)

where = h/2π. This represents a classical counting of the average number of accessible microstates — that is, kT divided by the quantum harmonic oscillator energy spacing. The partition function of the N-oscillator system would then be

QN (β) = [Q1(β)]N = (β ω)−N =

kT N ;
ω

(3)

note that in writing (3) we have assumed the oscillators to be distinguishable. This is so because, as we shall see later, these oscillators are merely a representation of the energy levels available in the system; they are not particles (or even “quasiparticles”). It is actually photons in one case and phonons in the other, which distribute themselves over the various oscillator levels, that are indistinguishable!
The Helmholtz free energy of the system is now given by

ω

A ≡ −kT ln QN = NkT ln kT ,

(4)

. 66 Chapter 3 The Canonical Ensemble

whereby

µ = kT ln

ω kT

,

(5)

P = 0,

(6)

kT

S = Nk ln ω + 1 ,

(7)

U = NkT ,

(8)

and

CP = CV = Nk.

(9)

We note that the mean energy per oscillator is in complete agreement with the equiparti-

tion

theorem,

namely

2

×

1 2

kT

,

for

we

have

here

two

independent

quadratic

terms

in

the

single-oscillator Hamiltonian.

We may determine the density of states, g(E), of this system from expression (3) for its

partition function. We have, in view of (3.4.7),

g(E) = (

11 ω)N 2π i

β

+i∞eβE βN

dβ

β −i∞

(β > 0),

that is,

 1 EN−1



g

(E)

=

 

(

ω)N (N − 1)!

for

E≥0

(10)



0

for E ≤ 0.

To test the correctness of (10), we may calculate the entropy of the system with the help of this formula. Taking N 1 and making use of the Stirling approximation, we get

S(N, E) = k ln g(E) ≈ Nk ln

E Nω

+1 ,

(11)

which gives for the temperature of the system

∂S −1 E

T = ∂E N = Nk .

(12)

Eliminating E between these two relations, we obtain precisely our earlier result (7) for the function S(N, T ).

3.8 A system of harmonic oscillators 67

We now take up the quantum-mechanical situation, according to which the energy eigenvalues of a one-dimensional harmonic oscillator are given by

εn =

1 n+ 2

ω; n = 0, 1, 2, . . .

(13)

Accordingly, we have for the single-oscillator partition function

∞
Q1(β) = e−β(n+1/2)
n=0

ω
=

exp

−

1 2

β

ω

1 − exp(−β ω)

=

2 sinh

1β ω 2

−1
.

(14)

The N-oscillator partition function is then given by

QN (β) = [Q1(β)]N =

2 sinh

1β ω 2

−N

= e−(N/2)β ω{1 − e−β ω}−N .

(15)

For the Helmholtz free energy of the system, we get

A = NkT ln 2 sinh

1β ω 2

=N

1 2

ω + kT ln{1 − e−β

ω
}

,

(16)

whereby

µ = A/N,

(17)

P = 0,

(18)

S = Nk

1β 2

ω coth

1β 2

ω

− ln 2 sinh

1β 2

ω

= Nk

β eβ

ω

ω −

1

−

ln{1

−

e−β

ω
}

,

(19)

1 U = 2N

ω coth

1β 2

ω

=N

1 2

ω + eβ

ω ω −1

,

(20)

and

CP = CV = Nk

1β 2

2
ω cosech2

1β 2

ω

= Nk(β

ω)2

(eβ

eβ
ω

ω
−

1)2

.

(21)

Formula (20) is especially signiﬁcant, for it shows that the quantum-mechanical oscillators do not obey the equipartition theorem. The mean energy per oscillator is different

. 68 Chapter 3 The Canonical Ensemble

2

ε ␻1

2

3

1

0

0

1

2

kT/ ␻

FIGURE 3.4 The mean energy ε of a simple harmonic oscillator as a function of temperature. 1, the Planck oscillator; 2, the Schro¨ dinger oscillator; and 3, the classical oscillator.

from the equipartition value kT ; actually, it is always greater than kT ; see curve 2 in

Figure 3.4. Only in the limit of high temperatures, where the thermal energy kT is much

larger than the energy quantum ω, does the mean energy per oscillator tend to the

equipartition

value.

It

should

be

noted

here

that

if

the

zero-point

energy

1 2

ω were not

present,

the

limiting

value

of

the

mean

energy

would

be

(kT

−

1 2

ω), and not kT — we

may call such an oscillator the Planck oscillator; see curve 1 in Figure 3.4. In passing, we

observe that the speciﬁc heat (21), which is the same for the Planck oscillator as for the

Schro¨dinger oscillator, is temperature-dependent; moreover, it is always less than, and at

high temperatures tends to, the classical value (9).

Indeed, for kT ω, formulae (14) through (21) go over to their classical counterparts,

namely (2) through (9), respectively.

We shall now determine the density of states g(E) of the N-oscillator system from its

partition function (15). Carrying out the binomial expansion of this expression, we have

∞
QN (β) =
R=0

N +R−1 R

e−β(

1 2

N

ω+R

ω).

(22)

Comparing this with the formula

∞
QN (β) = g(E)e−βE dE,
0

we conclude that

∞
g(E) =
R=0

N +R−1 R

δ

E−

1 R+ 2N

ω,

(23)

3.8 A system of harmonic oscillators 69

where δ(x) denotes the Dirac delta function. Equation (23) implies that there are

(N + R − 1)! /R! (N − 1)! microstates available to the system when its energy E has the dis-

crete

value

(R

+

1 2

N)

ω, where R = 0, 1, 2, . . . , and that no microstate is available for other

values of E. This is hardly surprising, but it is instructive to look at this result from a slightly

different point of view.

We consider the following problem that arises naturally in the microcanonical ensem-

ble theory. Given an energy E for distribution among a set of N harmonic oscillators, each

of which can be in any one of the eigenstates (13), what is the total number of distinct ways

in which the process of distribution can be carried out? Now, in view of the form of the

eigenvalues εn, it makes sense to give away, right in the beginning, the zero-point energy

1 2

ω to each of the N oscillators and convert the rest of it into quanta (of energy

ω). Let R

be the number of these quanta; then

R=

1 E− 2N

ω

ω.

(24)

Clearly,

R

must

be

an

integer;

by

implication,

E

must

be

of

the

form

(R

+

1 2

N

)

ω. The prob-

lem then reduces to determining the number of distinct ways of allotting R quanta to N

oscillators, such that an oscillator may have 0 or 1 or 2 . . . quanta; in other words, we have

to determine the number of distinct ways of putting R indistinguishable balls into N dis-

tinguishable boxes, such that a box may receive 0 or 1 or 2. . . balls. A little reﬂection will

show that this is precisely the number of permutations that can be realized by shufﬂing R

balls, placed along a row, with (N − 1) partitioning lines (that divide the given space into N

boxes); see Figure 3.5. The answer clearly is

(R + N − 1)! R! (N − 1)!

,

(25)

which agrees with (23). We can now determine the entropy of the system from the number (25). Since N 1,
we have

S ≈ k{ln(R + N)! − ln R! − ln N! }

≈ k{(R + N) ln(R + N) − R ln R − N ln N};

(26)

FIGURE 3.5 Distributing 17 indistinguishable balls among 7 distinguishable boxes. The arrangement shown here represents one of the 23! /17! 6! distinct ways of carrying out the distribution.

. 70 Chapter 3 The Canonical Ensemble

the number R is, of course, a measure of the energy E of the system; see (24). For the temperature of the system, we obtain

1 T=

∂S

∂E

=
N

∂S ∂R

N

1 ω=

k ln ω

R+N R

=

k ln ω

E

+

1 2

N

E

−

1 2

N

ω ω

,

(27)

so that

E1 N =2

ω exp( exp(

ω/kT ω/kT

) )

+ −

1 1

,

(28)

which is identical to (20). It can be further checked that, by eliminating R between (26) and (27), we obtain precisely the formula (19) for S(N, T ). Thus, once again, we ﬁnd that the results obtained by following the microcanonical approach and the canonical approach are the same in the thermodynamic limit.
Finally, we may consider the classical limit when E/N, the mean energy per oscillator, is much larger than the energy quantum ω, that is, when R N. The expression (25) may, in that case, be replaced by

with

(R + N

− 1)(R + N − 2) . . . (R (N − 1)!

+ 1)

≈

RN −1 (N − 1)! ,

(25a)

R ≈ E/ ω.

The corresponding expression for the entropy turns out to be

S ≈ k{N ln(R/N) + N} ≈ Nk ln

E Nω

+1 ,

(26a)

which gives

1 ∂S

Nk

T=

∂E

≈
N

E

,

(27a)

so that

E N ≈ kT .

(28a)

These results are identical to the ones derived in the classical limit earlier in this section.

3.9 The statistics of paramagnetism
Next, we study a system of N magnetic dipoles, each having a magnetic moment µ. In the presence of an external magnetic ﬁeld H, the dipoles will experience a torque tending to

3.9 The statistics of paramagnetism 71

align them in the direction of the ﬁeld. If there were nothing else to check this tendency, the dipoles would align themselves precisely in this direction and we would achieve a complete magnetization of the system. In reality, however, thermal agitation in the system offers resistance to this tendency and, in equilibrium, we obtain only a partial magnetization. Clearly, as T → 0 K, the thermal agitation becomes ineffective and the system exhibits a complete orientation of the dipole moments, whatever the strength of the applied ﬁeld; at the other extreme, as T → ∞, we approach a state of complete randomization of the dipole moments, which implies a vanishing magnetization. At intermediate temperatures, the situation is governed by the parameter (µH/kT ).
The model adopted for this study consists of N identical, localized (and, hence, distinguishable), practically static, mutually noninteracting and freely orientable dipoles. We consider ﬁrst the case of classical dipoles that can be oriented in any direction relative to the applied magnetic ﬁeld. It is obvious that the only energy we need to consider here is the potential energy of the dipoles that arises from the presence of the external ﬁeld H and is determined by the orientations of the dipoles with respect to the direction of the ﬁeld:

N

N

N

E = Ei = − µi · H = −µH cos θi.

(1)

i=1

i=1

i=1

The partition function of the system is then given by

QN (β) = [Q1(β)]N ,

(2)

where

Q1(β) = exp(βµH cos θ ).

(3)

θ

The mean magnetic moment M of the system will obviously be in the direction of the ﬁeld H; for its magnitude we shall have

µ cos θ exp(βµH cos θ)

Mz = N µ cos θ = N θ

exp(βµH cos θ)

θ

=

N β

∂ ∂H

ln Q1(β)

=

−

∂A ∂H

.
T

(4)

Thus, to determine the degree of magnetization in the system all we have to do is to evaluate the single-dipole partition function (3).
First, we proceed classically (after Langevin, 1905a,b). Using (sin θdθdφ) as the elemental solid angle representing a small range of orientations of the dipole, we get

2π π

Q1(β) =

eβµH

cos θ

sin

θ

dθ dφ

=

4π

sinh(βµH βµH

)

,

(5)

00

. 72 Chapter 3 The Canonical Ensemble

so that

µz

≡

Mz N

=µ

coth(β µH )

−

1 βµH

= µL(βµH),

(6)

where L(x) is the so-called Langevin function

L(x)

=

coth

x

−

1 x

;

(7)

a plot of the Langevin function is shown in Figure 3.6. We note that the parameter βµH denotes the strength of the (magnetic) potential energy µH compared to the (thermal) kinetic energy kT .
If we have N0 dipoles per unit volume in the system, then the magnetization of the system, namely the mean magnetic moment per unit volume, is given by

Mz0 = N0µz = N0µL(x) (x = βµH).

(8)

For magnetic ﬁelds so strong (or temperatures so low) that the parameter x 1, the function L(x) is almost equal to 1; the system then acquires a state of magnetic saturation:

µz µ and Mz0 N0µ.

(9)

For temperatures so high (or magnetic ﬁelds so weak) that the parameter x 1, the function L(x) may be written as

x x3

3 − 45 + · · ·

(10)

which, in the lowest approximation, gives

Mz0

N0µ2 H. 3kT

(11)

1.0 0.5
0 0
FIGURE 3.6 The Langevin function L(x).

L(x)

4

8

12

x

3.9 The statistics of paramagnetism 73

The high-temperature isothermal susceptibility of the system is, therefore, given by

χ
T

= Lim
H →0

∂ Mz0 ∂H

T

N0µ2 3kT

=

C ,
T

say.

(12)

Equation (12) is the Curie law of paramagnetism, the parameter C being the Curie constant of the system. Figure 3.7 shows a plot of the susceptibility of a powdered sample of copper– potassium sulphate hexahydrate as a function of T −1; the fact that the plot is linear and passes almost through the origin vindicates the Curie law for this particular salt.
We shall now treat the problem of paramagnetism quantum-mechanically. The major modiﬁcation here arises from the fact that the magnetic dipole moment µ and its component µz in the direction of the applied ﬁeld cannot have arbitrary values. Quite generally, we have a direct relationship between the magnetic moment µ of a given dipole and its angular momentum l:

µ=

e g
2mc

l,

(13)

with

l2 = J( J + 1) 2;

J

=

1 2

,

3 2

,

5 2

,

...

or

0, 1, 2, . . .

(14)

The quantity g(e/2mc) is the gyromagnetic ratio of the dipole while the number g is Lande’s g-factor. If the net angular momentum of the dipole is due solely to electron spins, then

(␹ · 106)

80 70 60 50 40 30 20 10
0 20 40 60 80 (103/T in K21)
FIGURE 3.7 χ versus 1/T plot for a powdered sample of copper–potassium sulphate hexahydrate (after Hupse, 1942).

. 74 Chapter 3 The Canonical Ensemble

g = 2; on the other hand, if it is due solely to orbital motions, then g = 1. In general, however, its origin is mixed; g is then given by the formula

g

=

3 2

+

S(S

+ 1) − L(L 2J( J + 1)

+

1)

,

(15)

S and L being, respectively, the spin and the orbital quantum numbers of the dipole. Note that there is no upper or lower bound on the values that g can have!
Combining (13) and (14), we can write

µ2 = g2µ2B J( J + 1),

(16)

where µB(= e /2mc) is the Bohr magneton. The component µz of the magnetic moment in the direction of the applied ﬁeld is, on the other hand, given by

µz = gµBm, m = −J, −J + 1, . . . , J − 1, J.

(17)

Thus, a dipole whose magnetic moment µ conforms to expression (16) can have no other orientations with respect to the applied ﬁeld except the ones conforming to the values (17) of the component µz; obviously, the number of allowed orientations, for a given value of J, is (2J + 1). In view of this, the single-dipole partition function Q1(β) is now given by, see (3),

J

Q1(β) =

exp(β g µB mH ).

(18)

m=−J

Introducing a parameter x, deﬁned by

x = β(gµB J)H,

(19)

equation (18) becomes

Q1 (β )

=

J m=−J

emx/J

=

e−x {e(2J +1)x/J ex/J − 1

− 1}

e(2J+1)x/2J − e−(2J+1)x/2J

=

ex/2J − e−x/2J

1 = sinh 1 + 2J x

1 sinh x .
2J

(20)

The mean magnetic moment of the system is then given by, see equation (4),

Mz

=

N µz

=

N β

∂ ∂H

ln Q1(β)

1

1

1

1

= N(gµB J) 1 + 2J coth 1 + 2J x − 2J coth 2J x .

(21)

