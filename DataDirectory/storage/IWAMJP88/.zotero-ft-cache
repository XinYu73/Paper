Accelerat ing t he world's research.

DFT a practical approach
Arslan Ullah

Related papers

Download a PDF Pack of t he best relat ed papers 

Nit rogen Act ivat ion in a Mars - Van Krevelen Mechanism for Ammonia Synt hesis on Co3Mo3N Const ant inos D Zeinalipour-Yazdi

Densit y funct ional st udy of copper segregat ion in aluminum Anouar Benali
Sit e preference of CO chemisorbed on Pt (1 1 1) from densit y funct ional calculat ions Josep M Ricart

DENSITY FUNCTIONAL THEORY
A Practical Introduction
DAVID S. SHOLL Georgia Institute of Technology JANICE A. STECKEL National Energy Technology Laboratory

DENSITY FUNCTIONAL THEORY

DENSITY FUNCTIONAL THEORY
A Practical Introduction
DAVID S. SHOLL Georgia Institute of Technology JANICE A. STECKEL National Energy Technology Laboratory

Copyright # 2009 by John Wiley & Sons, Inc. All rights reserved. Prepared in part with support by the National Energy Technology Laboratory
Published by John Wiley & Sons, Inc., Hoboken, New Jersey Published simultaneously in Canada

No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment of the appropriate per copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750 8400, fax (978) 750 4470, or on the web at www.copyright.com. Requests to the Publisher for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748 6011, fax (201) 748 6008, or online at http://www.wiley.com/go/permission.

Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing this book, they make no representations or warranties with respect to the accuracy or completeness of the contents of this book and speciﬁcally disclaim any implied warranties of merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by sales representatives or written sales materials. The advice and strategies contained herein may not be suitable for your situation. You should consult with a professional where appropriate. Neither the publisher nor author shall be liable for any loss of proﬁt or any other commercial damages, including but not limited to special, incidental, consequential, or other damages.

For general information on our other products and services or for technical support, please contact our Customer Care Department within the United States at (800) 762 2974, outside the United States at (317) 572 3993 or fax (317) 572 4002.

Wiley also publishes its books in variety of electronic formats. Some content that appears in print may not be available in electronic formats. For more information about Wiley products, visit our web site at www. wiley.com.

Library of Congress Cataloging-in-Publication Data:

Sholl, David S. Density functional theory : a practical introduction / David S. Sholl and Jan Steckel.
p. cm. Includes index. ISBN 978 0 470 37317 0 (cloth) 1. Density functionals. 2. Mathematical physics. 3. Quantum chemistry. I. II. Title.
QC20.7.D43S55 2009 530.1404 dc22

Steckel, Jan. 2008038603

Printed in the United States of America 10 9 8 7 6 5 4 3 2 1

CONTENTS

Preface

xi

1 What Is Density Functional Theory?

1

1.1 How to Approach This Book, 1 1.2 Examples of DFT in Action, 2
1.2.1 Ammonia Synthesis by Heterogeneous Catalysis, 2 1.2.2 Embrittlement of Metals by Trace Impurities, 4 1.2.3 Materials Properties for Modeling Planetary Formation, 6 1.3 The Schro¨dinger Equation, 7 1.4 Density Functional Theory—From Wave Functions to Electron Density, 10 1.5 Exchange –Correlation Functional, 14 1.6 The Quantum Chemistry Tourist, 16 1.6.1 Localized and Spatially Extended Functions, 16 1.6.2 Wave-Function-Based Methods, 18 1.6.3 Hartree– Fock Method, 19 1.6.4 Beyond Hartree–Fock, 23 1.7 What Can DFT Not Do?, 28 1.8 Density Functional Theory in Other Fields, 30 1.9 How to Approach This Book (Revisited), 30 References, 31 Further Reading, 32

v

vi CONTENTS

2 DFT Calculations for Simple Solids

35

2.1 Periodic Structures, Supercells, and Lattice Parameters, 35 2.2 Face-Centered Cubic Materials, 39 2.3 Hexagonal Close-Packed Materials, 41 2.4 Crystal Structure Prediction, 43 2.5 Phase Transformations, 44 Exercises, 46 Further Reading, 47 Appendix Calculation Details, 47

3 Nuts and Bolts of DFT Calculations

49

3.1 Reciprocal Space and k Points, 50 3.1.1 Plane Waves and the Brillouin Zone, 50 3.1.2 Integrals in k Space, 53 3.1.3 Choosing k Points in the Brillouin Zone, 55 3.1.4 Metals—Special Cases in k Space, 59 3.1.5 Summary of k Space, 60
3.2 Energy Cutoffs, 61 3.2.1 Pseudopotentials, 63
3.3 Numerical Optimization, 65 3.3.1 Optimization in One Dimension, 65 3.3.2 Optimization in More than One Dimension, 69 3.3.3 What Do I Really Need to Know about Optimization?, 73
3.4 DFT Total Energies—An Iterative Optimization Problem, 73 3.5 Geometry Optimization, 75
3.5.1 Internal Degrees of Freedom, 75 3.5.2 Geometry Optimization with Constrained Atoms, 78 3.5.3 Optimizing Supercell Volume and Shape, 78 Exercises, 79 References, 80 Further Reading, 80 Appendix Calculation Details, 81

4 DFT Calculations for Surfaces of Solids

83

4.1 Importance of Surfaces, 83 4.2 Periodic Boundary Conditions and Slab Models, 84 4.3 Choosing k Points for Surface Calculations, 87 4.4 Classiﬁcation of Surfaces by Miller Indices, 88 4.5 Surface Relaxation, 94 4.6 Calculation of Surface Energies, 96

4.7 Symmetric and Asymmetric Slab Models, 98 4.8 Surface Reconstruction, 100 4.9 Adsorbates on Surfaces, 103
4.9.1 Accuracy of Adsorption Energies, 106 4.10 Effects of Surface Coverage, 107 Exercises, 110 References, 111 Further Reading, 111 Appendix Calculation Details, 112

CONTENTS vii

5 DFT Calculations of Vibrational Frequencies

113

5.1 Isolated Molecules, 114 5.2 Vibrations of a Collection of Atoms, 117 5.3 Molecules on Surfaces, 120 5.4 Zero-Point Energies, 122 5.5 Phonons and Delocalized Modes, 127 Exercises, 128 Reference, 128 Further Reading, 128 Appendix Calculation Details, 129

6 Calculating Rates of Chemical Processes Using

Transition State Theory

131

6.1 One-Dimensional Example, 132 6.2 Multidimensional Transition State Theory, 139 6.3 Finding Transition States, 142
6.3.1 Elastic Band Method, 144 6.3.2 Nudged Elastic Band Method, 145 6.3.3 Initializing NEB Calculations, 147 6.4 Finding the Right Transition States, 150 6.5 Connecting Individual Rates to Overall Dynamics, 153 6.6 Quantum Effects and Other Complications, 156 6.6.1 High Temperatures/Low Barriers, 156 6.6.2 Quantum Tunneling, 157 6.6.3 Zero-Point Energies, 157 Exercises, 158 Reference, 159 Further Reading, 159 Appendix Calculation Details, 160

viii CONTENTS

7 Equilibrium Phase Diagrams from Ab Initio

Thermodynamics

163

7.1 Stability of Bulk Metal Oxides, 164 7.1.1 Examples Including Disorder—Conﬁgurational Entropy, 169
7.2 Stability of Metal and Metal Oxide Surfaces, 172 7.3 Multiple Chemical Potentials and Coupled Chemical
Reactions, 174 Exercises, 175 References, 176 Further Reading, 176 Appendix Calculation Details, 177

8 Electronic Structure and Magnetic Properties

179

8.1 Electronic Density of States, 179 8.2 Local Density of States and Atomic Charges, 186 8.3 Magnetism, 188 Exercises, 190 Further Reading, 191 Appendix Calculation Details, 192

9 Ab Initio Molecular Dynamics

193

9.1 Classical Molecular Dynamics, 193 9.1.1 Molecular Dynamics with Constant Energy, 193 9.1.2 Molecular Dynamics in the Canonical Ensemble, 196 9.1.3 Practical Aspects of Classical Molecular Dynamics, 197
9.2 Ab Initio Molecular Dynamics, 198 9.3 Applications of Ab Initio Molecular Dynamics, 201
9.3.1 Exploring Structurally Complex Materials: Liquids and Amorphous Phases, 201
9.3.2 Exploring Complex Energy Surfaces, 204 Exercises, 207 Reference, 207 Further Reading, 207 Appendix Calculation Details, 208

CONTENTS ix

10 Accuracy and Methods beyond “Standard” Calculations

209

10.1 How Accurate Are DFT Calculations?, 209 10.2 Choosing a Functional, 215 10.3 Examples of Physical Accuracy, 220
10.3.1 Benchmark Calculations for Molecular Systems—Energy and Geometry, 220
10.3.2 Benchmark Calculations for Molecular Systems—Vibrational Frequencies, 221
10.3.3 Crystal Structures and Cohesive Energies, 222 10.3.4 Adsorption Energies and Bond Strengths, 223 10.4 DFTþX Methods for Improved Treatment of Electron Correlation, 224 10.4.1 Dispersion Interactions and DFT-D, 225 10.4.2 Self-Interaction Error, Strongly Correlated Electron
Systems, and DFTþU, 227 10.5 Larger System Sizes with Linear Scaling Methods and Classical
Force Fields, 229 10.6 Conclusion, 230 References, 231 Further Reading, 232

Index

235

PREFACE
The application of density functional theory (DFT) calculations is rapidly becoming a “standard tool” for diverse materials modeling problems in physics, chemistry, materials science, and multiple branches of engineering. Although a number of highly detailed books and articles on the theoretical foundations of DFT are available, it remains difﬁcult for a newcomer to these methods to rapidly learn the tools that allow him or her to actually perform calculations that are now routine in the ﬁelds listed above. This book aims to ﬁll this gap by guiding the reader through the applications of DFT that might be considered the core of continually growing scientiﬁc literature based on these methods. Each chapter includes a series of exercises to give readers experience with calculations of their own.
We have aimed to ﬁnd a balance between brevity and detail that makes it possible for readers to realistically plan to read the entire text. This balance inevitably means certain technical details are explored in a limited way. Our choices have been strongly inﬂuenced by our interactions over multiple years with graduate students and postdocs in chemical engineering, physics, chemistry, materials science, and mechanical engineering at Carnegie Mellon University and the Georgia Institute of Technology. A list of Further Reading is provided in each chapter to deﬁne appropriate entry points to more detailed treatments of the area. These reading lists should be viewed as identifying highlights in the literature, not as an effort to rigorously cite all relevant work from the thousands of studies that exist on these topics.
xi

xii PREFACE
One important choice we made to limit the scope of the book was to focus solely on one DFT method suitable for solids and spatially extended materials, namely plane-wave DFT. Although many of the foundations of plane-wave DFT are also relevant to complementary approaches used in the chemistry community for isolated molecules, there are enough differences in the applications of these two groups of methods that including both approaches would only have been possible by signiﬁcantly expanding the scope of the book. Moreover, several resources already exist that give a practical “handson” introduction to computational chemistry calculations for molecules.
Our use of DFT calculations in our own research and our writing of this book has beneﬁted greatly from interactions with numerous colleagues over an extended period. We especially want to acknowledge J. Karl Johnson (University of Pittsburgh), Aravind Asthagiri (University of Florida), Dan Sorescu (National Energy Technology Laboratory), Cathy Stampﬂ (University of Sydney), John Kitchin (Carnegie Mellon University), and Duane Johnson (University of Illinois). We thank Jeong-Woo Han for his help with a number of the ﬁgures. Bill Schneider (University of Notre Dame), Ken Jordan (University of Pittsburgh), and Taku Watanabe (Georgia Institute of Technology) gave detailed and helpful feedback on draft versions. Any errors or inaccuracies in the text are, of course, our responsibility alone.
DSS dedicates this book to his father and father-in-law, whose love of science and curiosity about the world are an inspiration. JAS dedicates this book to her husband, son, and daughter.
DAVID SHOLL
Georgia Institute of Technology, Atlanta, GA, USA
JAN STECKEL
National Energy Technology Laboratory, Pittsburgh, PA, USA

1
WHAT IS DENSITY FUNCTIONAL THEORY?
1.1 HOW TO APPROACH THIS BOOK
There are many ﬁelds within the physical sciences and engineering where the key to scientiﬁc and technological progress is understanding and controlling the properties of matter at the level of individual atoms and molecules. Density functional theory is a phenomenally successful approach to ﬁnding solutions to the fundamental equation that describes the quantum behavior of atoms and molecules, the Schro¨dinger equation, in settings of practical value. This approach has rapidly grown from being a specialized art practiced by a small number of physicists and chemists at the cutting edge of quantum mechanical theory to a tool that is used regularly by large numbers of researchers in chemistry, physics, materials science, chemical engineering, geology, and other disciplines. A search of the Science Citation Index for articles published in 1986 with the words “density functional theory” in the title or abstract yields less than 50 entries. Repeating this search for 1996 and 2006 gives more than 1100 and 5600 entries, respectively.
Our aim with this book is to provide just what the title says: an introduction to using density functional theory (DFT) calculations in a practical context. We do not assume that you have done these calculations before or that you even understand what they are. We do assume that you want to ﬁnd out what is possible with these methods, either so you can perform calculations
Density Functional Theory: A Practical Introduction. By David S. Sholl and Janice A. Steckel Copyright # 2009 John Wiley & Sons, Inc.
1

2 WHAT IS DENSITY FUNCTIONAL THEORY?
yourself in a research setting or so you can interact knowledgeably with collaborators who use these methods.
An analogy related to cars may be useful here. Before you learned how to drive, it was presumably clear to you that you can accomplish many useful things with the aid of a car. For you to use a car, it is important to understand the basic concepts that control cars (you need to put fuel in the car regularly, you need to follow basic trafﬁc laws, etc.) and spend time actually driving a car in a variety of road conditions. You do not, however, need to know every detail of how fuel injectors work, how to construct a radiator system that efﬁciently cools an engine, or any of the other myriad of details that are required if you were going to actually build a car. Many of these details may be important if you plan on undertaking some especially difﬁcult car-related project such as, say, driving yourself across Antarctica, but you can make it across town to a friend’s house and back without understanding them.
With this book, we hope you can learn to “drive across town” when doing your own calculations with a DFT package or when interpreting other people’s calculations as they relate to physical questions of interest to you. If you are interested in “building a better car” by advancing the cutting edge of method development in this area, then we applaud your enthusiasm. You should continue reading this chapter to ﬁnd at least one sureﬁre project that could win you a Nobel prize, then delve into the books listed in the Further Reading at the end of the chapter.
At the end of most chapters we have given a series of exercises, most of which involve actually doing calculations using the ideas described in the chapter. Your knowledge and ability will grow most rapidly by doing rather than by simply reading, so we strongly recommend doing as many of the exercises as you can in the time available to you.
1.2 EXAMPLES OF DFT IN ACTION
Before we even deﬁne what density functional theory is, it is useful to relate a few vignettes of how it has been used in several scientiﬁc ﬁelds. We have chosen three examples from three quite different areas of science from the thousands of articles that have been published using these methods. These speciﬁc examples have been selected because they show how DFT calculations have been used to make important contributions to a diverse range of compelling scientiﬁc questions, generating information that would be essentially impossible to determine through experiments.
1.2.1 Ammonia Synthesis by Heterogeneous Catalysis
Our ﬁrst example involves an industrial process of immense importance: the catalytic synthesis of ammonia (NH3). Ammonia is a central component of

1.2 EXAMPLES OF DFT IN ACTION 3
fertilizers for agriculture, and more than 100 million tons of ammonia are produced commercially each year. By some estimates, more than 1% of all energy used in the world is consumed in the production of ammonia. The core reaction in ammonia production is very simple:
N2 þ 3H2 ! 2NH3:
To get this reaction to proceed, the reaction is performed at high temperatures (.4008C) and high pressures (.100 atm) in the presence of metals such as iron (Fe) or ruthenium (Ru) that act as catalysts. Although these metal catalysts were identiﬁed by Haber and others almost 100 years ago, much is still not known about the mechanisms of the reactions that occur on the surfaces of these catalysts. This incomplete understanding is partly because of the structural complexity of practical catalysts. To make metal catalysts with high surface areas, tiny particles of the active metal are dispersed throughout highly porous materials. This was a widespread application of nanotechnology long before that name was applied to materials to make them sound scientiﬁcally exciting! To understand the reactivity of a metal nanoparticle, it is useful to characterize the surface atoms in terms of their local coordination since differences in this coordination can create differences in chemical reactivity; surface atoms can be classiﬁed into “types” based on their local coordination. The surfaces of nanoparticles typically include atoms of various types (based on coordination), so the overall surface reactivity is a complicated function of the shape of the nanoparticle and the reactivity of each type of atom.
The discussion above raises a fundamental question: Can a direct connection be made between the shape and size of a metal nanoparticle and its activity as a catalyst for ammonia synthesis? If detailed answers to this question can be found, then they can potentially lead to the synthesis of improved catalysts. One of the most detailed answers to this question to date has come from the DFT calculations of Honkala and co-workers,1 who studied nanoparticles of Ru. Using DFT calculations, they showed that the net chemical reaction above proceeds via at least 12 distinct steps on a metal catalyst and that the rates of these steps depend strongly on the local coordination of the metal atoms that are involved. One of the most important reactions is the breaking of the N2 bond on the catalyst surface. On regions of the catalyst surface that were similar to the surfaces of bulk Ru (more speciﬁcally, atomically ﬂat regions), a great deal of energy is required for this bond-breaking reaction, implying that the reaction rate is extremely slow. Near Ru atoms that form a common kind of surface step edge on the catalyst, however, a much smaller amount of energy is needed for this reaction. Honkala and co-workers used additional DFT calculations to predict the relative stability of many different local coordinations of surface atoms in Ru nanoparticles in a way that allowed

4 WHAT IS DENSITY FUNCTIONAL THEORY?
them to predict the detailed shape of the nanoparticles as a function of particle size. This prediction makes a precise connection between the diameter of a Ru nanoparticle and the number of highly desirable reactive sites for breaking N2 bonds on the nanoparticle. Finally, all of these calculations were used to develop an overall model that describes how the individual reaction rates for the many different kinds of metal atoms on the nanoparticle’s surfaces couple together to deﬁne the overall reaction rate under realistic reaction conditions. At no stage in this process was any experimental data used to ﬁt or adjust the model, so the ﬁnal result was a truly predictive description of the reaction rate of a complex catalyst. After all this work was done, Honkala et al. compared their predictions to experimental measurements made with Ru nanoparticle catalysts under reaction conditions similar to industrial conditions. Their predictions were in stunning quantitative agreement with the experimental outcome.
1.2.2 Embrittlement of Metals by Trace Impurities
It is highly likely that as you read these words you are within 1 m of a large number of copper wires since copper is the dominant metal used for carrying electricity between components of electronic devices of all kinds. Aside from its low cost, one of the attractions of copper in practical applications is that it is a soft, ductile metal. Common pieces of copper (and other metals) are almost invariably polycrystalline, meaning that they are made up of many tiny domains called grains that are each well-oriented single crystals. Two neighboring grains have the same crystal structure and symmetry, but their orientation in space is not identical. As a result, the places where grains touch have a considerably more complicated structure than the crystal structure of the pure metal. These regions, which are present in all polycrystalline materials, are called grain boundaries.
It has been known for over 100 years that adding tiny amounts of certain impurities to copper can change the metal from being ductile to a material that will fracture in a brittle way (i.e., without plastic deformation before the fracture). This occurs, for example, when bismuth (Bi) is present in copper (Cu) at levels below 100 ppm. Similar effects have been observed with lead (Pb) or mercury (Hg) impurities. But how does this happen? Qualitatively, when the impurities cause brittle fracture, the fracture tends to occur at grain boundaries, so something about the impurities changes the properties of grain boundaries in a dramatic way. That this can happen at very low concentrations of Bi is not completely implausible because Bi is almost completely insoluble in bulk Cu. This means that it is very favorable for Bi atoms to segregate to grain boundaries rather than to exist inside grains, meaning that the

1.2 EXAMPLES OF DFT IN ACTION 5
local concentration of Bi at grain boundaries can be much higher than the net concentration in the material as a whole.
Can the changes in copper caused by Bi be explained in a detailed way? As you might expect for an interesting phenomena that has been observed over many years, several alternative explanations have been suggested. One class of explanations assigns the behavior to electronic effects. For example, a Bi atom might cause bonds between nearby Cu atoms to be stiffer than they are in pure Cu, reducing the ability of the Cu lattice to deform smoothly. A second type of electronic effect is that having an impurity atom next to a grain boundary could weaken the bonds that exist across a boundary by changing the electronic structure of the atoms, which would make fracture at the boundary more likely. A third explanation assigns the blame to size effects, noting that Bi atoms are much larger than Cu atoms. If a Bi atom is present at a grain boundary, then it might physically separate Cu atoms on the other side of the boundary from their natural spacing. This stretching of bond distances would weaken the bonds between atoms and make fracture of the grain boundary more likely. Both the second and third explanations involve weakening of bonds near grain boundaries, but they propose different root causes for this behavior. Distinguishing between these proposed mechanisms would be very difﬁcult using direct experiments.
Recently, Schweinfest, Paxton, and Finnis used DFT calculations to offer a deﬁnitive description of how Bi embrittles copper; the title of their study gives away the conclusion.2 They ﬁrst used DFT to predict stress–strain relationships for pure Cu and Cu containing Bi atoms as impurities. If the bond stiffness argument outlined above was correct, the elastic moduli of the metal should be increased by adding Bi. In fact, the calculations give the opposite result, immediately showing the bond-stiffening explanation to be incorrect. In a separate and much more challenging series of calculations, they explicitly calculated the cohesion energy of a particular grain boundary that is known experimentally to be embrittled by Bi. In qualitative consistency with experimental observations, the calculations predicted that the cohesive energy of the grain boundary is greatly reduced by the presence of Bi. Crucially, the DFT results allow the electronic structure of the grain boundary atoms to be examined directly. The result is that the grain boundary electronic effect outlined above was found to not be the cause of embrittlement. Instead, the large change in the properties of the grain boundary could be understood almost entirely in terms of the excess volume introduced by the Bi atoms, that is, by a size effect. This reasoning suggests that Cu should be embrittled by any impurity that has a much larger atomic size than Cu and that strongly segregates to grain boundaries. This description in fact correctly describes the properties of both Pb and Hg as impurities in Cu, and, as mentioned above, these impurities are known to embrittle Cu.

6 WHAT IS DENSITY FUNCTIONAL THEORY?
1.2.3 Materials Properties for Modeling Planetary Formation
To develop detailed models of how planets of various sizes have formed, it is necessary to know (among many other things) what minerals exist inside planets and how effective these minerals are at conducting heat. The extreme conditions that exist inside planets pose some obvious challenges to probing these topics in laboratory experiments. For example, the center of Jupiter has pressures exceeding 40 Mbar and temperatures well above 15,000 K. DFT calculations can play a useful role in probing material properties at these extreme conditions, as shown in the work of Umemoto, Wentzcovitch, and Allen.3 This work centered on the properties of bulk MgSiO3, a silicate mineral that is important in planet formation. At ambient conditions, MgSiO3 forms a relatively common crystal structure known as a perovskite. Prior to Umemoto et al.’s calculations, it was known that if MgSiO3 was placed under conditions similar to those in the core–mantle boundary of Earth, it transforms into a different crystal structure known as the CaIrO3 structure. (It is conventional to name crystal structures after the ﬁrst compound discovered with that particular structure, and the naming of this structure is an example of this convention.)
Umemoto et al. wanted to understand what happens to the structure of MgSiO3 at conditions much more extreme than those found in Earth’s core– mantle boundary. They used DFT calculations to construct a phase diagram that compared the stability of multiple possible crystal structures of solid MgSiO3. All of these calculations dealt with bulk materials. They also considered the possibility that MgSiO3 might dissociate into other compounds. These calculations predicted that at pressures of $11 Mbar, MgSiO3 dissociates in the following way:
MgSiO3 [CaIrO3 structure] ! MgO [CsCl structure]
þ SiO2 [cotunnite structure]:
In this reaction, the crystal structure of each compound has been noted in the square brackets. An interesting feature of the compounds on the right-hand side is that neither of them is in the crystal structure that is the stable structure at ambient conditions. MgO, for example, prefers the NaCl structure at ambient conditions (i.e., the same crystal structure as everyday table salt). The behavior of SiO2 is similar but more complicated; this compound goes through several intermediate structures between ambient conditions and the conditions relevant for MgSiO3 dissociation. These transformations in the structures of MgO and SiO2 allow an important connection to be made between DFT calculations and experiments since these transformations occur at conditions that can be directly probed in laboratory experiments. The transition pressures

1.3 THE SCHRO¨ DINGER EQUATION 7
predicted using DFT and observed experimentally are in good agreement, giving a strong indication of the accuracy of these calculations.
The dissociation reaction predicted by Umemoto et al.’s calculations has important implications for creating good models of planetary formation. At the simplest level, it gives new information about what materials exist inside large planets. The calculations predict, for example, that the center of Uranus or Neptune can contain MgSiO3, but that the cores of Jupiter or Saturn will not. At a more detailed level, the thermodynamic properties of the materials can be used to model phenomena such as convection inside planets. Umemoto et al. speculated that the dissociation reaction above might severely limit convection inside “dense-Saturn,” a Saturn-like planet that has been discovered outside the solar system with a mass of $67 Earth masses.
A legitimate concern about theoretical predictions like the reaction above is that it is difﬁcult to envision how they can be validated against experimental data. Fortunately, DFT calculations can also be used to search for similar types of reactions that occur at pressures that are accessible experimentally. By using this approach, it has been predicted that NaMgF3 goes through a series of transformations similar to MgSiO3; namely, a perovskite to postperovskite transition at some pressure above ambient and then dissociation in NaF and MgF2 at higher pressures.4 This dissociation is predicted to occur for pressures around 0.4 Mbar, far lower than the equivalent pressure for MgSiO3. These predictions suggest an avenue for direct experimental tests of the transformation mechanism that DFT calculations have suggested plays a role in planetary formation.
We could ﬁll many more pages with research vignettes showing how DFT calculations have had an impact in many areas of science. Hopefully, these three examples give some ﬂavor of the ways in which DFT calculations can have an impact on scientiﬁc understanding. It is useful to think about the common features between these three examples. All of them involve materials in their solid state, although the ﬁrst example was principally concerned with the interface between a solid and a gas. Each example generated information about a physical problem that is controlled by the properties of materials on atomic length scales that would be (at best) extraordinarily challenging to probe experimentally. In each case, the calculations were used to give information not just about some theoretically ideal state, but instead to understand phenomena at temperatures, pressures, and chemical compositions of direct relevance to physical applications.
1.3 THE SCHRO¨ DINGER EQUATION
By now we have hopefully convinced you that density functional theory is a useful and interesting topic. But what is it exactly? We begin with

8 WHAT IS DENSITY FUNCTIONAL THEORY?
the observation that one of the most profound scientiﬁc advances of the twentieth century was the development of quantum mechanics and the repeated experimental observations that conﬁrmed that this theory of matter describes, with astonishing accuracy, the universe in which we live.
In this section, we begin a review of some key ideas from quantum mechanics that underlie DFT (and other forms of computational chemistry). Our goal here is not to present a complete derivation of the techniques used in DFT. Instead, our goal is to give a clear, brief, introductory presentation of the most basic equations important for DFT. For the full story, there are a number of excellent texts devoted to quantum mechanics listed in the Further Reading section at the end of the chapter.
Let us imagine a situation where we would like to describe the properties of some well-deﬁned collection of atoms—you could think of an isolated molecule or the atoms deﬁning the crystal of an interesting mineral. One of the fundamental things we would like to know about these atoms is their energy and, more importantly, how their energy changes if we move the atoms around. To deﬁne where an atom is, we need to deﬁne both where its nucleus is and where the atom’s electrons are. A key observation in applying quantum mechanics to atoms is that atomic nuclei are much heavier than individual electrons; each proton or neutron in a nucleus has more than 1800 times the mass of an electron. This means, roughly speaking, that electrons respond much more rapidly to changes in their surroundings than nuclei can. As a result, we can split our physical question into two pieces. First, we solve, for ﬁxed positions of the atomic nuclei, the equations that describe the electron motion. For a given set of electrons moving in the ﬁeld of a set of nuclei, we ﬁnd the lowest energy conﬁguration, or state, of the electrons. The lowest energy state is known as the ground state of the electrons, and the separation of the nuclei and electrons into separate mathematical problems is the Born – Oppenheimer approximation. If we have M nuclei at positions R1, . . . , RM, then we can express the ground-state energy, E, as a function of the positions of these nuclei, E(R1, . . . , RM). This function is known as the adiabatic potential energy surface of the atoms. Once we are able to calculate this potential energy surface we can tackle the original problem posed above—how does the energy of the material change as we move its atoms around?
One simple form of the Schro¨dinger equation—more precisely, the timeindependent, nonrelativistic Schro¨dinger equation—you may be familiar with is Hc ¼ Ec. This equation is in a nice form for putting on a T-shirt or a coffee mug, but to understand it better we need to deﬁne the quantities that appear in it. In this equation, H is the Hamiltonian operator and c is a set of solutions, or eigenstates, of the Hamiltonian. Each of these solutions,

1.3 THE SCHRO¨ DINGER EQUATION 9

cn, has an associated eigenvalue, En, a real numberÃ that satisﬁes the eigenvalue equation. The detailed deﬁnition of the Hamiltonian depends on the physical system being described by the Schro¨dinger equation. There are several well-known examples like the particle in a box or a harmonic oscillator where the Hamiltonian has a simple form and the Schro¨dinger equation can be solved exactly. The situation we are interested in where multiple electrons are interacting with multiple nuclei is more complicated. In this case, a more complete description of the Schro¨dinger is

"

h2 2m

N
X
i1

r2i

þ

N
X
i1

V (ri)

þ

N
X
i1

X
j,i

U(ri,

# rj) c

¼

Ec:

(1:1)

Here, m is the electron mass. The three terms in brackets in this equation deﬁne, in order, the kinetic energy of each electron, the interaction energy between each electron and the collection of atomic nuclei, and the interaction energy between different electrons. For the Hamiltonian we have chosen, c is the electronic wave function, which is a function of each of the spatial coordinates of each of the N electrons, so c ¼ c(r1, . . . , rN), and E is the groundstate energy of the electrons.ÃÃ The ground-state energy is independent of time, so this is the time-independent Schro¨dinger equation.†
Although the electron wave function is a function of each of the coordinates of all N electrons, it is possible to approximate c as a product of individual electron wave functions, c ¼ c1(r)c2(r), . . . , cN(r). This expression for the wave function is known as a Hartree product, and there are good motivations for approximating the full wave function into a product of individual oneelectron wave functions in this fashion. Notice that N, the number of electrons, is considerably larger than M, the number of nuclei, simply because each atom has one nucleus and lots of electrons. If we were interested in a single molecule of CO2, the full wave function is a 66-dimensional function (3 dimensions for each of the 22 electrons). If we were interested in a nanocluster of 100 Pt atoms, the full wave function requires more the 23,000 dimensions! These numbers should begin to give you an idea about why solving the Schro¨dinger equation for practical materials has occupied many brilliant minds for a good fraction of a century.

ÃThe value of the functions cn are complex numbers, but the eigenvalues of the Schro¨dinger equation are real numbers. ÃÃFor clarity of presentation, we have neglected electron spin in our description. In a complete
presentation, each electron is deﬁned by three spatial variables and its spin. †The dynamics of electrons are deﬁnped by the time dependent Schro¨dinger equation, ih(@c=@t) ¼ Hc. The appearance of i ¼ 1 in this equation makes it clear that the wave func tion is a complex valued function, not a real valued function.

10 WHAT IS DENSITY FUNCTIONAL THEORY?

The situation looks even worse when we look again at the Hamiltonian, H. The term in the Hamiltonian deﬁning electron –electron interactions is the most critical one from the point of view of solving the equation. The form of this contribution means that the individual electron wave function we deﬁned above, ci(r), cannot be found without simultaneously considering the individual electron wave functions associated with all the other electrons. In other words, the Schro¨dinger equation is a many-body problem.
Although solving the Schro¨dinger equation can be viewed as the fundamental problem of quantum mechanics, it is worth realizing that the wave function for any particular set of coordinates cannot be directly observed. The quantity that can (in principle) be measured is the probability that the N electrons are at a particular set of coordinates, r1, . . . , rN. This probability is equal to cÃ(r1, . . . , rN)c(r1, . . . , rN), where the asterisk indicates a complex conjugate. A further point to notice is that in experiments we typically do not care which electron in the material is labeled electron 1, electron 2, and so on. Moreover, even if we did care, we cannot easily assign these labels. This means that the quantity of physical interest is really the probability that a set of N electrons in any order have coordinates r1, . . . , rN. A closely related quantity is the density of electrons at a particular position in space, n(r). This can be written in terms of the individual electron wave functions as

n(r) ¼ 2 X cÃi (r)ci(r):
i

(1:2)

Here, the summation goes over all the individual electron wave functions that are occupied by electrons, so the term inside the summation is the probability that an electron in individual wave function ci(r) is located at position r. The factor of 2 appears because electrons have spin and the Pauli exclusion principle states that each individual electron wave function can be occupied by two separate electrons provided they have different spins. This is a purely quantum mechanical effect that has no counterpart in classical physics. The point of this discussion is that the electron density, n(r), which is a function of only three coordinates, contains a great amount of the information that is actually physically observable from the full wave function solution to the Schro¨dinger equation, which is a function of 3N coordinates.

1.4 DENSITY FUNCTIONAL THEORY—FROM WAVE FUNCTIONS TO ELECTRON DENSITY
The entire ﬁeld of density functional theory rests on two fundamental mathematical theorems proved by Kohn and Hohenberg and the derivation of a

1.4 DENSITY FUNCTIONAL THEORY FROM WAVE FUNCTIONS 11
set of equations by Kohn and Sham in the mid-1960s. The ﬁrst theorem, proved by Hohenberg and Kohn, is: The ground-state energy from Schro¨dinger’s equation is a unique functional of the electron density.
This theorem states that there exists a one-to-one mapping between the ground-state wave function and the ground-state electron density. To appreciate the importance of this result, you ﬁrst need to know what a “functional” is. As you might guess from the name, a functional is closely related to the more familiar concept of a function. A function takes a value of a variable or variables and deﬁnes a single number from those variables. A simple example of a function dependent on a single variable is f (x) ¼ x2 þ 1. A functional is similar, but it takes a function and deﬁnes a single number from the function. For example,
1
ð F[ f ] ¼ f (x) dx,
1
is a functional of the function f (x). If we evaluate this functional using f (x) ¼ x2 þ 1, we get F[ f ] ¼ 83. So we can restate Hohenberg and Kohn’s result by saying that the ground-state energy E can be expressed as E[n(r)], where n(r) is the electron density. This is why this ﬁeld is known as density functional theory.
Another way to restate Hohenberg and Kohn’s result is that the ground-state electron density uniquely determines all properties, including the energy and wave function, of the ground state. Why is this result important? It means that we can think about solving the Schro¨dinger equation by ﬁnding a function of three spatial variables, the electron density, rather than a function of 3N variables, the wave function. Here, by “solving the Schro¨dinger equation” we mean, to say it more precisely, ﬁnding the ground-state energy. So for a nanocluster of 100 Pd atoms the theorem reduces the problem from something with more than 23,000 dimensions to a problem with just 3 dimensions.
Unfortunately, although the ﬁrst Hohenberg –Kohn theorem rigorously proves that a functional of the electron density exists that can be used to solve the Schro¨dinger equation, the theorem says nothing about what the functional actually is. The second Hohenberg – Kohn theorem deﬁnes an important property of the functional: The electron density that minimizes the energy of the overall functional is the true electron density corresponding to the full solution of the Schro¨dinger equation. If the “true” functional form were known, then we could vary the electron density until the energy from the functional is minimized, giving us a prescription for ﬁnding the relevant electron density. This variational principle is used in practice with approximate forms of the functional.

12 WHAT IS DENSITY FUNCTIONAL THEORY?
A useful way to write down the functional described by the Hohenberg – Kohn theorem is in terms of the single-electron wave functions, ci(r). Remember from Eq. (1.2) that these functions collectively deﬁne the electron density, n(r). The energy functional can be written as

E[{ci}] ¼ Eknown[{ci}] þ EXC[{ci}],

(1:3)

where we have split the functional into a collection of terms we can write down
in a simple analytical form, Eknown[{ci}], and everything else, EXC. The “known” terms include four contributions:

Eknown[{ci}] ¼

h2 m

X
i

ð

cÃi r2cid3r

þ

ð

V

(r)n(r)

d3r

þ e2 ð 2

ð

n(r)n(r0) jr r0j

d3r

d3r0 þ Eion:

(1:4)

The terms on the right are, in order, the electron kinetic energies, the Coulomb interactions between the electrons and the nuclei, the Coulomb interactions between pairs of electrons, and the Coulomb interactions between pairs of nuclei. The other term in the complete energy functional, EXC[{ci}], is the exchange –correlation functional, and it is deﬁned to include all the quantum mechanical effects that are not included in the “known” terms.
Let us imagine for now that we can express the as-yet-undeﬁned exchange – correlation energy functional in some useful way. What is involved in ﬁnding minimum energy solutions of the total energy functional? Nothing we have presented so far really guarantees that this task is any easier than the formidable task of fully solving the Schro¨dinger equation for the wave function. This difﬁculty was solved by Kohn and Sham, who showed that the task of ﬁnding the right electron density can be expressed in a way that involves solving a set of equations in which each equation only involves a single electron.
The Kohn –Sham equations have the form

h2 2m

r2

þ

V (r)

þ

VH (r)

þ

! VXC(r) ci(r)

¼

1ici(r):

(1:5)

These equations are superﬁcially similar to Eq. (1.1). The main difference is that the Kohn– Sham equations are missing the summations that appear inside the full Schro¨dinger equation [Eq. (1.1)]. This is because the solution of the Kohn– Sham equations are single-electron wave functions that depend on only three spatial variables, ci(r). On the left-hand side of the Kohn– Sham equations there are three potentials, V, VH, and VXC. The ﬁrst

1.4 DENSITY FUNCTIONAL THEORY FROM WAVE FUNCTIONS 13

of these also appeared in the full Schro¨dinger equation (Eq. (1.1)) and in the “known” part of the total energy functional given above (Eq. (1.4)). This potential deﬁnes the interaction between an electron and the collection of atomic nuclei. The second is called the Hartree potential and is deﬁned by

VH (r)

¼

e2

ð

n(r0) jr r0

jd3

r0:

(1:6)

This potential describes the Coulomb repulsion between the electron being considered in one of the Kohn – Sham equations and the total electron density deﬁned by all electrons in the problem. The Hartree potential includes a socalled self-interaction contribution because the electron we are describing in the Kohn – Sham equation is also part of the total electron density, so part of VH involves a Coulomb interaction between the electron and itself. The selfinteraction is unphysical, and the correction for it is one of several effects that are lumped together into the ﬁnal potential in the Kohn– Sham equations, VXC, which deﬁnes exchange and correlation contributions to the singleelectron equations. VXC can formally be deﬁned as a “functional derivative” of the exchange – correlation energy:

VXC(r)

¼

dEXC(r) dn(r)

:

(1:7)

The strict mathematical deﬁnition of a functional derivative is slightly more subtle than the more familiar deﬁnition of a function’s derivative, but conceptually you can think of this just as a regular derivative. The functional derivative is written using d rather than d to emphasize that it not quite identical to a normal derivative.
If you have a vague sense that there is something circular about our discussion of the Kohn –Sham equations you are exactly right. To solve the Kohn – Sham equations, we need to deﬁne the Hartree potential, and to deﬁne the Hartree potential we need to know the electron density. But to ﬁnd the electron density, we must know the single-electron wave functions, and to know these wave functions we must solve the Kohn –Sham equations. To break this circle, the problem is usually treated in an iterative way as outlined in the following algorithm:

1. Deﬁne an initial, trial electron density, n(r).
2. Solve the Kohn –Sham equations deﬁned using the trial electron density to ﬁnd the single-particle wave functions, ci(r).
3. Calculate the electron density deﬁned by the Kohn – Sham singleparticle wave functions from step 2, nKS(r) ¼ 2 P cÃi (r)ci(r).
i

14 WHAT IS DENSITY FUNCTIONAL THEORY?
4. Compare the calculated electron density, nKS(r), with the electron density used in solving the Kohn –Sham equations, n(r). If the two densities are the same, then this is the ground-state electron density, and it can be used to compute the total energy. If the two densities are different, then the trial electron density must be updated in some way. Once this is done, the process begins again from step 2.
We have skipped over a whole series of important details in this process (How close do the two electron densities have to be before we consider them to be the same? What is a good way to update the trial electron density? How should we deﬁne the initial density?), but you should be able to see how this iterative method can lead to a solution of the Kohn –Sham equations that is self-consistent.

1.5 EXCHANGE – CORRELATION FUNCTIONAL
Let us brieﬂy review what we have seen so far. We would like to ﬁnd the ground-state energy of the Schro¨dinger equation, but this is extremely difﬁcult because this is a many-body problem. The beautiful results of Kohn, Hohenberg, and Sham showed us that the ground state we seek can be found by minimizing the energy of an energy functional, and that this can be achieved by ﬁnding a self-consistent solution to a set of single-particle equations. There is just one critical complication in this otherwise beautiful formulation: to solve the Kohn – Sham equations we must specify the exchange –correlation function, EXC[{ci}]. As you might gather from Eqs. (1.3) and (1.4), deﬁning EXC[{ci}] is very difﬁcult. After all, the whole point of Eq. (1.4) is that we have already explicitly written down all the “easy” parts.
In fact, the true form of the exchange – correlation functional whose existence is guaranteed by the Hohenberg –Kohn theorem is simply not known. Fortunately, there is one case where this functional can be derived exactly: the uniform electron gas. In this situation, the electron density is constant at all points in space; that is, n(r) ¼ constant. This situation may appear to be of limited value in any real material since it is variations in electron density that deﬁne chemical bonds and generally make materials interesting. But the uniform electron gas provides a practical way to actually use the Kohn – Sham equations. To do this, we set the exchange –correlation potential at each position to be the known exchange –correlation potential from the uniform electron gas at the electron density observed at that position:

VXC(r) ¼ VXeClectron gas[n(r)]:

(1:8)

1.5 EXCHANGE CORRELATION FUNCTIONAL 15
This approximation uses only the local density to deﬁne the approximate exchange – correlation functional, so it is called the local density approximation (LDA). The LDA gives us a way to completely deﬁne the Kohn– Sham equations, but it is crucial to remember that the results from these equations do not exactly solve the true Schro¨dinger equation because we are not using the true exchange – correlation functional.
It should not surprise you that the LDA is not the only functional that has been tried within DFT calculations. The development of functionals that more faithfully represent nature remains one of the most important areas of active research in the quantum chemistry community. We promised at the beginning of the chapter to pose a problem that could win you the Nobel prize. Here it is: Develop a functional that accurately represents nature’s exact functional and implement it in a mathematical form that can be efﬁciently solved for large numbers of atoms. (This advice is a little like the Hohenberg –Kohn theorem—it tells you that something exists without providing any clues how to ﬁnd it.)
Even though you could become a household name (at least in scientiﬁc circles) by solving this problem rigorously, there are a number of approximate functionals that have been found to give good results in a large variety of physical problems and that have been widely adopted. The primary aim of this book is to help you understand how to do calculations with these existing functionals. The best known class of functional after the LDA uses information about the local electron density and the local gradient in the electron density; this approach deﬁnes a generalized gradient approximation (GGA). It is tempting to think that because the GGA includes more physical information than the LDA it must be more accurate. Unfortunately, this is not always correct.
Because there are many ways in which information from the gradient of the electron density can be included in a GGA functional, there are a large number of distinct GGA functionals. Two of the most widely used functionals in calculations involving solids are the Perdew –Wang functional (PW91) and the Perdew –Burke– Ernzerhof functional (PBE). Each of these functionals are GGA functionals, and dozens of other GGA functionals have been developed and used, particularly for calculations with isolated molecules. Because different functionals will give somewhat different results for any particular conﬁguration of atoms, it is necessary to specify what functional was used in any particular calculation rather than simple referring to “a DFT calculation.”
Our description of GGA functionals as including information from the electron density and the gradient of this density suggests that more sophisticated functionals can be constructed that use other pieces of physical information. In fact, a hierarchy of functionals can be constructed that gradually include

16 WHAT IS DENSITY FUNCTIONAL THEORY?
more and more detailed physical information. More information about this hierarchy of functionals is given in Section 10.2.

1.6 THE QUANTUM CHEMISTRY TOURIST
As you read about the approaches aside from DFT that exist for ﬁnding numerical solutions of the Schro¨dinger equation, it is likely that you will rapidly encounter a bewildering array of acronyms. This experience could be a little bit like visiting a sophisticated city in an unfamiliar country. You may recognize that this new city is beautiful, and you deﬁnitely wish to appreciate its merits, but you are not planning to live there permanently. You could spend years in advance of your trip studying the language, history, culture, and geography of the country before your visit, but most likely for a brief visit you are more interested in talking with some friends who have already visited there, reading a few travel guides, browsing a phrase book, and perhaps trying to identify a few good local restaurants. This section aims to present an overview of quantum chemical methods on the level of a phrase book or travel guide.

1.6.1 Localized and Spatially Extended Functions
One useful way to classify quantum chemistry calculations is according to the types of functions they use to represent their solutions. Broadly speaking, these methods use either spatially localized functions or spatially extended functions. As an example of a spatially localized function, Fig. 1.1 shows the function

f (x) ¼ f1(x) þ f2(x) þ f3(x),

(1:9)

where

f1(x) ¼ exp( x2),

f2(x) ¼ x2 exp( x2=2),

f3(x)

¼

1 10

x2(1

x)2 exp( x2=4).

Figure 1.1 also shows f1, f2, and f3. All of these functions rapidly approach zero for large values of jxj. Functions like this are entirely appropriate for representing the wave function or electron density of an isolated atom. This example incorporates the idea that we can combine multiple individual functions with different spatial extents, symmetries, and so on to deﬁne an overall function. We could include more information in this ﬁnal function by including more individual functions within its deﬁnition. Also, we could build up functions that describe multiple atoms simply by using an appropriate set of localized functions for each individual atom.

1.6 THE QUANTUM CHEMISTRY TOURIST 17

Figure 1.1 Example of spatially localized functions deﬁned in the text.

Spatially localized functions are an extremely useful framework for thinking about the quantum chemistry of isolated molecules because the wave functions of isolated molecules really do decay to zero far away from the molecule. But what if we are interested in a bulk material such as the atoms in solid silicon or the atoms beneath the surface of a metal catalyst? We could still use spatially localized functions to describe each atom and add up these functions to describe the overall material, but this is certainly not the only way forward. A useful alternative is to use periodic functions to describe the wave functions or electron densities. Figure 1.2 shows a simple example of this idea by plotting

f (x) ¼ f1(x) þ f2(x) þ f3(x),

where

f1(x)

¼

sin2px, 4

f2(x)

¼

1 3

cos2px, 2

f3(x)

¼

1 10

sin2

(px):

The resulting function is periodic; that is

f (x þ 4n) ¼ f (x),

18 WHAT IS DENSITY FUNCTIONAL THEORY?
Figure 1.2 Example of spatially periodic functions deﬁned in the text.
for any integer n. This type of function is useful for describing bulk materials since at least for defect-free materials the electron density and wave function really are spatially periodic functions.
Because spatially localized functions are the natural choice for isolated molecules, the quantum chemistry methods developed within the chemistry community are dominated by methods based on these functions. Conversely, because physicists have historically been more interested in bulk materials than in individual molecules, numerical methods for solving the Schro¨dinger equation developed in the physics community are dominated by spatially periodic functions. You should not view one of these approaches as “right” and the other as “wrong” as they both have advantages and disadvantages.
1.6.2 Wave-Function-Based Methods A second fundamental classiﬁcation of quantum chemistry calculations can be made according to the quantity that is being calculated. Our introduction to DFT in the previous sections has emphasized that in DFT the aim is to compute the electron density, not the electron wave function. There are many methods, however, where the object of the calculation is to compute the full electron wave function. These wave-function-based methods hold a crucial advantage over DFT calculations in that there is a well-deﬁned hierarchy of methods that, given inﬁnite computer time, can converge to the exact solution of the Schro¨dinger equation. We cannot do justice to the breadth of this ﬁeld in just a few paragraphs, but several excellent introductory texts are available

1.6 THE QUANTUM CHEMISTRY TOURIST 19
and are listed in the Further Reading section at the end of this chapter. The strong connections between DFT and wave-function-based methods and their importance together within science was recognized in 1998 when the Nobel prize in chemistry was awarded jointly to Walter Kohn for his work developing the foundations of DFT and John Pople for his groundbreaking work on developing a quantum chemistry computer code for calculating the electronic structure of atoms and molecules. It is interesting to note that this was the ﬁrst time that a Nobel prize in chemistry or physics was awarded for the development of a numerical method (or more precisely, a class of numerical methods) rather than a distinct scientiﬁc discovery. Kohn’s Nobel lecture gives a very readable description of the advantages and disadvantages of wave-function-based and DFT calculations.5
Before giving a brief discussion of wave-function-based methods, we must ﬁrst describe the common ways in which the wave function is described. We mentioned earlier that the wave function of an N-particle system is an N-dimensional function. But what, exactly, is a wave function? Because we want our wave functions to provide a quantum mechanical description of a system of N electrons, these wave functions must satisfy several mathematical properties exhibited by real electrons. For example, the Pauli exclusion principle prohibits two electrons with the same spin from existing at the same physical location simultaneously.‡ We would, of course, like these properties to also exist in any approximate form of the wave function that we construct.

1.6.3 Hartree–Fock Method
Suppose we would like to approximate the wave function of N electrons. Let us assume for the moment that the electrons have no effect on each other. If this is true, the Hamiltonian for the electrons may be written as

N
X H ¼ hi,
i1

(1:10)

where hi describes the kinetic and potential energy of electron i. The full electronic Hamiltonian we wrote down in Eq. (1.1) takes this form if we simply neglect electron–electron interactions. If we write down the Schro¨dinger

‡Spin is a quantum mechanical property that does not appear in classical mechanics. An electron can have one of two distinct spins, spin up or spin down. The full speciﬁcation of an electron’s state must include both its location and its spin. The Pauli exclusion principle only applies to electrons with the same spin state.

20 WHAT IS DENSITY FUNCTIONAL THEORY?

equation for just one electron based on this Hamiltonian, the solutions would satisfy

hx ¼ Ex:

(1:11)

The eigenfunctions deﬁned by this equation are called spin orbitals. For each single-electron equation there are multiple eigenfunctions, so this deﬁnes a set of spin orbitals xj(xi) ( j 1, 2, . . .) where xi is a vector of coordinates that deﬁnes the position of electron i and its spin state (up or down). We will denote the energy of spin orbital xj(xi) by Ej. It is useful to label the spin orbitals so that the orbital with j 1 has the lowest energy, the orbital with j 2 has the next highest energy, and so on. When the total Hamiltonian is simply a sum of one-electron operators, hi, it follows that the eigenfunctions of H are products of the one-electron spin orbitals:

c(x1, . . . , xN) ¼ x j1 (x1)x j2 (x2) Á Á Á x jN (xN ):

(1:12)

The energy of this wave function is the sum of the spin orbital energies, E ¼ E j1 þ Á Á Á þ E jN . We have already seen a brief glimpse of this approximation to the N-electron wave function, the Hartree product, in Section 1.3.
Unfortunately, the Hartree product does not satisfy all the important criteria for wave functions. Because electrons are fermions, the wave function must change sign if two electrons change places with each other. This is known as the antisymmetry principle. Exchanging two electrons does not change the sign of the Hartree product, which is a serious drawback. We can obtain a better approximation to the wave function by using a Slater determinant. In a Slater determinant, the N-electron wave function is formed by combining one-electron wave functions in a way that satisﬁes the antisymmetry principle. This is done by expressing the overall wave function as the determinant of a matrix of single-electron wave functions. It is best to see how this works for the case of two electrons. For two electrons, the Slater determinant is

c(x1,

x2)

¼

p1 2

det

xj(x1) xk (x1 )

xj(x2) ! xk (x1 )

¼

p1 2

h xj (x1 )xk (x1 )

i xj(x2)xk(x1) :

(1:13)

The coefﬁcient of (1/p2) is simply a normalization factor. This expression builds in a physical description of electron exchange implicitly; it changes

sign if two electrons are exchanged. This expression has other advantages.

For example, it does not distinguish between electrons and it disappears if two electrons have the same coordinates or if two of the one-electron wave functions are the same. This means that the Slater determinant satisﬁes

1.6 THE QUANTUM CHEMISTRY TOURIST 21
the conditions of the Pauli exclusion principle. The Slater determinant may be generalized to a system of N electrons easily; it is the determinant of an N Â N matrix of single-electron spin orbitals. By using a Slater determinant, we are ensuring that our method for solving the Schro¨dinger equation will include exchange. Unfortunately, this is not the only kind of electron correlation that we need to describe in order to arrive at good computational accuracy.
The description above may seem a little unhelpful since we know that in any interesting system the electrons interact with one another. The many different wave-function-based approaches to solving the Schro¨dinger equation differ in how these interactions are approximated. To understand the types of approximations that can be used, it is worth looking at the simplest approach, the Hartree–Fock method, in some detail. There are also many similarities between Hartree– Fock calculations and the DFT calculations we have described in the previous sections, so understanding this method is a useful way to view these ideas from a slightly different perspective.
In a Hartree–Fock (HF) calculation, we ﬁx the positions of the atomic nuclei and aim to determine the wave function of N-interacting electrons. The ﬁrst part of describing an HF calculation is to deﬁne what equations are solved. The Schro¨dinger equation for each electron is written as

h2 2m

r2

þ

V

(r)

þ

! VH(r) xj(x)

¼

Ejxj(x):

(1:14)

The third term on the left-hand side is the same Hartree potential we saw in Eq. (1.5):

VH

(r)

¼

e2

ð

n(r0) jr r0j

d3r0:

(1:15)

In plain language, this means that a single electron “feels” the effect of other electrons only as an average, rather than feeling the instantaneous repulsive forces generated as electrons become close in space. If you compare Eq. (1.14) with the Kohn– Sham equations, Eq. (1.5), you will notice that the only difference between the two sets of equations is the additional exchange – correlation potential that appears in the Kohn – Sham equations.
To complete our description of the HF method, we have to deﬁne how the solutions of the single-electron equations above are expressed and how these solutions are combined to give the N-electron wave function. The HF approach assumes that the complete wave function can be approximated using a single Slater determinant. This means that the N lowest energy spin orbitals of the

22 WHAT IS DENSITY FUNCTIONAL THEORY?

single-electron equation are found, xj(x) for j 1, . . . , N, and the total wave function is formed from the Slater determinant of these spin orbitals.
To actually solve the single-electron equation in a practical calculation, we have to deﬁne the spin orbitals using a ﬁnite amount of information since we cannot describe an arbitrary continuous function on a computer. To do this, we deﬁne a ﬁnite set of functions that can be added together to approximate the exact spin orbitals. If our ﬁnite set of functions is written as f1(x), f2(x), . . . , fK(x), then we can approximate the spin orbitals as

K
X xj(x) ¼ aj,ifi(x):
i1

(1:16)

When using this expression, we only need to ﬁnd the expansion coefﬁcients, a j,i, for i 1, . . . , K and j 1, . . . , N to fully deﬁne all the spin orbitals that are used in the HF method. The set of functions f1(x), f2(x), . . . , fK(x) is called the basis set for the calculation. Intuitively, you can guess that using a larger basis set (i.e., increasing K ) will increase the accuracy of the calculation but also increase the amount of effort needed to ﬁnd a solution. Similarly, choosing basis functions that are very similar to the types of spin orbitals that actually appear in real materials will improve the accuracy of an HF calculation. As we hinted at in Section 1.6.1, the characteristics of these functions can differ depending on the type of material that is being considered.
We now have all the pieces in place to perform an HF calculation—a basis set in which the individual spin orbitals are expanded, the equations that the spin orbitals must satisfy, and a prescription for forming the ﬁnal wave function once the spin orbitals are known. But there is one crucial complication left to deal with; one that also appeared when we discussed the Kohn – Sham equations in Section 1.4. To ﬁnd the spin orbitals we must solve the singleelectron equations. To deﬁne the Hartree potential in the single-electron equations, we must know the electron density. But to know the electron density, we must deﬁne the electron wave function, which is found using the individual spin orbitals! To break this circle, an HF calculation is an iterative procedure that can be outlined as follows:

1.

Make

an

initial

estimate

of

the

spin

orbitals

xj(x)

¼

PK
i

1

a j,ifi(x)

by

specifying the expansion coefﬁcients, a j,i:

2. From the current estimate of the spin orbitals, deﬁne the electron density, n(r0):

3. Using the electron density from step 2, solve the single-electron

equations for the spin orbitals.

1.6 THE QUANTUM CHEMISTRY TOURIST 23
4. If the spin orbitals found in step 3 are consistent with orbitals used in step 2, then these are the solutions to the HF problem we set out to calculate. If not, then a new estimate for the spin orbitals must be made and we then return to step 2.
This procedure is extremely similar to the iterative method we outlined in Section 1.4 for solving the Kohn –Sham equations within a DFT calculation. Just as in our discussion in Section 1.4, we have glossed over many details that are of great importance for actually doing an HF calculation. To identify just a few of these details: How do we decide if two sets of spin orbitals are similar enough to be called consistent? How can we update the spin orbitals in step 4 so that the overall calculation will actually converge to a solution? How large should a basis set be? How can we form a useful initial estimate of the spin orbitals? How do we efﬁciently ﬁnd the expansion coefﬁcients that deﬁne the solutions to the single-electron equations? Delving into the details of these issues would take us well beyond our aim in this section of giving an overview of quantum chemistry methods, but we hope that you can appreciate that reasonable answers to each of these questions can be found that allow HF calculations to be performed for physically interesting materials.
1.6.4 Beyond Hartree–Fock
The Hartree–Fock method provides an exact description of electron exchange. This means that wave functions from HF calculations have exactly the same properties when the coordinates of two or more electrons are exchanged as the true solutions of the full Schro¨dinger equation. If HF calculations were possible using an inﬁnitely large basis set, the energy of N electrons that would be calculated is known as the Hartree– Fock limit. This energy is not the same as the energy for the true electron wave function because the HF method does not correctly describe how electrons inﬂuence other electrons. More succinctly, the HF method does not deal with electron correlations.
As we hinted at in the previous sections, writing down the physical laws that govern electron correlation is straightforward, but ﬁnding an exact description of electron correlation is intractable for any but the simplest systems. For the purposes of quantum chemistry, the energy due to electron correlation is deﬁned in a speciﬁc way: the electron correlation energy is the difference between the Hartree–Fock limit and the true (non-relativistic) ground-state energy. Quantum chemistry approaches that are more sophisticated than the HF method for approximately solving the Schro¨dinger equation capture some part of the electron correlation energy by improving in some way upon one of the assumptions that were adopted in the Hartree –Fock approach.

24 WHAT IS DENSITY FUNCTIONAL THEORY?
How do more advanced quantum chemical approaches improve on the HF method? The approaches vary, but the common goal is to include a description of electron correlation. Electron correlation is often described by “mixing” into the wave function some conﬁgurations in which electrons have been excited or promoted from lower energy to higher energy orbitals. One group of methods that does this are the single-determinant methods in which a single Slater determinant is used as the reference wave function and excitations are made from that wave function. Methods based on a single reference determinant are formally known as “post –Hartree– Fock” methods. These methods include conﬁguration interaction (CI), coupled cluster (CC), Møller –Plesset perturbation theory (MP), and the quadratic conﬁguration interaction (QCI) approach. Each of these methods has multiple variants with names that describe salient details of the methods. For example, CCSD calculations are coupled-cluster calculations involving excitations of single electrons (S), and pairs of electrons (double—D), while CCSDT calculations further include excitations of three electrons (triples—T). Møller –Plesset perturbation theory is based on adding a small perturbation (the correlation potential) to a zeroorder Hamiltonian (the HF Hamiltonian, usually). In the Møller –Plesset perturbation theory approach, a number is used to indicate the order of the perturbation theory, so MP2 is the second-order theory and so on.
Another class of methods uses more than one Slater determinant as the reference wave function. The methods used to describe electron correlation within these calculations are similar in some ways to the methods listed above. These methods include multiconﬁgurational self-consistent ﬁeld (MCSCF), multireference single and double conﬁguration interaction (MRDCI), and N-electron valence state perturbation theory (NEVPT) methods.§
The classiﬁcation of wave-function-based methods has two distinct components: the level of theory and the basis set. The level of theory deﬁnes the approximations that are introduced to describe electron –electron interactions. This is described by the array of acronyms introduced in the preceding paragraphs that describe various levels of theory. It has been suggested, only half-jokingly, that a useful rule for assessing the accuracy of a quantum chemistry calculation is that “the longer the acronym, the better the level of theory.”6 The second, and equally important, component in classifying wave-functionbased methods is the basis set. In the simple example we gave in Section 1.6.1 of a spatially localized function, we formed an overall function by adding together three individual functions. If we were aiming to approximate a particular function in this way, for example, the solution of the Schro¨dinger
§This may be a good time to remind yourself that this overview of quantum chemistry is meant to act something like a phrase book or travel guide for a foreign city. Details of the methods listed here may be found in the Further Reading section at the end of this chapter.

1.6 THE QUANTUM CHEMISTRY TOURIST 25
equation, we could always achieve this task more accurately by using more functions in our sum. Using a basis set with more functions allows a more accurate representation of the true solution but also requires more computational effort since the numerical coefﬁcients deﬁning the magnitude of each function’s contribution to the net function must be calculated. Just as there are multiple levels of theory that can be used, there are many possible ways to form basis sets.
To illustrate the role of the level of theory and the basis set, we will look at two properties of a molecule of CH4, the C –H bond length and the ionization energy. Experimentally, the C –H bond length is 1.094 A˚ 7 and the ionization energy for methane is 12.61 eV. First, we list these quantities calculated with four different levels of theory using the same basis set in Table 1.1. Three of the levels of theory shown in this table are wave-function-based, namely HF, MP2, and CCSD. We also list results from a DFT calculation using the most popular DFT functional for isolated molecules, that is, the B3LYP functional. (We return at the end of this section to the characteristics of this functional.) The table also shows the computational time needed for each calculation normalized by the time for the HF calculation. An important observation from this column is that the computational time for the HF and DFT calculations are approximately the same—this is a quite general result. The higher levels of theory, particularly the CCSD calculation, take considerably more computational time than the HF (or DFT) calculations.
All of the levels of theory listed in Table 1.1 predict the C –H bond length with accuracy within 1%. One piece of cheering information from Table 1.1 is that the DFT method predicts this bond length as accurately as the much more computationally expensive CCSD approach. The error in the ionization energy predicted by HF is substantial, but all three of the other methods give better predictions. The higher levels of theory (MP2 and CCSD) give considerably more accurate results for this quantity than DFT.
Now we look at the properties of CH4 predicted by a set of calculations in which the level of theory is ﬁxed and the size of the basis set is varied.

TABLE 1.1 Computed Properties of CH4 Molecule for Four Levels of Theory Using pVTZ Basis Seta

Level of Theory

CH (A˚ )

Percent Error

Ionization (eV)

Percent Error

Relative Time

HF

1.085

0.8

11.49

DFT (B3LYP)

1.088

0.5

12.46

MP2

1.085

0.8

12.58

CCSD

1.088

0.5

12.54

8.9

1

1.2

1

0.2

2

0.5

18

aErrors are deﬁned relative to the experimental value.

26 WHAT IS DENSITY FUNCTIONAL THEORY?

TABLE 1.2 Properties of CH4 Calculated Using DFT (B3LYP) with Four Different Basis Setsa

Basis Set

Number of Basis C H Percent Ionization Percent Relative

Functions

(A˚ )

Error

(eV)

Error

Time

STO 3G cc pVDZ cc pVTZ cc pVQZ

27

1.097

0.3

12.08

4.2

1

61

1.100

0.6

12.34

2.2

1

121

1.088

0.5

12.46

1.2

2

240

1.088

0.5

12.46

1.2

13

aErrors are deﬁned relative to the experimental value. Time is deﬁned relative to the STO 3G calculation.

Table 1.2 contains results of this kind using DFT calculations with the B3LYP functional in each case. There is a complicated series of names associated with different basis sets. Without going into the details, let us just say that STO-3G is a very common “minimal” basis set while cc-pVDZ, cc-pVTZ, and ccpVQZ (D stands for double, T for triple, etc.) is a popular series of basis sets that have been carefully developed to be numerically efﬁcient for molecular calculations. The table lists the number of basis functions used in each calculation and also the computational time relative to the most rapid calculation. All of the basis sets listed in Table 1.2 give C –H bond lengths that are within 1% of the experimental value. The ionization energy, however, becomes signiﬁcantly more accurate as the size of the basis set becomes larger.
One other interesting observation from Table 1.2 is that the results for the two largest basis sets, pVTZ and pVQZ, are identical (at least to the numerical precision we listed in the table). This occurs when the basis sets include enough functions to accurately describe the solution of the Schro¨dinger equation, and when it occurs the results are said to be “converged with respect to basis set.” When it happens, this is a good thing! An unfortunate fact of nature is that a basis set that is large enough for one level of theory, say DFT, is not necessarily large enough for higher levels of theory. So the results in Table 1.2 do not imply that the pVTZ basis set used for the CCSD calculations in Table 1.1 were converged with respect to basis set.
In order to use wave-function-based methods to converge to the true solution of the Schro¨dinger equation, it is necessary to simultaneously use a high level of theory and a large basis set. Unfortunately, this approach is only feasible for calculations involving relatively small numbers of atoms because the computational expense associated with these calculations increases rapidly with the level of theory and the number of basis functions. For a basis set with N functions, for example, the computational expense of a conventional HF calculation typically requires $N4 operations, while a conventional coupled-cluster calculation requires $N7 operations. Advances have been made that improve the scaling of both HF and post-HF calculations. Even with these improvements, however you can appreciate the problem with

1.6 THE QUANTUM CHEMISTRY TOURIST 27
scaling if you notice from Table 1.2 that a reasonable basis set for even a tiny molecule like CH4 includes hundreds of basis functions. The computational expense of high-level wave-function-based methods means that these calculations are feasible for individual organic molecules containing 10 –20 atoms, but physical systems larger than this fall into either the “very challenging” or “computationally infeasible” categories.
This brings our brief tour of quantum chemistry almost to an end. As the title of this book suggests, we are going to focus throughout the book on density functional theory calculations. Moreover, we will only consider methods based on spatially periodic functions—the so-called plane-wave methods. Plane-wave methods are the method of choice in almost all situations where the physical material of interest is an extended crystalline material rather than an isolated molecule. As we stated above, it is not appropriate to view methods based on periodic functions as “right” and methods based on spatially localized functions as “wrong” (or vice versa). In the long run, it will be a great advantage to you to understand both classes of methods since having access to a wide range of tools can only improve your chances of solving signiﬁcant scientiﬁc problems. Nevertheless, if you are interested in applying computational methods to materials other than isolated molecules, then plane-wave DFT is an excellent place to start.
It is important for us to emphasize that DFT calculations can also be performed using spatially localized functions—the results in Tables 1.1 and 1.2 are examples of this kind of calculation. Perhaps the main difference between DFT calculations using periodic and spatially localized functions lies in the exchange – correlation functionals that are routinely used. In Section 1.4 we deﬁned the exchange –correlation functional by what it does not include—it is the parts of the complete energy functional that are left once we separate out the contributions that can be written in simple ways. Our discussion of the HF method, however, indicates that it is possible to treat the exchange part of the problem in an exact way, at least in principle. The most commonly used functionals in DFT calculations based on spatially localized basis functions are “hybrid” functionals that mix the exact results for the exchange part of the functional with approximations for the correlation part. The B3LYP functional is by far the most widely used of these hybrid functionals. The B stands for Becke, who worked on the exchange part of the problem, the LYP stands for Lee, Yang, and Parr, who developed the correlation part of the functional, and the 3 describes the particular way that the results are mixed together. Unfortunately, the form of the exact exchange results mean that they can be efﬁciently implemented for applications based on spatially localized functions but not for applications using periodic functions! Because of this fact, the functionals that are commonly used in plane-wave DFT calculations do not include contributions from the exact exchange results.

28 WHAT IS DENSITY FUNCTIONAL THEORY?
1.7 WHAT CAN DFT NOT DO?
It is very important to come to grips with the fact that practical DFT calculations are not exact solutions of the full Schro¨dinger equation. This inexactness exists because the exact functional that the Hohenberg –Kohn theorem applies to is not known. So any time you (or anyone else) performs a DFT calculation, there is an intrinsic uncertainty that exists between the energies calculated with DFT and the true ground-state energies of the Schro¨dinger equation. In many situations, there is no direct way to estimate the magnitude of this uncertainty apart from careful comparisons with experimental measurements. As you read further through this book, we hope you will come to appreciate that there are many physical situations where the accuracy of DFT calculations is good enough to make powerful predictions about the properties of complex materials. The vignettes in Section 1.2 give several examples of this idea. We discuss the complicated issue of the accuracy of DFT calculations in Chapter 10.
There are some important situations for which DFT cannot be expected to be physically accurate. Below, we brieﬂy discuss some of the most common problems that fall into this category. The ﬁrst situation where DFT calculations have limited accuracy is in the calculation of electronic excited states. This can be understood in a general way by looking back at the statement of the Hohenberg –Kohn theorems in Section 1.4; these theorems only apply to the ground-state energy. It is certainly possible to make predictions about excited states from DFT calculations, but it is important to remember that these predictions are not—theoretically speaking—on the same footing as similar predictions made for ground-state properties.
A well-known inaccuracy in DFT is the underestimation of calculated band gaps in semiconducting and insulating materials. In isolated molecules, the energies that are accessible to individual electrons form a discrete set (usually described in terms of molecular orbitals). In crystalline materials, these energies must be described by continuous functions known as energy bands. The simplest deﬁnition of metals and insulators involves what energy levels are available to the electrons in the material with the highest energy once all the low-energy bands are ﬁlled in accordance with the Pauli exclusion principle. If the next available electronic state lies only at an inﬁnitesimal energy above the highest occupied state, then the material is said to be a metal. If the next available electronic state sits a ﬁnite energy above the highest occupied state, then the material is not a metal and the energy difference between these two states is called the band gap. By convention, materials with “large” band gaps (i.e., band gaps of multiple electron volts) are called insulators while materials with “small” band gaps are called semiconductors. Standard DFT calculations with existing functionals have limited accuracy for band gaps,

1.7 WHAT CAN DFT NOT DO? 29
with errors larger than 1 eV being common when comparing with experimental data. A subtle feature of this issue is that it has been shown that even the formally exact Kohn –Sham exchange –correlation functional would suffer from the same underlying problem.k
Another situation where DFT calculations give inaccurate results is associated with the weak van der Waals (vdW) attractions that exist between atoms and molecules. To see that interactions like this exist, you only have to think about a simple molecule like CH4 (methane). Methane becomes a liquid at sufﬁciently low temperatures and high enough pressures. The transportation of methane over long distances is far more economical in this liquid form than as a gas; this is the basis of the worldwide liqueﬁed natural gas (LNG) industry. But to become a liquid, some attractive interactions between pairs of CH4 molecules must exist. The attractive interactions are the van der Waals interactions, which, at the most fundamental level, occur because of correlations that exist between temporary ﬂuctuations in the electron density of one molecule and the energy of the electrons in another molecule responding to these ﬂuctuations. This description already hints at the reason that describing these interactions with DFT is challenging; van der Waals interactions are a direct result of long range electron correlation. To accurately calculate the strength of these interactions from quantum mechanics, it is necessary to use high-level wave-function-based methods that treat electron correlation in a systematic way. This has been done, for example, to calculate the very weak interactions that exist between pairs of H2 molecules, where it is known experimentally that energy of two H2 molecules in their most favored geometry is $0.003 eV lower than the energy of the same molecules separated by a long distance.8
There is one more fundamental limitation of DFT that is crucial to appreciate, and it stems from the computational expense associated with solving the mathematical problem posed by DFT. It is reasonable to say that calculations that involve tens of atoms are now routine, calculations involving hundreds of atoms are feasible but are considered challenging research-level problems, and calculations involving a thousand or more atoms are possible but restricted to a small group of people developing state-of-the-art codes and using some of the world’s largest computers. To keep this in a physical perspective, a droplet of water 1 mm in radius contains on the order of 1011 atoms. No conceivable increase in computing technology or code efﬁciency will allow DFT
kDevelopment of methods related to DFT that can treat this situation accurately is an active area of research where considerable progress is being made. Two representative examples of this kind of work are P. Rinke, A. Qteish, J. Neugebauer, and M. Schefﬂer, Exciting Prospects for Solids: Exact Exchange Based Functional Meet Quasiparticle Energy Calculations, Phys. Stat. Sol. 245 (2008), 929, and J. Uddin, J. E. Peralta, and G. E. Scuseria, Density Functional Theory Study of Bulk Platinum Monoxide, Phys. Rev. B, 71 (2005), 155112.

30 WHAT IS DENSITY FUNCTIONAL THEORY?
calculations to directly examine collections of atoms of this size. As a result, anyone using DFT calculations must clearly understand how information from calculations with extremely small numbers of atoms can be connected with information that is physically relevant to real materials.
1.8 DENSITY FUNCTIONAL THEORY IN OTHER FIELDS
For completeness, we need to point out that the name density functional theory is not solely applied to the type of quantum mechanics calculations we have described in this chapter. The idea of casting problems using functionals of density has also been used in the classical theory of ﬂuid thermodynamics. In this case, the density of interest is the ﬂuid density not the electron density, and the basic equation of interest is not the Schro¨dinger equation. Realizing that these two distinct scientiﬁc communities use the same name for their methods may save you some confusion if you ﬁnd yourself in a seminar by a researcher from the other community.
1.9 HOW TO APPROACH THIS BOOK (REVISITED)
We began this chapter with an analogy about learning to drive to describe our aims for this book. Now that we have introduced much of the terminology associated with DFT and quantum chemistry calculations, we can state the subject matter and approach of the book more precisely. The remaining chapters focus on using plane-wave DFT calculations with commonly applied functionals to physical questions involving bulk materials, surfaces, nanoparticles, and molecules. Because codes to perform these plane-wave calculations are now widely available, we aim to introduce many of the issues associated with applying these methods to interesting scientiﬁc questions in a computationally efﬁcient way.
The book has been written with two audiences in mind. The primary audience is readers who are entering a ﬁeld of research where they will perform DFT calculations (and perhaps other kinds of computational chemistry or materials modeling) on a daily basis. If this describes you, it is important that you perform as many of the exercises at the end of the chapters as possible. These exercises have been chosen to require relatively modest computational resources while exploring most of the key ideas introduced in each chapter. Simply put, if your aim is to enter a ﬁeld where you will perform calculations, then you must actually do calculations of your own, not just read about other people’s work. As in almost every endeavor, there are many details that are best learned by experience. For readers in this group, we recommend reading through every chapter sequentially.

REFERENCES 31
The second audience is people who are unlikely to routinely perform their own calculations, but who work in a ﬁeld where DFT calculations have become a “standard” approach. For this group, it is important to understand the language used to describe DFT calculations and the strengths and limitations of DFT. This situation is no different from “standard” experimental techniques such as X-ray diffraction or scanning electron microscopy, where a working knowledge of the basic methods is indispensable to a huge community of researchers, regardless of whether they personally apply these methods. If you are in this audience, we hope that this book can help you become a sophisticated consumer of DFT results in a relatively efﬁcient way. If you have a limited amount of time (a long plane ﬂight, for example), we recommend that you read Chapter 3, Chapter 10, and then read whichever of Chapters 4–9 appears most relevant to you. If (when?) your ﬂight is delayed, read one of the chapters that doesn’t appear directly relevant to your speciﬁc research interests—we hope that you will learn something interesting.
We have consciously limited the length of the book in the belief that the prospect of reading and understanding an entire book of this length is more appealing than the alternative of facing (and carrying) something the size of a large city’s phone book. Inevitably, this means that our coverage of various topics is limited in scope. In particular, we do not examine the details of DFT calculations using localized basis sets beyond the cursory treatment already presented in this chapter. We also do not delve deeply into the theory of DFT and the construction of functionals. In this context, the word “introduction” appears in the title of the book deliberately. You should view this book as an entry point into the vibrant world of DFT, computational chemistry, and materials modeling. By following the resources that are listed at the end of each chapter in the Further Reading section, we hope that you will continue to expand your horizons far beyond the introduction that this book gives.
We have opted to defer the crucial issue of the accuracy of DFT calculations until chapter 10, after introducing the application of DFT to a wide variety of physical properties in the preceding chapters. The discussion in that chapter emphasizes that this topic cannot be described in a simplistic way. Chapter 10 also points to some of the areas in which rapid developments are currently being made in the application of DFT to challenging physical problems.
REFERENCES
1. K. Honkala, A. Hellman, I. N. Remediakis, A. Logadottir, A. Carlsson, S. Dahl, C. H. Christensen, and J. K. Nørskov, Ammonia Synthesis from First-Principles Calculations, Science 307 (2005), 555.
2. R. Schweinfest, A. T. Paxton, and M. W. Finnis, Bismuth Embrittlement of Copper is an Atomic Size Effect, Nature 432 (2004), 1008.

32 WHAT IS DENSITY FUNCTIONAL THEORY?
3. K. Umemoto, R. M. Wentzcovitch, and P. B. Allen, Dissociation of MgSiO3 in the Cores of Gas Giants and Terrestrial Exoplanets, Science 311 (2006), 983.
4. K. Umemoto, R. M. Wentzcovitch, D. J. Weidner, and J. B. Parise, NaMgF3: A Low-Pressure Analog of MgSiO3, Geophys. Res. Lett. 33 (2006), L15304.
5. W. Kohn, Nobel Lecture: Electronic Structure of Matter-Wave Functions and Density Functionals, Rev. Mod. Phys. 71 (1999), 1253.
6. C. J. Cramer and J. T. Roberts, Computational Analysis: Y2K, Science 286 (1999), 2281.
7. G. Herzberg, Electronic Spectra and Electronic Structure of Polyatomic Molecules, Van Nostrand, New York, 1966.
8. P. Diep and J. K. Johnson, An Accurate H2 H2 Interaction Potential from First Principles, J. Chem. Phys. 112 (2000), 4465.
FURTHER READING
Throughout this book, we will list resources for further reading at the end of each chapter. You should think of these lists as pointers to help you learn about topics we have mentioned or simpliﬁed in a detailed way. We have made no attempt to make these lists exhaustive in any sense (to understand why, ﬁnd out how many textbooks exist dealing with “quantum mechanics” in some form or another).
Among the many books on quantum mechanics that have been written, the following are good places to start if you would like to review the basic concepts we have touched on in this chapter:
P. W. Atkins and R. S. Friedman, Molecular Quantum Mechanics, Oxford University Press, Oxford, UK, 1997.
D. A. McQuarrie, Quantum Chemistry, University Science Books, Mill Valley, CA, 1983.
M. A. Ratner and G. C. Schatz, Introduction to Quantum Mechanics in Chemistry, Prentice Hall, Upper Saddle River, NJ, 2001.
J. Simons and J. Nichols, Quantum Mechanics in Chemistry, Oxford University Press, New York, 1997.
Detailed accounts of DFT are available in:
W. Koch and M. C. Holthausen, A Chemist’s Guide to Density Functional Theory, Wiley-VCH, Weinheim, 2000.
R. M. Martin, Electronic Structure: Basic Theory and Practical Methods, Cambridge University Press, Cambridge, UK, 2004.
R. G. Parr and W. Yang, Density-Functional Theory of Atoms and Molecules, Oxford University Press, Oxford, UK, 1989.

FURTHER READING 33
Resources for learning about the wide range of quantum chemistry calculation methods that go beyond DFT include:
J. B. Foresman and A. Frisch, Exploring Chemistry with Electronic Structure Methods, Gaussian Inc., Pittsburgh, 1996.
A. Szabo and N. S. Ostlund, Modern Quantum Chemistry: Introduction to Advanced Electronic Structure Theory, Dover, Minneola, NY, 1996.
D. Young, Computational Chemistry: A Practical Guide for Applying Techniques to Real World Problems, Wiley, New York, 2001.
A book that gives a relatively brief overview of band theory is:
A. P. Sutton, Electronic Structure of Materials, Oxford University Press, Oxford, UK, 1993.
Two traditional sources for a more in-depth view of this topic are:
N. W. Ashcroft and N. D. Mermin, Solid State Physics, Saunders College Publishing, Orlando, 1976.
C. Kittel, Introduction to Solid State Physics, Wiley, New York, 1976.
A good source if you want to learn about the ﬂuid thermodynamics version of DFT is:
H. Ted Davis, Statistical Mechanics of Phases, Interfaces, and Thin Films, WileyVCH, 1995.

2
DFT CALCULATIONS FOR SIMPLE SOLIDS
In this chapter, we explore how DFT calculations can be used to predict an important physical property of solids, namely their crystal structure. We will do this without delving into the technical details of actually performing such calculations because it is important to have a clear understanding of how DFT calculations can be used before getting too involved in the details of convergence and so on. This is not to say that these details are unimportant—they are absolutely crucial for performing reliable calculations! For the purposes of this chapter, we simply assume that we can use a DFT code to calculate the total energy of some collection of atoms. At the end of the chapter we give a few speciﬁc suggestions for doing calculations of this type that are needed if you perform the exercises that are suggested throughout the chapter.
2.1 PERIODIC STRUCTURES, SUPERCELLS, AND LATTICE PARAMETERS
It is quite likely that you are already familiar with the concept of crystal structures, but this idea is so central to using plane-wave DFT calculations that we are going to begin by considering the simplest possible example. Our task is to deﬁne the location of all atoms in a crystal of a pure metal. For now, imagine
Density Functional Theory: A Practical Introduction. By David S. Sholl and Janice A. Steckel Copyright # 2009 John Wiley & Sons, Inc.
35

36 DFT CALCULATIONS FOR SIMPLE SOLIDS
that we ﬁll three-dimensional space with cubes of side length a and place a single metal atom at the corner of each cube. That is, the positions of all atoms are deﬁned using normal three-dimensional Cartesian coordinates by r ¼ (n1a,n2a,n3a) for any integers n1, n2, and n3. This is called the simple cubic structure, and there is an element whose atoms are actually arranged this way in nature: polonium. What information do we need to specify to completely deﬁne the crystal structure of a simple cubic metal? Just one number, namely the lattice constant, a.
We now need to deﬁne a collection of atoms that can be used in a DFT calculation to represent a simple cubic material. Said more precisely, we need to specify a set of atoms so that when this set is repeated in every direction, it creates the full three-dimensional crystal structure. Although it is not really necessary for our initial example, it is useful to split this task into two parts. First, we deﬁne a volume that ﬁlls space when repeated in all directions. For the simple cubic metal, the obvious choice for this volume is a cube of side length a with a corner at (0,0,0) and edges pointing along the x, y, and z coordinates in threedimensional space. Second, we deﬁne the position(s) of the atom(s) that are included in this volume. With the cubic volume we just chose, the volume will contain just one atom and we could locate it at (0,0,0). Together, these two choices have completely deﬁned the crystal structure of an element with the simple cubic structure. The vectors that deﬁne the cell volume and the atom positions within the cell are collectively referred to as the supercell, and the deﬁnition of a supercell is the most basic input into a DFT calculation.
The choices we made above to deﬁne a simple cubic supercell are not the only possible choices. For example, we could have deﬁned the supercell as a cube with side length 2a containing four atoms located at (0,0,0), (0,0,a), (0,a,0), and (a,0,0). Repeating this larger volume in space deﬁnes a simple cubic structure just as well as the smaller volume we looked at above. There is clearly something special about our ﬁrst choice, however, since it contains the minimum number of atoms that can be used to fully deﬁne the structure (in this case, 1). The supercell with this conceptually appealing property is called the primitive cell.
If we go back to our deﬁnition of the primitive cell for the simple cubic structure, we are not required to place the atom in the supercell at (0,0,0). We could just as well place the atom at (a/2,a/2,a/2) or (0,0,a/2) or even (0.647a,0.2293a,0.184a). For every one of these choices, we are deﬁning just one atom in the supercell, and the repetition of the supercell in space creates the simple cubic structure. Putting the atom at (0,0,0) may seem like the best choice from an aesthetic point of view, although (a/2,a/2,a/2) is also appealing because it puts the atom in the middle of the supercell. The point to remember is that any of these choices is mathematically equivalent, so they are all equally good from the point of view of doing an actual calculation.

2.1 PERIODIC STRUCTURES, SUPERCELLS, AND LATTICE PARAMETERS 37

You now know how to deﬁne a supercell for a DFT calculation for a material with the simple cubic crystal structure. We also said at the outset that we assume for the purposes of this chapter that we have a DFT code that can give us the total energy of some collection of atoms. How can we use calculations of this type to determine the lattice constant of our simple cubic metal that would be observed in nature? The sensible approach would be to calculate the total energy of our material as a function of the lattice constant, that is, Etot(a). A typical result from doing this type of calculation is shown in Fig. 2.1. The details of how these calculations (and the other calculations described in the rest of the chapter) were done are listed in the Appendix at the end of the chapter.
The shape of the curve in Fig. 2.1 is simple; it has a single minimum at a value of a we will call a0. If the simple cubic metal exists with any value of a larger or smaller than a0, the total energy of the material could be reduced by changing the lattice parameter to a0. Since nature always seeks to minimize energy, we have made a direct physical prediction with our calculations: DFT predicts that the lattice parameter of our simple cubic material is a0.
To extract a value of a0 from our calculations, it is useful to think about the functional form of Etot(a). The simplest approach is to write the total energy using a truncated Taylor expansion:

Etot(a) ﬃ Etot(a0) þ a(a a0) þ b(a a0)2

(2:1)

Figure 2.1 Total energy, Etot, of Cu in the simple cubic crystal structure as a function of the lattice parameter, a. The ﬁlled symbols show the results of DFT calculations, while the three curves show the ﬁts of the DFT data described in the text.

38 DFT CALCULATIONS FOR SIMPLE SOLIDS

with

a

¼

dEtot=daja0

and

b

¼

1 2

d2Etot=da2ja0.

By

deﬁnition,

a

0 if a0 is the

lattice parameter corresponding to the minimum energy. This suggests that we

can ﬁt our numerical data to

Etot(a) ﬃ E0 þ b(a a0)2,

(2:2)

where E0, b, and a0 are treated as ﬁtting parameters. The solid black curve
shown in Fig. 2.1 is the result of ﬁtting this curve to our data using values of a from 2.25 to 2.6 A˚ . This ﬁtted curve predicts that a0 is 2.43 A˚ .
Although we treated b simply as a ﬁtting parameter in the equation above, it

actually has direct physical signiﬁcance. The equilibrium bulk modulus, B0, of a material is deﬁned by B0 ¼ Vd2Etot=dV2, where the derivative is evaluated at the
equilibrium lattice parameter. Comparing this expression with the Taylor expan-

sion

above,

we

ﬁnd

that

B0

¼

2 9

(1=a0)b.

That

is,

the

curve

ﬁtting

we

have

per-

formed gives us a value for both the equilibrium lattice parameter and the

equilibrium bulk modulus. The bulk modulus from the solid curve in Fig. 2.1 is 0.641 eV/A˚ 3. These units are perfectly natural when doing a DFT calculation,

but they are awkward for comparing to macroscopic data. Converting this result

into more familiar units, we have predicted that B0 103 GPa. Our derivation of Eq. (2.2) gave a simple relationship between Etot and a, but
it is only valid for a small range of lattice constants around the equilibrium

value. This can be seen directly from Fig. 2.1, where the quadratic ﬁt to the data is shown as a gray curve for values of a . 2.6 A˚ . This range is of special

interest because the DFT data in this range was not used in ﬁtting the curve. It is

clear from the ﬁgure that the ﬁtted curve begins to deviate quite strongly from

the DFT data as the lattice parameter increases. The root of this problem is that

the overall shape of Etot(a) is not simply a quadratic function of the lattice parameter. More detailed mathematical treatments can give equations of state that

relate these two quantities over a wider range of lattice constants. One well-

known example is the Birch –Murnaghan equation of state for isotropic solids:

(

Etot(a)

¼

E0

þ

9V0B0 16

a02 a

!3 1 B00

þ a02

!2 1 6

) 4a02! :

a

a

(2:3)

In this expression, a0 is the equilibrium lattice constant, V0 is the equilibrium volume per atom, B0 is the bulk modulus at zero pressure, P 0, and B00 ¼ (@B=@P)T . To apply this equation to our data, we treat a0, B0, and B00 and E0 as ﬁtting parameters. The results of ﬁtting this equation of state to the full set of DFT data shown in Fig. 2.1 are shown in the ﬁgure with a dashed

2.2 FACE CENTERED CUBIC MATERIALS 39
line. It is clear from the ﬁgure that the equation of state allows us to accurately ﬁt the data. The outcome from this calculation is the prediction that for Cu in a cubic crystal structure, a0 is 2.41 A˚ and B0 102 GPa. It should not surprise you that these predictions are very similar to the ones we made with the simpler quadratic model for Etot(a) because for lattice parameters close to a0, Eq. (2.3) reduces to Eq. (2.2).

2.2 FACE-CENTERED CUBIC MATERIALS

The simple cubic crystal structure we discussed above is the simplest crystal structure to visualize, but it is of limited practical interest at least for elements in their bulk form because other than polonium no elements exist with this structure. A much more common crystal structure in the periodic table is the face-centered-cubic (fcc) structure. We can form this structure by ﬁlling space with cubes of side length a that have atoms at the corners of each cube and also atoms in the center of each face of each cube. We can deﬁne a supercell for an fcc material using the same cube of side length a that we used for the simple cubic material and placing atoms at (0,0,0), (0,a/2,a/2), (a/2,0,a/2), and (a/2,a/2,0). You should be able to check this statement for yourself by sketching the structure.
Unlike the deﬁnition of the supercell for a simple cubic material we used above, this supercell for an fcc metal contains four distinct atoms. This gives us a hint that we may not have described the primitive cell. This suspicion can be given more weight by drawing a sketch to count the number of atoms that neighbor an atom at a corner in our “natural” structure and then repeating this exercise for an atom in a cube face in our structure. Doing this shows that both types of atoms have 12 neighboring atoms arranged in the same geometry and that the distances between any atom and all of its 12 neighbors are identical.
The primitive cell for the fcc metal can be deﬁned by connecting the atom at the origin in the structure deﬁned above with three atoms in the cube faces adjacent to that atom. That is, we deﬁne cell vectors

a1

¼

aÀ12

,

1 2

,0Á

a2

¼

aÀ0,

1 2

,

1Á
2

a3 ¼ aÀ12 ,0, 12Á:

(2:4)

These vectors deﬁne the fcc lattice if we place atoms at positions

r ¼ n1a1 þ n2a2 þ n3a3

(2:5)

for all integers n1, n2, and n3. This situation is illustrated in Fig. 2.2. We can see from these deﬁnitions that there is one atom in the cell, so it must be the primitive cell. Also, the geometry of the crystal is fully deﬁned by a single

40 DFT CALCULATIONS FOR SIMPLE SOLIDS
Figure 2.2 Illustration of the cell vectors of the fcc metal deﬁned in Eq. (2.4).
paprameter, a. The distance between nearest-neighbor atoms in an fcc metal is a= 2. A third important observation is that the cell vectors are not orthogonal (i.e., none of ai Á aj are zero).
The results from calculating the total energy of fcc Cu as a function of the lattice parameter are shown in Fig. 2.3. The shape of the curve is similar to the one we saw for Cu in the simple cubic crystal structure (Fig. 2.1), but
Figure 2.3 Total energy, E, of Cu in the fcc crystal structure as a function of the lattice para meter, a. Data points are from DFT calculations and the dashed curve is the Birch Murnaghan equation of state.

2.3 HEXAGONAL CLOSE PACKED MATERIALS 41
the minimum energy in the fcc structure has a lower value than the minimum energy for the simple cubic crystal. This observation has a simple physical interpretation: the fcc crystal structure of Cu is more stable than the simple cubic crystal structure. This conclusion is not surprising since Cu is in reality an fcc metal, but it is pleasing to ﬁnd that our DFT calculations are in agreement with physical reality. We can also compare our calculated lattice constant with the experimental result. From the curve in Fig. 2.3 (the Birch – Murnaghan ﬁt to the DFT data) we predict that a0 3.64 A˚ and B0 142 GPa for Cu. Experimentally, the Cu lattice constant is 3.62 A˚ and B0 140 GPa. For both quantities, our predictions are very close (although not equal to) the experimental values.

2.3 HEXAGONAL CLOSE-PACKED MATERIALS

It is not hard to understand why many metals favor an fcc crystal structure: there

is no packing of hard spheres in space that creates a higher density than the fcc

structure. (A mathematical proof of this fact, known as the Kepler conjecture, has

only been discovered in the past few years.) There is, however, one other packing

that has exactly the same density as the fcc packing, namely the hexagonal close-

packed (hcp) structure. As our third example of applying DFT to a periodic crys-

tal structure, we will now consider the hcp metals.

The supercell for an hcp metal is a little more complicated than for the

simple cubic or fcc examples with which we have already dealt. The supercell

can be deﬁned using the following cell vectors:

p

a1 ¼ (a,0,0)

a2

¼

a 2

,

3a  ,0
2

a3 ¼ (0,0,c)

(2:6)

and placing two distinpct atoms within the supercell with Cartesian coordinates (0,0,0) and (a/2,a/2 3,c/2). You should convince yourself that if you consider only the vectors a1 and a2 and the atom at (0,0,0) in the supercell these together generate a hexagonal array of atoms in the x –y plane. Also notice that this deﬁnition of the complete supercell involves two unknown parameters, a and c, rather than the one unknown lattice constant that we had for the simple cubic and fcc structures. In the ideal hard sphere packing, c p8=3 a ¼ 1:633a, and the distance between all possible pairs of adjacent atoms in the crystal are identical. In real hcp metals, however, small distortions of the crystal are often observed. For example, for scandium, c/a 1.59 experimentally.
The deﬁnition of the hcp supercell given above is useful to introduce one more concept that is commonly used in deﬁning atomic coordinates in periodic geometries. As our deﬁnition stands, the vectors deﬁning the shape of the

42 DFT CALCULATIONS FOR SIMPLE SOLIDS

supercell (the lattice vectors) and the atom positions have been given in threedimensional Cartesian coordinates. It is typically more convenient to deﬁne the lattice vectors in Cartesian coordinates and then to deﬁne the atom positions in terms of the lattice vectors. In general, we can write the position of atom j in the supercell as

3
rj ¼ X f j,iai:
j1

(2:7)

Because we can always choose each atom so it lies within the supercell, 0 fj,i 1 for all i and j. These coefﬁcients are called the fractional coordinates of the

atoms in the supercell. The fractional coordinates are often written in terms of a

vector for each distinct atom. In the hcp structure deﬁned above, for example,

the

two

atoms

lie

at

fractional

coordinates

(0,0,0)

and

(13

,

1 3

,

13).

Notice

that

with

this deﬁnition the only place that the lattice parameters appear in the deﬁnition

of the supercell is in the lattice vectors. The deﬁnition of a supercell with a set of

lattice vectors and a set of fractional coordinates is by far the most convenient

way to describe an arbitrary supercell, and it is the notation we will use through-

out the remainder of this book. Most, if not all, popular DFT packages allow or

require you to deﬁne supercells using this notation.

Using DFT to predict the lattice constant of Cu in the simple cubic or fcc

crystal structures was straightforward; we just did a series of calculations of

the total energy as a function of the lattice parameter, a. The fact that the

hcp structure has two independent parameters, a and c, complicates this pro-

cess. Most DFT packages have the capability to handle multivariable problems

like this in an automated way, but for now let us stick with the assumption

that we only know how to use our package to compute a total energy for

one cell volume and geometry at a time. One way to proceed is to simply

ﬁx the value of c/a and then calculate a series of total energies as a function

of a. Results from a series of calculations like this are shown in Fig. 2.4. In this

ﬁgure, the lines simply connect the data points to guide the eye. From these

calculations, we see that the distortion along the c axis away from hard

sphere packing for Cu is predicted to be small. More importantly, the mini-

mum energy of the hcp Cu structure is larger than the minimum energy for

the fcc structure by $0.015 eV/atom (cf. Fig. 2.3), so our calculations

agree with the observation that Cu is an fcc metal, not an hcp metal. The pre-

dicted energy difference, 1.4 kJ/mol, between the two crystal structures is

quite small. This is not unreasonable since the two structures are very similar

in many respects.

There are (at least) two things that our calculations for hcp Cu should make

you think about. The ﬁrst concerns the numerical accuracy of our DFT calcu-

lations. Can we reliably use these calculations to distinguish between the

2.4 CRYSTAL STRUCTURE PREDICTION 43
Figure 2.4 Total energy, Etot, of Cu in the hcp crystal structure for several values of c/a. Each data point is from a DFT calculation. Straight lines connecting the data points are to guide the eye.
stability of two crystal structures that differ by only $1 kJ/mol in energy? What about structures that differ by $0.1 kJ/mol in energy? The answer to this question is intimately tied to our ability to numerically solve the complicated mathematical problem deﬁned by DFT for a particular set of atoms. We will be occupied by this crucial topic of numerical convergence for much of the next chapter.
Second, it should be clear that performing the calculations for an example where we had to determine two lattice parameters was considerably more work than when we only had to determine one. This example is partly intended to give you a taste for the fact that when there are many degrees of freedom in a problem, whether they are atom positions within a supercell or the lattice parameters associated with cell vectors, minimizing the overall energy by varying one parameter at a time soon becomes untenable. After discussing numerical convergence, the next chapter deals with the important concept of how energy minima can be efﬁciently calculated for complex conﬁgurations of atoms without having to systematically vary each degree of freedom.
2.4 CRYSTAL STRUCTURE PREDICTION It is tempting to say that we have predicted that crystal structure of Cu with our calculations in the previous sections, but this is not strictly true. To be precise, we should say that we have predicted that fcc Cu is more stable than hcp Cu or simple cubic Cu. Based on our calculations alone we cannot exclude the

44 DFT CALCULATIONS FOR SIMPLE SOLIDS
possibility that Cu in fact adopts some other crystal structure that we have not examined. For Cu this is a fairly pedantic point since we already know from experiments that it is an fcc metal. So we could state that our calculations are entirely consistent with the experimental crystal structure and that we now have a prediction for the lattice parameter of fcc Cu.
To make this point in another way, imagine that you have been asked to predict the crystal structure of di-yttrium potassium (Y2K), a substance for which no experimental data is available. You could attempt this task using DFT by making a list of all known crystal structures with stoichiometry AB2, then minimizing the total energy of Y2K in each of these crystal structures. This is far from a simple task; more than 80 distinct AB2 crystal structures are known, and many of them are quite complicated. To give just one example, NiMg2, NbZn2, ScFe2, ThMg2, HfCr2, and UPt2 all exist in an ordered structure known as the C36 hexagonal Laves phase that has 106 distinct atoms in the primitive cell. Even if you completed the somewhat heroic task of performing all these calculations, you could not be sure that Y2K does not actually form a new crystal structure that has not been previously observed! This discussion illustrates why determining the crystal structure of new compounds remains an interesting scientiﬁc endeavor. The main message from this discussion is that DFT is very well suited to predicting the energy of crystal structures within a set of potential structures, but calculations alone are almost never sufﬁcient to truly predict new structures in the absence of experimental data.

2.5 PHASE TRANSFORMATIONS

All of our analysis of the Cu crystal structure has been based on the reasonable idea that the crystal structure with the lowest energy is the structure preferred by nature. This idea is correct, but we need to be careful about how we deﬁne a materials’ energy to make it precise. To be precise, the preferred crystal structure is the one with the lowest Gibbs free energy, G G(P, T ). The Gibbs free energy can be written as

G(P, T) ¼ Ecoh þ PV TS,

(2:8)

where Ecoh, V, and S are the cohesive energy (i.e., the energy to pull a material apart into a collection of isolated atoms), volume, and entropy of a material. If we are comparing two possible crystal structures, then we are interested in the change in Gibbs free energy between the two structures:

DG(P, T) ¼ DEcoh þ P DV T DS:

(2:9)

2.5 PHASE TRANSFORMATIONS 45

In solids, the ﬁrst two terms tend to be much larger than the entropic contribution from the last term in this expression, so

DG(P, T) ﬃ DEcoh þ P DV:

(2:10)

Both of the quantities on the right-hand side have a simple meaning in terms of the energies we have been calculating with DFT. The change in cohesive energy between the two structures is just the difference between the DFT total energies. The pressure that is relevant for any point on the energy curves we have calculated is deﬁned by

P¼

@Ecoh @V

:

(2:11)

In Sections 2.1–2.3 we interpreted the minimum in a plot of the DFT energy as a function of the lattice parameter as the preferred lattice parameter for the crystal structure used in the calculations. Looking at Eqs. (2.8) and (2.11), you can see that a more precise interpretation is that a minimum of this kind deﬁnes the preferred lattice parameter at P 0 and T 0.
An interesting consequence of Eq. (2.10) is that two crystal structures with different cohesive energies can have the same Gibbs free energy if DEcoh 2P DV. Comparing this condition with Eq. (2.11), you can see that two structures satisfy this condition if they share a common tangent on a plot of DEcoh as a function of V. This situation is illustrated in Fig. 2.5. In this ﬁgure, the preferred crystal structure at P 0 is structure 1, and the lattice parameter of this preferred structure deﬁnes a volume V0. Moving along the

Figure 2.5 Schematic illustration of a pressure induced transformation between two crystal structures of a material.

46 DFT CALCULATIONS FOR SIMPLE SOLIDS
curve for the cohesive energy from the minimum at V/V0 1 toward the point labeled A is equivalent to increasing the pressure on the material. When the pressure corresponding to point A is reached, the curve for structure 1 shares a common tangent with structure 2. At higher pressures, the Gibbs free energy of structure 2 is lower than for structure 1, so the ﬁgure predicts that a pressure-induced phase transformation would occur at this pressure that would change the crystal structure of the material. In this phase change the cohesive energy goes up, but this change is balanced within the free energy by the reduction in the volume of the material.
Pressure-induced phase transformations are known to occur for a wide range of solids. Bulk Si, for example, has the diamond structure at ambient conditions but converts to the b-tin structure at pressures around 100 kbar. Figure 2.5 shows how it is possible to use the kinds of information we have calculated in this chapter using DFT to predict the existence of pressure-induced phase transformations. It was essentially this idea that was used to make the geologically relevant predictions of the properties of minerals such as MgSiO3 that were mentioned as one of the motivating examples in Section 1.2.
EXERCISES
To perform these exercises, you will need access to a code that performs planewave DFT calculations and be able to use this code to determine the total energy of a supercell. As you will discover in subsequent chapters, there are a number of input parameters that you need to provide to any DFT code. At this point, we suggest that you attempt the exercises below using, where possible, default input parameters, perhaps set up for you by a more experienced colleague. This should make you curious about what these input parameters mean, a curiosity that should be satisﬁed in the following chapters. The aim of the following exercises is to develop some proﬁciency with setting up supercells for bulk materials and practice some of the issues associated with predicting the crystal structures of these materials.
1. Perform calculations to determine whether Pt prefers the simple cubic, fcc, or hcp crystal structure. Compare your DFT-predicted lattice parameter(s) of the preferred structure with experimental observations.
2. Hf is experimentally observed to be an hcp metal with c/a 1.58. Perform calculations to predict the lattice parameters for Hf and compare them with experimental observations.
3. A large number of solids with stoichiometry AB form the CsCl structure. In this structure, atoms of A deﬁne a simple cubic structure and atoms of B reside in the center of each cube of A atoms. Deﬁne the cell vectors

APPENDIX CALCULATION DETAILS 47
and fractional coordinates for the CsCl structure, then use this structure to predict the lattice constant of ScAl. 4. Another common structure for AB compounds is the NaCl structure. In this structure, A and B atoms alternate along any axis of the simple cubic structure. Predict the lattice parameter for ScAl in this structure and show by comparison to your results from the previous exercise that ScAl does not prefer the NaCl structure.
FURTHER READING
The fundamental concepts and notations associated with crystal structures are described in almost all textbooks dealing with solid-state physics or materials science. Two well-known examples are C. Kittel, Introduction to Solid State Physics, Wiley, New York, 1976, and N. W. Ashcroft and N. D. Mermin, Solid State Physics, Saunders College, Orlando, FL, 1976.
For a detailed account of the crystal structures of AB compounds, as well as an excellent overview of the various notations used to deﬁne crystal structures, see David Pettifor Bonding and Structure of Molecules and Solids, Oxford University Press, Oxford, UK, 1995.
For an interesting history of the Kepler conjecture, see G. G. Szpiro, Kepler’s Conjecture: How Some of the Greatest Minds in History Helped Solve One of the Oldest Math Problems in the World, Wiley, Hoboken, NJ, 2003.
To see how quantum chemistry was used to predict the gas-phase properties of Y2K, see C. J. Cramer and J. T. Roberts, Science 286 (1999), 2281. To appreciate the timeliness of this publication, note the date of publication.
APPENDIX CALCULATION DETAILS
In each chapter where we give results from speciﬁc calculations, we will include an appendix like this one where some details about how these calculations were performed are listed.
All of the calculations we describe throughout the book were done using the Vienna ab initio Simulation Package (VASP).Ã Although we enjoy using VASP and have the greatest appreciation for the people who have put many person-years of effort into developing it, our use of it here is not intended to be a formal endorsement of this package. VASP is one of several widely used plane-wave DFT packages, and other equally popular software packages could have been used for all the calculations we describe. The choice of which package is right for you will be inﬂuenced by factors such as the availability of
ÃInformation about this package is available from http://cms.mpi.univie.ac.at/$vasp.

48 DFT CALCULATIONS FOR SIMPLE SOLIDS
licenses for the software at your institution and the experience your and your co-workers have accumulated with one or more packages. We have attempted to only use features of VASP that are also available in essentially all planewave DFT codes, so you should have little trouble reproducing the calculations we have shown as examples using any mainstream code.
Unless otherwise stated, all of our calculations used the generalized gradient approximation as deﬁned by the Perdew –Wang 91 functional. In shorthand, we used PW91-GGA calculations. Unless otherwise stated k points were placed in reciprocal space using the Monkhorst –Pack method. Some speciﬁc details for the calculations from each section of the chapter are listed below.
Section 2.1 The cubic Cu calculations in Fig. 2.1 used a cubic supercell with 1 Cu atom, a cutoff energy of 292 eV, and 12Â12Â12 k points.
Section 2.2 The fcc Cu calculations in Fig. 2.3 used a cubic supercell with 4 Cu atoms, a cutoff energy of 292 eV, and 12Â12Â12 k points.
Section 2.3 The hcp Cu calculations in Fig. 2.4 used a primitive supercell with 2 Cu atoms, a cutoff energy of 292 eV, and 12Â12Â8 k points placed in reciprocal space using a G-centered grid.

3
NUTS AND BOLTS OF DFT CALCULATIONS
Throughout Chapter 2, we deliberately ignored many of the details that are necessary in performing DFT calculations in order to illustrate some of the physical quantities that these calculations can treat. This state of affairs is a useful starting point when you are learning about these methods, but it is, of course, not a useful strategy in the longer term. In this chapter, we dig into some of the details that make the difference between DFT calculations that provide reliable physical information and calculations that simply occupy time on your computer.
A key concept that we will reiterate many times is convergence. As you perform DFT calculations (or as you interact with other people who are performing such calculations), you should always be asking whether the calculations are converged.Ã What do we mean by convergence? To answer this question, it is useful to brieﬂy look back at the description of DFT given in Chapter 1. The ground-state electron density of a conﬁguration of atoms as deﬁned by DFT is deﬁned by the solution to a complicated set of mathematical equations. To actually solve this problem on a computer, we must make a series of numerical approximations: integrals in multidimensional space must be evaluated by
ÃThis forms the basis for a foolproof question that can be asked of any theoretician when they give a talk: “Can you comment on how well converged your results are?” There is an equivalent generic question for experimenters: “Are there possible effects of contamination in your experiments?”
Density Functional Theory: A Practical Introduction. By David S. Sholl and Janice A. Steckel Copyright # 2009 John Wiley & Sons, Inc.
49

50 NUTS AND BOLTS OF DFT CALCULATIONS
examining the function to be integrated at a ﬁnite collection of points, solutions that formally are expressed as inﬁnite sums must be truncated to ﬁnite sums, and so on. In each numerical approximation of this kind, it is possible to ﬁnd a solution that is closer and closer to the exact solution by using more and more computational resources. This is the process we will refer to as convergence. A “well-converged” calculation is one in which the numerically derived solution accurately approximates the true solution of the mathematical problem posed by DFT with a speciﬁc exchange –correlation functional.
The concept of numerical convergence is quite separate from the question of whether DFT accurately describes physical reality. The mathematical problem deﬁned by DFT is not identical to the full Schro¨dinger equation (because we do not know the precise form of the exchange – correlation functional). This means that the exact solution of a DFT problem is not identical to the exact solution of the Schro¨dinger equation, and it is the latter that we are presumably most interested in. This issue, the physical accuracy of DFT, is of the utmost important, but it is more complicated to fully address than the topic of numerical convergence. The issue of the physical accuracy of DFT calculations is addressed in Chapter 10.
In this chapter, we ﬁrst concentrate on what is required to perform wellconverged DFT calculations. After all, we need to be able to conﬁdently ﬁnd precise solutions to the numerical problems deﬁned by DFT before we can reasonably discuss the agreement (or lack thereof) between DFT results and physical reality.
3.1 RECIPROCAL SPACE AND k POINTS
Our ﬁrst foray into the realm of numerical convergence takes us away from the comfortable three-dimensional physical space where atom positions are deﬁned and into what is known as reciprocal space. The concepts associated with reciprocal space are fundamental to much of solid-state physics; that there are many physicists who can barely fathom the possibility that anyone might ﬁnd them slightly mysterious. It is not our aim here to give a complete description of these concepts. Several standard solid-state physics texts that cover these topics in great detail are listed at the end of the chapter. Here, we aim to cover what we think are the most critical ideas related to how reciprocal space comes into practical DFT calculations, with particular emphasis on the relationship between these ideas and numerical convergence.
3.1.1 Plane Waves and the Brillouin Zone
We emphasized in Chapter 2 that we are interested in applying DFT calculations to arrangements of atoms that are periodic in space. We deﬁned the

3.1 RECIPROCAL SPACE AND k POINTS 51

shape of the cell that is repeated periodically in space, the supercell, by lattice

vectors a1, a2, and a3. If we solve the Schro¨dinger equation for this periodic system, the solution must satisfy a fundamental property known as Bloch’s

theorem, which states that the solution can be expressed as a sum of terms

with the form

Á fk(r) ¼ exp(ik r)uk(r),

(3:1)

where uk(r) is periodic in space with the same periodicity as the supercell. That is, uk(r þ n1a1 þ n2a2 þ n3a3) ¼ uk(r) for any integers n1, n2, and n3. This theorem means that it is possible to try and solve the Schro¨dinger equation for each value of k independently. We have stated this result in terms of solutions of the Schro¨dinger equation, but it also applies to quantities derived from solutions to this equation such as the electron density.
It turns out that many parts of the mathematical problems posed by DFT are much more convenient to solve in terms of k than they are to solve in terms of
Á r. Because the functions exp(ik r) are called plane waves, calculations based
on this idea are frequently referred to as plane-wave calculations. The space of vectors r is called real space, and the space of vectors k is called reciprocal space (or simply k space). The idea of using reciprocal space is so central to the calculations we will discuss in the rest of the book that it is important to introduce several features of k space.
Just as we deﬁned positions in real space in terms of the lattice vectors a1, a2, and a3, it is useful to deﬁne three vectors that deﬁne positions in reci-
Á procal space. These vectors are called the reciprocal lattice vectors, b1, b2, and
b3, and are deﬁned so that ai bj is 2p if i j and 0 otherwise. This choice means that

Á Á Á b1

¼

2p

a1

a2 Â a3 (a2 Â a3)

,

b2

¼

2p

a2

a3 Â a1 (a3 Â a1)

,

b3

¼

2p

a3

a1 Â a2 (a1 Â a2)

:

(3:2)

A simple example of this calculation is the simple cubic lattice we discussed in Chapter 2. In that case, the natural choice for the real space lattice vectors has jaij ¼ a for all i. You should verify that this means that the reciprocal lattice vectors satisfy jbij ¼ 2p=a for all i. For this system, the lattice vectors and the reciprocal lattice vectors both deﬁne cubes, the former with a side length of a and the latter with a side length of 2p=a.
In Chapter 2 we mentioned that a simple cubic supercell can be deﬁned with lattice vectors ai ¼ a or alternatively with lattice vectors ai ¼ 2a. The ﬁrst choice uses one atom per supercell and is the primitive cell for the simple cubic material, while the second choice uses eight atoms per supercell. Both choices deﬁne the same material. If we made the second choice, then

52 NUTS AND BOLTS OF DFT CALCULATIONS

our reciprocal lattice vectors would also change—they would become bi ¼ p=a. This example illustrates an important general observation: larger lattice vectors in real space correspond to shorter lattice vectors in reciprocal space.
The three-dimensional shape deﬁned by the reciprocal lattice vectors is not always the same as the shape of the supercell in real space. For the fcc primitive cell, we showed in Chapter 2 that

a1

¼

aÀ12

,

1 2

,0Á,

a2

¼

aÀ0,

1 2

,

12Á,

a3 ¼ aÀ12 ,0, 12Á:

(3:3)

The reciprocal vectors in this case are

b1

¼

2p a

(1,1,

1),

b2

¼

2p a

(

1,1,1),

b3

¼

2p a

(1,

1,1):

(3:4)

Again, notice that the length of the reciprocal lattice vectors are inversely related to the reciprocal of the length of the real space lattice vectors. Views of these lattice vectors in real space and reciprocal space are shown in Fig. 3.1.
We previously introduced the concept of a primitive cell as being the supercell that contains the minimum number of atoms necessary to fully deﬁne a periodic material with inﬁnite extent. A more general way of thinking about the primitive cell is that it is a cell that is minimal in terms of volume but still contains all the information we need. This concept can be made more precise by considering the so-called Wigner –Seitz cell. We will not go into

Figure 3.1 View of the real space and reciprocal space lattice vectors for the fcc primitive cell. In the real space picture, circles represent atoms. In the reciprocal space picture, the basis vectors are shown inside a cube with side length 4p/a centered at the origin.

3.1 RECIPROCAL SPACE AND k POINTS 53

the details of this construction here but simply state that the Wigner –Seitz cell can be deﬁned for reciprocal lattice vectors just as easily as it can be for real space vectors. In plainer language, we can deﬁne a primitive cell in reciprocal space. Because this cell has many special properties, it is given a name: it is the Brillouin zone (often abbreviated to BZ). The Brillouin zone plays a central role in the band theory of materials. Several points in the Brillouin zone with special signiﬁcance are given individual names. The most important of these is the point where k ¼ 0; this location in k space is called the G point. As a ﬁnal example of the link between length in real space and reciprocal space, we note that the volume of the BZ, VBZ, and the volume of the primitive cell in real space deﬁned by the Wigner – Seitz construction, Vcell, are related by

VBZ

¼

(2p)3 Vcell

:

(3:5)

3.1.2 Integrals in k Space

Why is the Brillouin zone so important in plane-wave DFT calculations? The simple answer is that in a practical DFT calculation, a great deal of the work reduces to evaluating integrals of the form

g

¼

Vcell (2p)3

ð

g(k)

dk:

BZ

(3:6)

The key features of this integral are that it is deﬁned in reciprocal space and that

it integrates only over the possible values of k in the Brillouin zone. Rather

than examining in detail where integrals such as these come from, let us con-

sider instead how we can evaluate them numerically.

Before we try and evaluate integrals such as those in Eq. (3.6), we will

look

at

the

simpler

task

of

evaluating

Ð1
1

f (x)

dx

numerically.

You

hopefully

remember from calculus that this integral can be thought of as the area under

the curve deﬁned by f (x) on the interval [21,1]. This interpretation suggests

that a simple way to approximate the integral is to break up the interval into

pieces of equal size and estimate the area under the curve by treating the

curve as a straight line between the end points of the interval. This gives us

the trapezoidal method:

1
ð
1

" f (x) dx ﬃ h f (
2

n1

#

X

1) þ f (þ1) þ 2 f (xj)

j1

(3:7)

with xj ¼ 1 þ jh and h ¼ 2=n.

54 NUTS AND BOLTS OF DFT CALCULATIONS

TABLE 3.1

Approximations

to

the

Integral

Ð 1 px
À1 2

sin(p

x)

dx

¼

1

Using the Trapezoidal and Legendre Quadrature Methods

Trapezoidal

Legendre

N

Method

Quadrature Method

2

0.6046

3

0.7854

4

0.8648

5

0.9070

1.7605 0.8793 1.0080 0.9997

As

a

simple

test

case,

we

can

evaluate

Ð

1 1

(px=2)

sin(px)

dx.

This

integral

can be evaluated exactly via integration by parts, and we have chosen the func-

tion so that the value of the integral is exactly 1. Some results from applying

the trapezoidal method to our test integral are shown in Table 3.1. Not surpris-

ingly, as we use a smaller step size for the space between values of x where we

evaluate the function, the method becomes more accurate. The results in

Table 3.1 suggest that using a value of n much larger than 5 would be necess-

ary to evaluate the integral to, say, an accuracy of 1%.

Two features of the trapezoidal method are that we use a uniform spacing

between the positions where we evaluate f (x) and that every evaluation of

f (x) (except the end points) is given equal weight. Neither of these conditions

is necessary or even desirable. An elegant class of integration methods called

Gaussian quadrature deﬁnes methods that have the form

1
ð

n

f (x) dx ﬃ X cj f (xj),

1

j1

(3:8)

where the integration points x j are related to roots of orthogonal polynomials and the weights cj are related to integrals involving these polynomials. For integrals on the domain [21,1], this approach is called Legendre quadrature. To give one speciﬁc example, when n 3, the weights and integration points in Eq. (3.8) are x1 ¼ x3 ¼ 0:775967, x2 ¼ 0, c1 ¼ c3 ¼ 0:555555, and c2 ¼ 0:88888. The results from applying this method to our test integral are listed in Table 3.1. In striking contrast to the trapezoidal method, the results converge very quickly to the correct result as n is increased. In this case, the error in the integral is ,1% once n . 3.
The example above of numerically integrating a one-dimensional function can be summarized in three main points that also apply to multidimensional integrals:

1. Integrals can be approximated by evaluating the function to be integrated at a set of discrete points and summing the function values with appropriate weighting for each point.

3.1 RECIPROCAL SPACE AND k POINTS 55
2. Well-behaved numerical methods of this type will give more accurate results as the number of discrete points used in the sum is made larger. In the limit of using a very large number of points, these numerical methods converge to the exact result for the integral.
3. Different choices for the placement and weighting of the functional evaluations can give dramatic differences in the rate the numerical methods converge to the exact integral.
3.1.3 Choosing k Points in the Brillouin Zone
Because integrals such as Eq. (3.6) take up so much of the computational effort of DFT calculations, it is not surprising that the problem of efﬁciently evaluating these integrals has been studied very carefully. The solution that is used most widely was developed by Monkhorst and Pack in 1976. Most DFT packages offer the option of choosing k points based on this method. To use this method, all that is needed is to specify how many k points are to be used in each direction in reciprocal space. For calculations with supercells that have the same length along each lattice vector, and therefore the same length along each reciprocal lattice vector, it is natural to use the same number of k points in each direction. If M k points are used in each direction, it is usual to label the calculations as using M Â M Â M k points.
From the general discussion of numerical integration above, it is clear that we should expect that a calculation using M Â M Â M k points will give a more accurate result than a calculation with N Â N Â N k points if M . N. But, in practice, how should we choose how many k points to use? Answering this apparently simple question is more subtle than you might ﬁrst think, as illustrated by the data in Table 3.2. The data in this table are from calculations done in the same way as the fcc Cu calculations described in Section 2.2 at the optimized lattice constant determined from those calculations, that is, a lattice constant of 3.64 A˚ . Each calculation listed in Table 3.2 used k points deﬁned using the Monkhorst – Pack approach with M Â M Â M k points. As in Section 2.2, these calculations used a cubic supercell for the fcc material containing four distinct atoms rather than the primitive cell. Selected results from this table are shown graphically in Fig. 3.2.
Look ﬁrst at the energies listed in Table 3.2 and plotted in Fig. 3.2. When M . 8, the total energy is seen to be (almost) independent of the number of k points, as we should expect if all our calculations in k space are numerically well converged. More speciﬁcally, the variation in the energy as M is varied in this range is less than 0.003 eV. For smaller numbers of k points, however, the energy varies considerably as the number of k points is changed—a clear indication that the number of k points is insufﬁcient to give a well-converged result.

56 NUTS AND BOLTS OF DFT CALCULATIONS

TABLE 3.2 Results from Computing the Total Energy of fcc Cu with M 3 M 3 M k Points Generated Using the Monkhorst Pack Method

M

E/atom (eV)

No. of k Points in IBZ

tM =t1

1

1.8061

2

3.0997

3

3.6352

4

3.7054

5

3.7301

6

3.7541

7

3.7676

8

3.7671

9

3.7680

10

3.7676

11

3.7662

12

3.7665

13

3.7661

14

3.7659

1

1.0

1

1.1

4

2.3

4

2.6

10

5.2

10

6.0

20

10.4

20

11.2

35

16.9

35

17.1

56

31.2

56

28.5

84

40.0

84

39.7

The last column in Table 3.2 lists the computational time taken for the total energy calculations, normalized by the result for M 1. Note that getting what we just deﬁned to be a converged result takes at least 20 times longer than a calculation involving just one k point. An initially curious feature of these results is that if M is an odd number, then the amount of time taken for the calculations with either M or (M þ 1) was close to the same. This occurs because the calculations take full advantage of the many symmetries that exist in a

Figure 3.2 Total energies (per atom) for bulk Cu calculated as described in Table 3.2 as a function of M for calculations using M Â M Â M k points. Results with even (odd) M are shown with unﬁlled (ﬁlled) symbols.

3.1 RECIPROCAL SPACE AND k POINTS 57
perfect fcc solid. These symmetries mean that the integrals in reciprocal space do not need to be evaluated using the entire BZ, instead they can just be evaluated in a reduced portion of the zone that can then be extended without approximation to ﬁll the entire BZ using symmetry. This reduced region in k space is called the irreducible Brillouin zone (IBZ). For very symmetric materials such as the perfect fcc crystal, using just the IBZ greatly reduces the numerical effort required to perform integrals in k space. For example, for the 10 Â 10 Â 10 Monkhorst –Pack sampling of the BZ, only 35 distinct points in k space lie within the IBZ for our current example (compared to the 1000 that would be used if no symmetry at all was used in the calculation).
Table 3.2 lists the number of k points in the IBZ for each calculation. Comparing these with the timings also listed in the table explains why pairs of calculations with odd and even values of M took the same time—they have the same number of distinct points to examine in the IBZ. This occurs because in the Monkhorst – Pack approach using an odd value of M includes some k points that lie on the boundaries of the IBZ (e.g., at the G point) while even values of M only give k points inside the IBZ. An implication of this observation is that when small numbers of k points are used, we can often expect slightly better convergence with the same amount of computational effort by using even values of M than with odd values of M.† Of course, it is always best to have demonstrated that your calculations are well converged in terms of k points. If you have done this, the difference between even and odd numbers of k points is of limited importance.
To show how helpful symmetry is in reducing the work required for a DFT calculation, we have repeated some of the calculations from Table 3.2 for a four-atom supercell in which each atom was given a slight displacement away from its fcc lattice position. These displacements were not large—they only changed the nearest-neighbor spacings between atoms by +0.09 A˚ , but they removed all symmetry from the system. In this case, the number of k points in the IBZ is M3=2. The results from these calculations are listed in Table 3.3. This table also lists DE, the energy difference between the symmetric and nonsymmetric calculations.
Although the total computational time in the examples in Table 3.3 is most closely related to the number of k points in the IBZ, the convergence of the calculations in k space is related to the density of k points in the full BZ. If we compare the entries in Table 3.3 with Fig. 3.2, we see that the calculation for the nonsymmetric system with M 8 is the only entry in the table that might be considered moderately well converged. Further calculations with larger numbers of k points would be desirable if a highly converged energy for the nonsymmetric system was needed.
†In some situations such as examining electronic structure, it can be important to include a k point at the G point.

58 NUTS AND BOLTS OF DFT CALCULATIONS

TABLE 3.3 Results from Computing the Total Energy of the Variant of fcc Cu with Broken Symmetrya

E/atom

DE/atom

No. of k

M

(eV)

(eV)

Points in IBZ

tM =t1

1

1.8148

0.009

1

1.0

2

3.0900

0.010

4

2.1

3

3.6272

0.008

14

5.6

4

3.6969

0.009

32

12.3

5

3.7210

0.009

63

21.9

6

3.7446

0.010

108

40.1

7

3.7577

0.010

172

57.5

8

3.7569

0.010

256

86.8

aDescribed in the text with M Â M Â M k points generated using the Monkhorst Pack method.

You may have noticed that in Table 3.3 the column of energy differences, DE, appears to converge more rapidly with the number of k points than the total energy E. This is useful because the energy difference between the two conﬁgurations is considerably more physically interesting than their absolute energies. How does this happen? For any particular set of k points, there is some systematic difference between our numerically evaluated integrals for a particular atomic conﬁguration and the true values of the same integrals. If we compare two conﬁgurations of atoms that are structurally similar, then it is reasonable to expect that this systematic numerical error is also similar. This means that the energy difference between the two states can be expected to cancel out at least a portion of this systematic error, leading to calculated energy differences that are more accurate than the total energies from which they are derived. It is important to appreciate that this heuristic argument relies on the two conﬁgurations of atoms being “similar enough” that the systematic error from using a ﬁnite number of k points for each system is similar. For the example in Table 3.3, we deliberately chose two conﬁgurations that differ only by small perturbations of atom positions in the supercell, so it is reasonable that this argument applies. It would be far less reasonable, however, to expect this argument to apply if we were comparing two signiﬁcantly different crystal structures for a material.
There are many examples where it is useful to use supercells that do not have the same length along each lattice vector. As a somewhat artiﬁcial example, imagine we wanted to perform our calculations for bulk Cu using a supercell that had lattice vectors

a1 ¼ a(1,0,0), a2 ¼ a(0,1,0), a3 ¼ a(0,0,4):

(3:9)

3.1 RECIPROCAL SPACE AND k POINTS 59
This supercell contains 16 distinct atoms rather than the 4 atoms we used in the calculations above with a cubic supercell. How many k points should we use for calculations with this extended supercell to get results as well converged as the results in Table 3.2 for the cubic supercell? A useful rule of thumb is that calculations that have similar densities of k points in reciprocal space will have similar levels of convergence. This rule of thumb suggests that using 8Â8Â2 k points would give reasonable convergence, where the three numbers refer to the number of k points along the reciprocal lattice vectors b1, b2, and b3, respectively. You should check from the deﬁnition of the reciprocal lattice vectors that this choice for the k points will deﬁne a set of k points with equal density in each direction in reciprocal space.
3.1.4 Metals—Special Cases in k Space
When we introduced the idea of numerical integration, we looked at the integral Ð 11(px=2) sin(px) dx as an example. A feature of this problem that we did not comment on before is that it is an integral of a continuous function. This is a useful mathematical property in terms of developing numerical methods that converge rapidly to the exact result of the integral, but it is not a property that is always available in the k space integrals in DFT calculations. An especially important example of this observation is for metals. One useful deﬁnition of a metal is that in a metal the Brillouin zone can be divided into regions that are occupied and unoccupied by electrons. The surface in k space that separates these two regions is called the Fermi surface. From the point of view of calculating integrals in k space, this is a signiﬁcant complication because the functions that are integrated change discontinuously from nonzero values to zero at the Fermi surface. If no special efforts are made in calculating these integrals, very large numbers of k points are needed to get well-converged results.
Metals are a rather important subset of all materials, so useful algorithms to improve the slow convergence just mentioned have been developed. We will describe the two best-known methods. The ﬁrst is called the tetrahedron method. The idea behind this method is to use the discrete set of k points to deﬁne a set of tetrahedra that ﬁll reciprocal space and to deﬁne the function being integrated at every point in a tetrahedron using interpolation. At the simplest level, linear interpolation can be used within each tetrahedron. Once this interpolation has been completed, the function to be integrated has a simple form at all positions in k space and the integral can then be evaluated using the entire space, not just the original discrete points. Blo¨chl developed a version of this method that includes interpolation that goes beyond linear interpolation; this is now the most widely used tetrahedron method (see Further Reading).
A different approach to the discontinuous integrals that appear for metals are the smearing methods. The idea of these methods is to force the function

60 NUTS AND BOLTS OF DFT CALCULATIONS

Figure 3.3 Fermi Dirac function [Eq. (3.10)] with k0 ¼ 1 and several values of s.

being integrated to be continuous by “smearing” out the discontinuity. An example of a smearing function is the Fermi –Dirac function:

k f

k0 ¼

k exp

k0

þ

! 1

1
:

s

s

(3:10)

Figure 3.3 shows the shape of this function for several values of s. It can be seen from the ﬁgure that as s ! 0, the function approaches a step function that changes discontinuously from 1 to 0 at k k0. The idea of using a smearing method to evaluate integrals is to replace any step functions with smooth functions such as the Fermi –Dirac function since this deﬁnes a continuous function that can be integrated using standard methods. Ideally, the result of the calculation should be obtained using some method that extrapolates the ﬁnal result to the limit where the smearing is eliminated (i.e., s ! 0 for the Fermi –Dirac function).
One widely used smearing method was developed by Methfessel and Paxton. Their method uses expressions for the smearing functions that are more complicated than the simple example above but are still characterized by a single parameter, s (see Further Reading).

3.1.5 Summary of k Space
Because making good choices about how reciprocal space is handled in DFT calculations is so crucial to performing meaningful calculations, you should

3.2 ENERGY CUTOFFS 61
read the following summary carefully and come back to it when you start doing calculations on your own. The key ideas related to getting well-converged results in k space include:
1. Before pursuing a large series of DFT calculations for a system of interest, numerical data exploring the convergence of the calculations with respect to the number of k points should be obtained.
2. The number of k points used in any calculation should be reported since not doing so makes reproduction of the result difﬁcult.
3. Increasing the volume of a supercell reduces the number of k points needed to achieve convergence because volume increases in real space correspond to volume decreases in reciprocal space.
4. If calculations involving supercells with different volumes are to be compared, choosing k points so that the density of k points in reciprocal space is comparable for the different supercells is a useful way to have comparable levels of convergence in k space.
5. Understanding how symmetry is used to reduce the number of k points for which calculations are actually performed can help in understanding how long individual calculations will take. But overall convergence is determined by the density of k points in the full Brillouin zone, not just the number of k points in the irreducible Brillouin zone.
6. Appropriate methods must be used to accurately treat k space for metals.

3.2 ENERGY CUTOFFS

Our lengthy discussion of k space began with Bloch’s theorem, which tells us that solutions of the Schro¨dinger equation for a supercell have the form

Á fk(r) ¼ exp(ik r)uk(r),

(3:11)

where uk(r) is periodic in space with the same periodicity as the supercell. It is now time to look at this part of the problem more carefully. The periodicity of uk(r) means that it can be expanded in terms of a special set of plane waves:

Á uk(r) ¼ X cG exp[iG r], G

(3:12)

where the summation is over all vectors deﬁned by G ¼ m1b1 þ m2b2 þ m3b3
Á with integer values for mi. These set of vectors deﬁned by G in reciprocal
space are deﬁned so that for any real space lattice vector ai, G ai ¼ 2pmi.

62 NUTS AND BOLTS OF DFT CALCULATIONS

(You can check this last statement by using the deﬁnition of the reciprocal lattice vectors.)
Combining the two equations above gives

fk(r) ¼ X ckþG exp[i(k þ G)r]:
G

(3:13)

According to this expression, evaluating the solution at even a single point in k space involves a summation over an inﬁnite number of possible values of G. This does not sound too promising for practical calculations! Fortunately, the functions appearing in Eq. (3.13) have a simple interpretation as solutions of the Schro¨dinger equation: they are solutions with kinetic energy

E ¼ h2 jk þ Gj2: 2m
It is reasonable to expect that the solutions with lower energies are more physically important than solutions with very high energies. As a result, it is usual to truncate the inﬁnite sum above to include only solutions with kinetic energies less than some value:

Ecut

¼

h2 2m

G2cut

:

The inﬁnite sum then reduces to

X

fk(r) ¼

ckþG exp[i(k þ G)r]:

jGþkj,Gcut

(3:14)

This expression includes slightly different numbers of terms for different values of k¯ .
The discussion above has introduced one more parameter that must be deﬁned whenever a DFT calculation is performed—the cutoff energy, Ecut. In many ways, this parameter is easier to deﬁne than the k points, as most packages will apply sensible default settings if no other information is supplied by the user. Just as with the k points, it is good practice to report the cutoff energy used in your calculations to allow people to reproduce your results easily. Figure 3.4 shows as an example of the convergence of the energy of fcc Cu as the energy cutoff in our calculations was varied.
There is one common situation where it is unwise to blindly accept default values for the cutoff energy. In most cases, a default cutoff energy is

3.2 ENERGY CUTOFFS 63
Figure 3.4 Energy per atom of fcc Cu with a lattice constant of 3.64 A˚ using 12 Â 12 Â 12 k points as a function of the energy cutoff, plotted using a similar energy scale to Fig. 3.2.
assigned for each element and the largest cutoff energy for any of the atoms in the supercell is assigned as the overall cutoff energy. Suppose we used this approach, for example, while attempting to calculate the energy change in forming an ordered body-centered cubic (bcc) copper-palladium alloy from bulk palladium and copper, fcc Pd(s) þ fcc Cu(s) ! bcc CuPd(s). We can calculate this energy change by ﬁnding well-converged energies for each of the solids separately. (Exercise 7 at the end of the chapter asks you to do exactly this.) For the purposes of this example, we will assume that the energy cutoffs for Pd and Cu satisfy EPd . ECu. If we used the default cutoff energies, we would calculate the energies of fcc Pd and bcc CuPd using an energy cutoff of EPd, but the energy of fcc Cu would be calculated using an energy cutoff of ECu. This approach introduces a systematic error into the calculated energy difference that can easily be removed by performing the fcc Cu calculation using the higher cutoff energy, EPd. The key point to remember here is whenever DFT calculations for multiple systems are compared to calculate energy differences, the same energy cutoff should be used in all calculations.
3.2.1 Pseudopotentials The discussion above points to the fact that large energy cutoffs must be used to include plane waves that oscillate on short length scales in real space. This is problematic because the tightly bound core electrons in atoms are associated

64 NUTS AND BOLTS OF DFT CALCULATIONS
with wave functions with exactly this kind of oscillation. From a physical point of view, however, core electrons are not especially important in deﬁning chemical bonding and other physical characteristics of materials; these properties are dominated by the less tightly bound valence electrons. From the earliest developments of plane-wave methods, it was clear that there could be great advantages in calculations that approximated the properties of core electrons in a way that could reduce the number of plane waves necessary in a calculation.
The most important approach to reducing the computational burden due to core electrons is to use pseudopotentials. Conceptually, a pseudopotential replaces the electron density from a chosen set of core electrons with a smoothed density chosen to match various important physical and mathematical properties of the true ion core. The properties of the core electrons are then ﬁxed in this approximate fashion in all subsequent calculations; this is the frozen core approximation. Calculations that do not include a frozen core are called all-electron calculations, and they are used much less widely than frozen core methods. Ideally, a pseudopotential is developed by considering an isolated atom of one element, but the resulting pseudopotential can then be used reliably for calculations that place this atom in any chemical environment without further adjustment of the pseudopotential. This desirable property is referred to as the transferability of the pseudopotential. Current DFT codes typically provide a library of pseudopotentials that includes an entry for each (or at least most) elements in the periodic table.
The details of a particular pseudopotential deﬁne a minimum energy cutoff that should be used in calculations including atoms associated with that pseudopotential. Pseudopotentials requiring high cutoff energies are said to be hard, while more computationally efﬁcient pseudopotentials with low cutoff energies are soft. The most widely used method of deﬁning pseudopotentials is based on work by Vanderbilt; these are the ultrasoft pseudopotentials (USPPs). As their name suggests, these pseudopotentials require substantially lower cutoff energies than alternative approaches.
One disadvantage of using USPPs is that the construction of the pseudopotential for each atom requires a number of empirical parameters to be speciﬁed. Current DFT codes typically only include USPPs that have been carefully developed and tested, but they do in some cases include multiple USPPs with varying degrees of softness for some elements. Another frozen core approach that avoids some of the disadvantages of USPPs is the projector augmented-wave (PAW) method originally introduced by Blo¨chl and later adapted for plane-wave calculations by Kresse and Joubert. Kresse and Joubert performed an extensive comparison of USPP, PAW, and allelectron calculations for small molecules and extended solids.1 Their work shows that well-constructed USPPs and the PAW method give results that

3.3 NUMERICAL OPTIMIZATION 65
are essentially identical in many cases and, just as importantly, these results are in good agreement with all-electron calculations. In materials with strong magnetic moments or with atoms that have large differences in electronegativity, the PAW approach gives more reliable results than USPPs.
3.3 NUMERICAL OPTIMIZATION
All of our work on numerical convergence in this chapter has so far focused on ﬁnding numerically precise solutions for the electron density and total energy of a conﬁguration of atoms within DFT. If you think back to the examples in Chapter 2, you will soon remember that we compared a series of total energy calculations to predict the most stable crystal structure and lattice constant for a simple material, bulk Cu. One of these examples, Cu in the hcp crystal structure, illustrated the idea that when a crystal structure has more than one degree of freedom, ﬁnding the minimum energy structure by systematically varying the degrees of freedom is, at best, tedious.
To make practical use of our ability to perform numerically converged DFT calculations, we also need methods that can help us effectively cope with situations where we want to search through a problem with many degrees of freedom. This is the topic of numerical optimization. Just like understanding reciprocal space, understanding the central concepts of optimization is vital to using DFT effectively. In the remainder of this chapter, we introduce these ideas.
3.3.1 Optimization in One Dimension
We begin with a topic that is crucial to performing DFT calculations but has ramiﬁcations in many other areas as well. A simple starting point is the following mathematical problem: ﬁnd a local minimum of f (x), where f (x) is some function of x that is uniquely deﬁned at each x. We will assume that f (x) is a “smooth” function, meaning that its derivatives exist and are continuous. You should also think of an example where f (x) is so complicated that you cannot possibly solve the problem algebraically. In other words, we have to use a computer to ﬁnd a solution to the problem numerically. Note that we are asking for “a” local minimum, not “all” local minima or the lowest possible value of the function. This means we are trying to solve a local optimization problem, not a global optimization problem.
It is hopefully not hard to see why this mathematical problem (or closely related problems) are important in DFT calculations. The problem of ﬁnding the lattice constant for an fcc metal that we looked at in Chapter 2 can be cast as a minimization problem. If we do not know the precise arrangement

66 NUTS AND BOLTS OF DFT CALCULATIONS
of N atoms within a supercell, then the total energy of the supercell can be written as E(x), where x is a 3N-dimensional vector deﬁning the atomic positions. Finding the physically preferred arrangement of atoms is equivalent to minimizing this energy function.Ã
For now we will consider the original one-dimensional problem stated above: Find a local minimum of f (x). If we can ﬁnd a value of x where f 0(x) ¼ df =dx ¼ 0, then this point deﬁnes either a local maximum or a minimum of the function. Since it is easy to distinguish a maximum from a minimum (either by looking at the second derivative or more simply by evaluating the function at some nearby points), we can redeﬁne our problem as: ﬁnd a value of x where f 0(x) ¼ 0. We will look at two numerical methods to solve this problem. (There are many other possible methods, but these two will illustrate some of the key concepts that show up in essentially all possible methods.)
The ﬁrst approach is the bisection method. This method works in more or less the way you ﬁnd a street address when driving in an unfamiliar neighborhood on a dark rainy night. In that case, you try and establish that the address you want is somewhere between two points on the street, then you look more carefully between those two points. To be more speciﬁc, we begin by ﬁnding two points, x1 ¼ a and x2 ¼ b so that the sign of f 0(x) at the two points is different: f 0(x1)f 0(x2) , 0. As long as f 0(x) is smooth, we then know that there is some x between a and b where f 0(x) ¼ 0. We do not know where this point is, so we look right in the middle by evaluating f 0(xÃ) at xÃ ¼ (x1 þ x2)=2. We can now see one of two things. If f 0(x1)f 0(xÃ) , 0, then we have established that f 0(x) ¼ 0 for some x between x1 and xÃ. Alternatively, if f 0(xÃ) f 0(x2) , 0, then f 0(x) ¼ 0 for some x between xÃ and x2. You should convince yourself by drawing a picture that if there is only one place where f 0(x) ¼ 0 in the interval [a,b] that exactly one of these two possibilities can occur (not both of them or neither of them). In either case, we have reduced the size of the interval in which we are searching by a factor of 2. We can now repeat the whole procedure for this new interval. Each time the calculation is repeated, a new value of xÃ is generated, and if we continue repeating the calculation long enough, these values of xÃ will get closer and closer to the situation we are looking for where f 0(x) ¼ 0.
As a simple example, we can apply the bisection method to ﬁnd a minimum of f (x) ¼ e x cos x. This function has a minimum at x 2.356194490. . . . If we use the bisection method starting with x1 ¼ 1:8 and x2 ¼ 3, then we generate the following series of approximations to the solution: xÃ 2.4, 2.1, 2.25, 2.325, 2.3625, 2.34375, . . . . These are getting closer to the actual solution as we continue to repeat the calculation, although after applying
ÃAs we will see in Section 3.4, minimization is also crucial in other aspects of DFT calculations.

3.3 NUMERICAL OPTIMIZATION 67
Figure 3.5 Illustration of one iteration of Newton’s method for a function g(x).
the algorithm six times we still only have a solution that is accurate to two signiﬁcant digits.
An alternative approach to our problem is Newton’s method. The idea behind this method is illustrated in Fig. 3.5. If we deﬁne g(x) ¼ f 0(x), then from a Taylor expansion g(x þ h) ﬃ g(x) þ hg0(x). This expression neglects terms with higher orders of h, so it is accurate for “small” values of h. If we have evaluated our function at some position where g(x) = 0, our approximate equation suggests that a good estimate for a place where the function is zero is to deﬁne xÃ ¼ x þ h ¼ x g(x)=g0(x). Because the expression from the Taylor expansion is only approximate, xÃ does not exactly deﬁne a place where g(xÃ) ¼ 0. Just as we did with the bisection method, we now have to repeat our calculation. For Newton’s method, we repeat the calculation starting with the estimate from the previous step.
Applying Newton’s method to ﬁnding a minimum of f (x) ¼ e x cos x starting with x 1.8 generates the following series of approximations to the solution: xÃ 1.8, 2.183348, 2.331986, 2.355627, 2.356194, . . . (showing seven signiﬁcant ﬁgures for each iterate). Just as with the bisection method, this series of solutions is converging to the actual value of x that deﬁnes a minimum of the function. A striking difference between the bisection method and Newton’s method is how rapidly the solutions converge. A useful way to characterize the rate of convergence is to deﬁne 1i ¼ jxi xj, where xi is the approximation to the true solution x after applying the numerical method i times. Figure 3.6 plots the result for the two algorithms. It is clear from this ﬁgure that Newton’s method is enormously better than the bisection method in terms of how rapidly it allows us to ﬁnd the solution.
There is one crucial aspect of these numerical methods that we have not yet discussed. Both methods generate a series of approximations that in the

68 NUTS AND BOLTS OF DFT CALCULATIONS
Figure 3.6 Convergence analysis for the bisection method and Newton’s method calculations described in the text.
long run get closer and closer to a solution. How do we know when to stop? In a real application, we do not know the exact solution, so we cannot use 1i ¼ jxi xj as our measure of how accurate our current estimate is. Instead, we have to deﬁne a stopping criteria that depends only on the series of approximations we have calculated. A typical choice is to continue iterating until the difference between successive iterates is smaller than some tolerance, that is, jxiþ1 xij , d. If we chose d 0.01 for the examples above, we would stop after 6 iterations of the bisection method, concluding that x ﬃ 2.34375, or after 3 iterations of Newton’s method, concluding that x ﬃ 2.355628. If we chose a smaller tolerance, d 0.001, we would stop after 11 (4) iterations of the bisection method (Newton’s method) with the approximation that x ﬃ 2.3554688 (x ﬃ 2.3561942).
The examples above demonstrate several general properties of numerical optimization that are extremely important to appreciate. They include:
1. The algorithms are iterative, so they do not provide an exact solution; instead, they provide a series of approximations to the exact solution.
2. An initial estimate for the solution must be provided to use the algorithms. The algorithms provide no guidance on how to choose this initial estimate.
3. The number of iterations performed is controlled by a tolerance parameter that estimates how close the current solution is to the exact solution.

3.3 NUMERICAL OPTIMIZATION 69
4. Repeating any algorithm with a different tolerance parameter or a different initial estimate for the solution will generate multiple ﬁnal approximate solutions that are (typically) similar but are not identical.
5. The rate at which different algorithms converge to a solution can vary greatly, so choosing an appropriate algorithm can greatly reduce the number of iterations needed.
There are other general properties of these methods that have not been directly illustrated by our examples but are equally important. These include:
6. These methods cannot tell us if there are multiple minima of the function we are considering if we just apply the method once. Applying a method multiple times with different initial estimates can yield multiple minima, but even in this case the methods do not give enough information to prove that all possible minima have been found. The function we used as an example above was chosen speciﬁcally because it has multiple minima. Exercise 1 at the end of the chapter asks you to explore this idea.
7. For most methods, no guarantees can be given that the method will converge to a solution at all for an arbitrary initial estimate. Exercise 2 at the end of the chapter gives a speciﬁc example to illustrate this idea.

3.3.2 Optimization in More than One Dimension

We now return to the problem mentioned above of minimizing the total energy of a set of atoms in a supercell, E(x), by varying the positions of the atoms within the supercell. This example will highlight some of the complications that appear when we try and solve multivariable optimization problems that are not present for the one-dimensional situations we have already discussed. If we deﬁne gi(x) ¼ @E(x)=@xi, then minimizing E(x) is equivalent to ﬁnding a set of positions for which gi(x) ¼ 0 simultaneously for i 1, . . . , 3N.
There is no natural way to generalize the one-dimensional bisection method to solve this multidimensional problem. But it is possible to generalize Newton’s method to this situation. The one-dimensional Newton method was derived using a Taylor expansion, and the multidimensional problem can be approached in the same way. The result involves a 3N Â 3N matrix of derivatives, J, with elements Jij ¼ @gi=@xj. Note that the elements of this matrix are the second partial derivatives of the function we are really interested in, E(x). Newton’s method deﬁnes a series of iterates by

xiþ1 ¼ xi J 1(xi)g(xi):

(3:15)

This looks fairly similar to the result we derived for the one-dimensional case, but in practice is takes a lot more effort to apply. Consider a situation where we

70 NUTS AND BOLTS OF DFT CALCULATIONS
have 10 distinct atoms, so we have 30 distinct variables in our problem. We will have to calculate 55 distinct matrix elements of J during each iteration. The other 45 matrix elements are then known because the matrix is symmetric. We then have to solve a linear algebra problem involving a 30 Â 30 matrix. You should compare this to how much work was involved in taking one step in the one-dimensional case. It is certainly quite feasible to perform all of these calculations for the multidimensional problem computationally. The main point of this discussion is that the amount of numerical effort required to tackle multidimensional optimization grows rather dramatically as the number of variables in the problem is increased.
Another distinction between one-dimensional and multidimensional problems is associated with searching for multiple solutions. If we aim to ﬁnd all the minima of a one-dimensional function, f(x), in an interval [a,b], we can in principle repeatedly apply Newton’s method with many initial estimates in the interval and build up a list of distinct local minima. We cannot rigorously prove that the list we build up in this way is complete, but it is not too difﬁcult for a simple example to do enough calculations to be convinced that no new minima will show up by performing more calculations. The same idea cannot be applied in 30 dimensions simply because of the vast number of possible initial estimates that would have to be considered to fully sample the possible space of solutions. As a result, it is in general extremely difﬁcult to perform optimization in multiple dimensions for problems relevant to DFT calculations in a way that gives any information other than the existence of a local minimum of a function. Algorithms to rigorously ﬁnd the global minimum value of a function do exist, but they are extraordinarily computationally intensive. To our knowledge, no one has even attempted to use algorithms of this kind in conjunction with DFT calculations.
At this point it may seem as though we can conclude our discussion of optimization methods since we have deﬁned an approach (Newton’s method) that will rapidly converge to optimal solutions of multidimensional problems. Unfortunately, Newton’s method simply cannot be applied to the DFT problem we set ourselves at the beginning of this section! To apply Newton’s method to minimize the total energy of a set of atoms in a supercell, E(x), requires calculating the matrix of second derivatives of the form @2E=@xi @xj. Unfortunately, it is very difﬁcult to directly evaluate second derivatives of energy within plane-wave DFT, and most codes do not attempt to perform these calculations. The problem here is not just that Newton’s method is numerically inefﬁcient—it just is not practically feasible to evaluate the functions we need to use this method. As a result, we have to look for other approaches to minimize E(x). We will brieﬂy discuss the two numerical methods that are most commonly used for this problem: quasi-Newton and

3.3 NUMERICAL OPTIMIZATION 71
conjugate-gradient methods. In both cases we will only sketch the main concepts. Resources that discuss the details of these methods are listed in the Further Reading section at the end of the chapter.
The essence of quasi-Newton methods is to replace Eq. (3.15) by

xiþ1 ¼ xi Ai 1(xi)g(xi),

(3:16)

where Ai is a matrix that is deﬁned to approximate the Jacobian matrix. This matrix is also updated iteratively during the calculation and has the form

Ai ¼ Ai 1 þ F[xi, xi 1, G(xi), g(xi 1)]:

(3:17)

We have referred to quasi-Newton methods rather than the quasi-Newton method because there are multiple deﬁnitions that can be used for the function F in this expression. The details of the function F are not central to our discussion, but you should note that this updating procedure now uses information from the current and the previous iterations of the method. This is different from all the methods we have introduced above, which only used information from the current iteration to generate a new iterate. If you think about this a little you will realize that the equations listed above only tell us how to proceed once several iterations of the method have already been made. Describing how to overcome this complication is beyond our scope here, but it does mean than when using a quasi-Newton method, the convergence of the method to a solution should really only be examined after performing a minimum of four or ﬁve iterations.
The conjugate-gradient method to minimizing E(x) is based on an idea that is quite different to the Newton-based methods. We will introduce these ideas with a simple example: calculating the minima of a two-dimensional function E(x) ¼ 3x2 þ y2, where x ¼ (x, y). Hopefully, you can see right away what the solution to this problem: the function has exactly one minimum and it lies at x (0,0). We will try to ﬁnd this solution iteratively starting from x0 ¼ (1,1). Since we are trying to minimize the function, it makes sense to look in a direction that will cause the function to decrease. A basic result from vector calculus is that the function E(x) decreases most rapidly along a direction deﬁned by the negative of the gradient of the function, rE(x). This suggests we should generate a new estimate by deﬁning

x1 ¼ x0 a0rE(x0) ¼ (1 6a0, 1 2a0):

(3:18)

This expression tells us to look along a particular line in space, but not where on that line we should place our next iterate. Methods of this type are known as

72 NUTS AND BOLTS OF DFT CALCULATIONS

steepest descent methods. Ideally, we would like to choose the step length, a0,

so that E(x1) is smaller than the result we would get with any other choice for
the step length. For our example, a little algebra shows that E[x1(a0)] is minimized if we choose a0 ¼ 258.
We can now repeat this procedure to generate a second iterate. This time, we

want to search along a direction deﬁned by the gradient of E(x) evaluated at

x1: rE(x1) ¼ À

3 7

,

97Á.

The

point

of

deﬁning

the

details

of

this

equation

is

to highlight the following observation:

Á rE(x0) rE(x1) ¼ 0:

(3:19)

In our two-dimensional space, these two search directions are perpendicular to one another. Saying this in more general mathematical terms, the two search

directions are orthogonal. This is not a coincidence that occurs just for the speciﬁc

example we have deﬁned; it is a general property of steepest descent methods provided that the line search problem deﬁned by Eq. (3.18) is solved optimally.
We can now qualitatively describe the conjugate-gradient method for minimizing a general function, E(x), where x is an N-dimensional vector. We begin with an initial estimate, x0. Our ﬁrst iterate is chosen to lie along the direction deﬁned by d0 ¼ rE(x0), so x1 ¼ x0 a0d0. Unlike the simple example above, the problem of choosing the best (or even a good) value of a0 cannot be solved exactly. We therefore choose the step size by some approximate method that may be as simple as evaluating E(x1) for several possible step lengths and selecting the best result.
If we have the optimal step length, then the next search direction will be orthogonal to our current search direction. In general, however, rE(x0)rE(x1) = 0 because we cannot generate the optimal step length for each line search. The key idea of the conjugate gradient is to deﬁne the new search direction by insisting that it is orthogonal to the original search direction. We can do this by deﬁning

Á Á d1 ¼

rE(x1)

þ

(rE(x1 (d0

) d0

d0) )

d0:

(3:20)

This deﬁnition uses rE(x1) as an estimate for the new search direction but removes the portion of this vector that can be projected along the original search direction, d0. This process is called orthogonalization, and the resulting search direction is a conjugate direction to the original direction (hence the
name of the overall method). We now generate a second iterate by looking at various step lengths within x2 ¼ x1 a1d1.
In generating a third iterate for the conjugate-gradient method, we now estimate the search direction by rE(x2) but insist that the search direction is orthogonal to both d0 and d1. This idea is then repeated for subsequent iterations.

3.4 DFT TOTAL ENERGIES AN ITERATIVE OPTIMIZATION PROBLEM 73
We cannot continue this process indeﬁnitely because in an N-dimensional space we can only make a vector orthogonal to at most (N21) other vectors. So to make this a well-deﬁned algorithm, we have to restart the process of deﬁning search directions after some number of iterations less than N.
The conjugate-gradient method is a powerful and robust algorithm. Using it in a practical problem involves making choices such as how to select step lengths during each iteration and how often to restart the process of orthogonalizing search directions. Like the quasi-Newton method, it can be used to minimize functions by evaluating only the function and its ﬁrst derivatives.
3.3.3 What Do I Really Need to Know about Optimization?
We have covered a lot of ground about numerical optimization methods, and, if you are unfamiliar with these methods, then it is important to distill all these details into a few key observations. This is not just a mathematical tangent because performing numerical optimization efﬁciently is central to getting DFT calculations to run effectively. We highly recommend that you go back and reread the seven-point summary of properties of numerical optimization methods at the end of Section 3.3.1. You might think of these (with apologies to Stephen Covey2) as the “seven habits of effective optimizers.” These points apply to both one-dimensional and multidimensional optimization. We further recommend that as you start to actually perform DFT calculations that you actively look for the hallmarks of each of these seven ideas.

3.4 DFT TOTAL ENERGIES—AN ITERATIVE OPTIMIZATION PROBLEM

The most basic type of DFT calculation is to compute the total energy of a set of atoms at prescribed positions in space. We showed results from many calculations of this type in Chapter 2 but have not said anything about how they actually are performed. The aim of this section is to show that this kind of calculation is in many respects just like the optimization problems we discussed above.
As we discussed in Chapter 1, the main aim of a DFT calculation is to ﬁnd the electron density that corresponds to the ground-state conﬁguration of the system, r(r). The electron density is deﬁned in terms of the solutions to the Kohn– Sham equations, cj(r), by

r(r) ¼ X cj(r)cÃj (r):
j

(3:21)

74 NUTS AND BOLTS OF DFT CALCULATIONS

The Kohn –Sham equations are

h2 2m

r2cj(r)

þ

Veff (r)cj(r)

¼

1jcj(r),

(3:22)

where Veff(r) is the effective potential. The fact that makes these equations awkward to solve directly is that the effective potential is itself a complicated function of r(r).
Our introduction to numerical optimization suggests a useful general strategy for solving the problem just posed, namely, attempt to solve the problem iteratively. We begin by estimating the overall electron density, then use this trial density to deﬁne the effective potential. The Kohn –Sham equations with this effective potential are then solved numerically with this effective potential, deﬁning a new electron density. If the new electron density and the old electron density do not match, then we have not solved the overall problem. The old and new electron densities are then combined in some way to give a new trial electron density. This new trial density is then used to deﬁne a new effective potential from which an updated electron density is found, and so on. If successful, this iterative process will lead to a self-consistent solution. This description has glossed over all kinds of numerical details that were absolutely vital in making modern DFT codes numerically efﬁcient.
The similarity of this iterative process to the more general optimization problems we talked about above suggest that our “seven habits of effective optimizers” might give us some useful ideas about how to calculate energies with DFT. In particular, we need to think about how to start our iterative process. In the absence of other information, the electron density can be initially approximated by superimposing the electron densities appropriate for each atom in its isolated state. This is typically the default initialization used by most DFT packages. But we will reach a self-consistent solution much more quickly (i.e., in fewer iterations) if a better initial approximation is available. So if we have previously calculated the electron density for a situation very similar to our current atomic conﬁguration, that electron density may be a useful initial approximation. For this reason, it can sometimes be helpful to store the electron density and related information from a large calculation for use in starting a subsequent similar calculation.
We also need to think about how to stop our iterative calculations. It is not necessarily convenient to directly compare two solutions for the electron density and determine how similar they are, even though this is the most direct test for whether we have found a self-consistent solution. A method that is easier to interpret is to calculate the energy corresponding to the electron density after each iteration. This is, after all, the quantity we are ultimately interested in ﬁnding. If our iterations are converging, then the difference in energy between consecutive iterates will approach zero. This suggests that the iterations can

3.5 GEOMETRY OPTIMIZATION 75
be stopped once the magnitude of the energy difference between iterates falls below an appropriately chosen tolerance parameter. Most DFT packages deﬁne a sensible default value for this parameter, but for high-precision calculations it may be desirable to use lower values. A general feature of the iterative algorithms used in calculations of this sort is that they converge rapidly once a good approximation for the electron density is found. This means that reducing the tolerance parameter that deﬁnes the end of a self-consistent calculation by even several orders of magnitude often leads to only a handful of additional iterative steps.
Before moving on from this section, it would be a good idea to understand within the DFT package that is available to you (i) what algorithms are used for solving the self-consistent Kohn –Sham problem and (ii) how you can verify from the output of a calculation that a self-consistent solution was reached.
3.5 GEOMETRY OPTIMIZATION
3.5.1 Internal Degrees of Freedom
We have so far only described DFT calculations in which the position of every atom in the supercell is ﬁxed. This is ﬁne if we only want to predict the lattice constants of simple solids, but other than that it can only give us a limited view of the world! Let us imagine, for example, that we are interested in a set of reactions involving nitrogen. One thing we would certainly like to know would be the energy of a molecule of gas-phase N2 since this will probably be needed for describing any overall reaction that begins with gaseous N2. To calculate this energy, we need to ﬁnd the geometry of N2 that minimizes the molecule’s total energy. Because this is such a simple molecule, this task means that we have to determine a single bond length. How can we do this using DFT calculations based on periodic supercells?
To mimic a gas-phase molecule, we need to build a supercell that is mainly empty space. One simple way to do this is to deﬁne a cubic supercell with side length L angstroms and place two N atoms in the supercell with fractional coordinates (0,0,0) and (þd/L,0,0). So long as L is considerably longer than d, this supercell represents an isolated N2 molecule with bond length d.Ã We can now ﬁnd the DFT-optimized bond length for N2 by using either the quasi-Newton or conjugate-gradient methods deﬁned above to minimize the total energy of our supercell, ﬁxing the size and shape of the supercell but allowing the fractional coordinates of the two atoms to vary. To do this, we must deﬁne the stopping criterion that will be used to decide whether these iterative schemes have converged to a minimum. Because we are
ÃAs long as the supercell is large enough, the inﬂuence of the periodic images of the molecule on the total energy will be small, particularly for a molecule with no dipole moment like N2.

76 NUTS AND BOLTS OF DFT CALCULATIONS
searching for a conﬁguration where the forces on both atoms are zero, we continue our iterative calculations until the magnitude of the force on both atoms is less than 0.01 eV/A˚ . In this example, the magnitude of the forces on the two atoms are always identical by symmetry, although, of course, the forces on the two atoms point in opposite direction. Why is this force criterion reasonable? If we change an atom’s position by a small amount, Dr, then the change in total energy due to this change can be estimated by jDEj ﬃ jFjjDrj, where jFj is the force on the atom. If the forces on all atoms are less than 0.01 eV/A˚ , then moving any individual atom by 0.1 A˚ , a relatively signiﬁcant distance in terms of chemical bond lengths, will change the total energy by less than 0.001 eV, a small amount of energy. This argument only tells us that the order of magnitude of the force in the stopping criterion is reasonable.
The second choice we must make is how far apart to place the atoms initially. It may seem like this choice is not too important since we are going to minimize the energy, and there is presumably only one bond length that actually deﬁnes this energy minimum. Unfortunately, choosing the initial estimate for the bond length can make a large impact on whether the calculation is successful or not. As an example, we will consider two calculations that used a conjugate-gradient optimization method that differed only in the initial distance between the atoms. When we initially placed the atoms 1 A˚ apart, our calculations proceeded for 11 conjugate-gradient iterations, after which the forces satisﬁed the criterion above and the bond length was 1.12 A˚ . The experimentally observed N2 bond length is 1.10 A˚ . As with the lattice parameters of bulk solids that we examined in Chapter 2, the DFT result is not exact, but the difference between the DFT optimized geometry and the experimental result is small.
In a second calculation, we initially placed the atoms 0.7 A˚ apart. In this case, after 25 conjugate-gradient steps then two atoms are 2.12 A˚ apart and, even worse, the algorithm has not converged to an energy minimum. What has happened? Let us think about our initial state with the two atoms 0.7 A˚ apart. This corresponds physically to a N2 molecule with its bond enormously compressed relative to its normal value. This means that there is an enormous repulsive force pushing the two atoms away from each other. This matters because our optimization methods involve estimating how rapidly the total energy changes based on derivatives of the energy evaluated at a single location. The numerical optimization method recognizes that the two atoms want to move away from each other and obliges by taking an initial step that separates the two atoms by a large distance.
Unfortunately, the two atoms are now in positions that in some sense have given an even worse approximation to the minimum energy state of the molecule than our initial estimate and the calculation is unable to recover and ﬁnd the true minimum. In short, our calculation has failed miserably because we

3.5 GEOMETRY OPTIMIZATION 77
used an initial geometry that was chemically implausible. The details of how this type of failure will unfold are dependent on the details of the optimization algorithm, but this is something that can and will happen with almost any optimization method if you use poor initial estimates for the geometries of the atoms in which you are interested. The critical lesson here is that expending effort to create good initial estimates for atomic coordinates will greatly increase the speed of your DFT calculations and in many cases make a decisive difference in whether your calculations can even converge to an energy minimum.
As a second example of optimizing the positions of atoms within a supercell, we will optimize the geometry of a molecule of CO2. If we again use a cubic supercell with side length L angstroms, we can create a CO2 molecule by placing a C atom at fractional coordinates (0,0,0) and O atoms at (þd/ L,0,0) and (2d/L,0,0). Optimizing the energy of the molecule from this initial state with d 1.3 A˚ and the same stopping criterion that we used for the N2 calculations gives us an optimized C– O bond length of 1.17 A˚ and an O– C –O bond angle of 1808.
Our CO2 results seem quite reasonable, but can we trust this result? Remember that we deﬁned our stopping criterion by the magnitude of the force on the atoms. Let us examine the force on one of the O atoms for the conﬁguration we used in our calculations. If we write this force as f ¼ ( fx, fy, fz), then by symmetry, fy ¼ fz ¼ 0, regardless of what value of d we choose. This means that as the geometry of the molecule is iteratively updated during energy minimization, the C –O bond lengths will be varied but the O –C – O bond angle will remain ﬁxed at 180o, the value we deﬁned in our initial conﬁguration. Saying this another way, the bond angle in our calculation is not really a converged result, it is an inevitable result of the symmetry we imposed in our original estimate for the molecule’s structure.
A more reliable approach to optimizing the geometry of this molecule is to choose an initial bond angle that does not make components of the forces vanish by symmetry alone. We can easily do this by starting from a conﬁguration with a C atom at fractional coordinates (0,0,0) and O atoms at (þa/L, b/L,0) and (2a/L, b/L,0). Minimizing the energy of the molecule starting from this structure with a 1.2 A˚ and b 0.1 A˚ , a conﬁguration with an O– C –O bond angle of 107.58, gives us a converged result with C –O bond lengths of 1.17 A˚ and a O –C –O bond angle of 179.828. This geometry is extremely close to the one we found starting from a linear molecule. We should not expect the two geometries to be exactly the same; they were determined using iterative optimization methods that were halted once we were “sufﬁciently close” to the exact solution as dictated by the stopping criterion we used. It is quite reasonable to draw the conclusion from these calculations that our DFT calculations have predicted that CO2 is a linear molecule (i.e., it

78 NUTS AND BOLTS OF DFT CALCULATIONS
has an O – C– O bond angle of 1808). Experimentally, CO2 is known to be a linear molecule with C –O bond lengths of 1.20 A˚ , so the DFT prediction is in good accord with experiment.
The ability to efﬁciently minimize the total energy of a collection of atoms is central to perhaps the majority of all DFT calculations. Before moving ahead, you should reread this section with the aim of summarizing the pitfalls we have identiﬁed in the two simple examples we examined. Developing a strong physical intuition about why these pitfalls exist and how they can be detected or avoided will save you enormous amounts of effort in your future calculations.
3.5.2 Geometry Optimization with Constrained Atoms
There are many types of calculations where it is useful to minimize the energy of a supercell by optimizing the position of some atoms while holding other atoms at ﬁxed positions. Some speciﬁc examples of this situation are given in the next chapter when we look at calculations involving solid surfaces. Optimization problems involving constraints are in general much more numerically challenging than unconstrained optimization problems. Fortunately, performing a force-based geometry optimization such as the method outlined above is easily extended to situations where one or more of the atoms in a supercell is constrained. For the constrained system, only the positions of the unconstrained atoms are updated during each iteration of the optimization calculation. If the stopping criterion in a calculation of this kind is based on the magnitudes of forces on each atom, only the unconstrained atoms are included in deﬁning the stopping criterion.
3.5.3 Optimizing Supercell Volume and Shape
The calculations above allowed the positions of atoms to change within a supercell while holding the size and shape of the supercell constant. But in the calculations we introduced in Chapter 2, we varied the size of the supercell to determine the lattice constant of several bulk solids. Hopefully you can see that the numerical optimization methods that allow us to optimize atomic positions can also be extended to optimize the size of a supercell. We will not delve into the details of these calculations—you should read the documentation of the DFT package you are using to ﬁnd out how to use your package to do these types of calculations accurately. Instead, we will give an example. In Chapter 2 we attempted to ﬁnd the lattice constant of Cu in the hcp crystal structure by doing individual calculations for many different values of the lattice parameters a and c (you should look back at Fig. 2.4). A much easier way to tackle this task is to create an initial supercell of hcp Cu with plausible values of a and c and to optimize the supercell volume and shape to minimize

EXERCISES 79
the total energy.Ã Performing this calculation using the same number of k points and so on as we used for hcp Cu in Chapter 2 gives an optimized structure with a 2.58 A˚ and c/a 1.629. These values are entirely consistent with our conclusions in Chapter 2, but they are considerably more precise, and we found them with much less work than was required to generate the data shown in Chapter 2.
EXERCISES
1. In the exercises for Chapter 2 we suggested calculations for several materials, including Pt in the cubic and fcc crystal structures and ScAl in the CsCl structure. Repeat these calculations, this time developing numerical evidence that your results are well converged in terms of sampling k space and energy cutoff.
2. We showed how to ﬁnd a minimum of f (x) ¼ e x cos x using the bisection method and Newton’s method. Apply both of these methods to ﬁnd the same minimum as was discussed above but using different initial estimates for the solution. How does this change the convergence properties illustrated in Fig. 3.6? This function has multiple minima. Use Newton’s method to ﬁnd at least two more of them.
3. Newton’s method is only guaranteed to converge for initial estimates that are sufﬁciently close to a solution. To see this for yourself, try applying Newton’s method to ﬁnd values of x for which g(x) ¼ tan 1 x ¼ 0. In this case, Newton’s method is xiþ1 ¼ xi (1 þ x2i ) tan 1 xi. Explore how well this method converges for initial estimates including x0 ¼ 0:1, x0 ¼ 1, and x0 10.
4. We did not deﬁne a stopping criterion for the multidimensional version of Newton’s method. How would you deﬁne such a criterion?
5. Use methods similar to those in Section 3.5.1 to optimize the geometry of H2O and hydrogen cyanide, HCN. HCN is a highly toxic gas that is nonetheless manufactured in large quantities because of its use as a chemical precursor in a wide range of industrial processes. Ensure that you have
ÃA subtlety in calculations where the volume of the supercell is allowed to change is that forces due to changes in supercell shape and volume can include systematic numerical errors unless the number of k points and energy cutoff are large. The artiﬁcial stress due to these effects is known as the Pulay stress (see Further Reading). One common approach to reducing this effect is to increase the energy cutoff by 30 50% during optimization of the supercell volume. If this is done, the total energy of the ﬁnal optimized structure must be recalculated using the standard energy cutoff to complete the calculation.

80 NUTS AND BOLTS OF DFT CALCULATIONS
sampled all relevant degrees of freedom by showing that multiple initial geometries for each molecule converge to the same ﬁnal state. Compare your calculated geometries with experimental data.
6. In the exercises for Chapter 2, we suggested you compute the lattice constants, a and c, for hexagonal Hf. Repeat this calculation using an approach that optimizes the supercell volume and shape within your calculation. Is your result consistent with the result obtained more laboriously in Chapter 2? How large is the distortion of c/a away from the ideal spherical packing value?
7. Perform the calculations necessary to estimate the energy difference associated with forming an ordered bcc CuPd alloy from fcc Pd and fcc Cu. The ordered alloy is formed by deﬁning a bcc crystal with Cu atoms at the corners of each cube and Pd atoms in the center of each cube (or vice versa). This ordered alloy is known to be the favored low temperature crystal structure of Pd and Cu when they are mixed with this stoichiometry. What does this observation tell you about the sign of the energy difference you are attempting to calculate? To calculate this energy difference you will need to optimize the lattice constant for each material and pay careful attention to how your energy cutoffs and k points are chosen.
REFERENCES
1. G. Kresse and D. Joubert, From Ultrasoft Pseudopotentials to the Projector Augmented-Wave Method, Phys. Rev. B 59 (1999), 1758.
2. S. R. Covey, The Seven Habits of Highly Effective People, Simon & Schuster, New York, 1989.
FURTHER READING
The concepts of reciprocal space, the Brillouin zone, and the like are staples in essentially all solid-state physics textbooks. The Further Reading sections in Chapters 1 and 2 list examples. Another source that gives a very clear introduction to the concepts of energy levels, energy bands, k space, and band structure is R. Hoffmann, C. Janiak, and C. Kollmar, Macromolecules, 24 (1991), 3725.
An excellent resource for learning about efﬁcient numerical methods for optimization (and many other problems) is W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, Numerical Recipes in Cþþ: The Art of Scientiﬁc Computing, Cambridge University Press, Cambridge, UK, 2002. Multiple editions of this book are available with equivalent information in other computing languages.

APPENDIX CALCULATION DETAILS 81
A useful source for an in-depth discussion of the conjugate-gradient method is J. R. Shewchuk, An Introduction to the Conjugate Gradient Method Without the Agonizing Pain (http://www.cs.cmu.edu/$quake-papers/painless-conjugategradient.pdf ).
The smearing method of Methfessel and Paxton is described in M. Methfessel and A. T. Paxton, Phys. Rev. B 40 (1989), 3616.
For details of tetrahedron methods, see P. E. Blo¨chl, O. Jepsen, and O. K. Andersen, Phys. Rev. B 49 (1994), 16223.
For more information on the history and details of pseudopotentials and the PAW method, see G. Kresse and D. Joubert, Phys. Rev. B 59 (1999), 1758, and the references therein.
For more information on Pulay stress and related complications associated with ﬁnite sets of plane waves and k points when calculating forces in supercells with varying volumes, see G. P. Francis and M. C. Payne, J. Phys. Condens. Matter 2 (1990), 4395.
APPENDIX CALCULATION DETAILS
All calculations in this chapter were performed using the PW91 GGA functional.
Sections 3.1.3 and 3.1.2 Bulk Cu calculations used a cubic supercell with 4 Cu atoms in the fcc structure and a cutoff energy of 292 eV. Methfessel –Paxton smearing with a width of 0.1 eV was used.
Section 3.5.1 Calculations for molecular N2 and CO2 used a cubic supercell of side length 10 A˚ , with reciprocal space sampled using 3Â3Â3 k points placed with the Monkhorst –Pack method. The energy cutoff for these calculations was 358 eV.

4
DFT CALCULATIONS FOR SURFACES OF SOLIDS
4.1 IMPORTANCE OF SURFACES
Surfaces are technologically important in many ﬁelds, including catalysis, interfaces, membranes for gas separations, and semiconductor fabrication. Understanding the geometry and electronic structure of surfaces is important; for example, it has been established that there is often a correlation between the structure of a surface and its catalytic activity. One area of research in catalysis where DFT has played an important role is the effort to improve the technology and reduce the cost of the three-way catalysts that reduce CO, NOx, and SOx emissions from cars. These catalysts achieve the rather ambitious goal of oxidizing hydrocarbons and CO while simultaneously reducing NOx. Traditionally, they have incorporated expensive materials such as platinum, and they have been subject to poisoning under certain conditions. DFT methods have played an important role in elucidating the mechanisms of the relevant reactions on catalytic metals, zeolites, and oxides and have led to improvements in the design of catalytic converters that improve their efﬁciency and lower their cost.
Surface science experiments and DFT have often been teammates in very successful projects. DFT has been used along with ultra-high-vacuum surface science experiments such as scanning tunneling microscopy (STM), temperatureprogrammed desorption, X-ray diffraction, and X-ray photoelectron spectroscopy
Density Functional Theory: A Practical Introduction. By David S. Sholl and Janice A. Steckel Copyright # 2009 John Wiley & Sons, Inc.
83

84 DFT CALCULATIONS FOR SURFACES OF SOLIDS
to determine the surface structure of metals, metal oxides, nanoparticles, carbides, and sulﬁdes. For example, in 1959 it was established by electron diffraction experiments that the Si(111) surface has a complex symmetry and that the arrangement of the atoms forming this surface must be very different from the bulk crystal structure of Si. Because of the technological importance of Si in microelectronics, there was great interest in understanding the details of this surface structure. It was not until 1992 that DFT calculations on a very large supercell allowed tests of the validity of what has since become accepted as the Si(111)-(7 Â 7) surface reconstruction. These calculations not only deﬁned the position of each atom in the surface, but they could be used to simulate how the surface would be imaged using STM, aiding in interpreting the beautiful but complex images seen experimentally.
Another technologically relevant material where DFT results have been coupled with a variety of experimental methods to produce a detailed understanding of chemistry on surfaces is titanium dioxide, TiO2. Titanium dioxide is an important material used in pigments, oxygen sensors, antimicrobials and as a support for metal catalysts. Titanium dioxide also acts as a photosensitizer for photovoltaic cells and may be used as an electrode coating in photoelectrolysis cells, enhancing the efﬁciency of electrolytic splitting of water. DFT studies have been coupled with a wide variety of experimental techniques in order to characterize the binding of various atomic and molecular species with the surface as well as characteristics of the surface itself. The sites on the surface where oxygen atoms are missing are very important to a number of processes, and DFT calculations have not only helped to show that this is the case but to explain the physics that underlie this phenomenon.
In this chapter, we look at how DFT calculations can be used to examine surfaces of solids. After introducing the ideas necessary to deﬁne the atomic structure of bare surfaces, we give several examples of calculations relevant to gas –surface interfaces.
4.2 PERIODIC BOUNDARY CONDITIONS AND SLAB MODELS
If our goal is to study a surface, our ideal model would be a slice of material that is inﬁnite in two dimensions, but ﬁnite along the surface normal. In order to accomplish this, it may seem natural to take advantage of periodic boundary conditions in two dimensions, but not the third. There are codes in which this technique is implemented, but it is more common to study a surface using a code that applies periodic boundary conditions in all three dimensions, and it is this approach we will discuss. The basic idea is illustrated in Fig. 4.1, where the supercell contains atoms along only a fraction of the vertical

