COMPUTER SIMULATION OF LIQUIDS

Computer Simulation of Liquids
Second Edition
Michael P. Allen
Department of Physics, University of Warwick, UK H. H. Wills Physics Laboratory, University of Bristol, UK
Dominic J. Tildesley
Centre Européen de Calcul Atomique et Moléculaire (CECAM), EPFL, Switzerland
3

3
Great Clarendon Street, Oxford, OX2 6DP, United Kingdom
Oxford University Press is a department of the University of Oxford. It furthers the University’s objective of excellence in research, scholarship, and education by publishing worldwide. Oxford is a registered trade mark of
Oxford University Press in the UK and in certain other countries
© M. P. Allen and D. J. Tildesley 2017
The moral rights of the authors have been asserted
First Edition published in 1987 Second Edition published in 2017
Impression: 1
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, without the prior permission in writing of Oxford University Press, or as expressly permitted by law, by licence or under terms agreed with the appropriate reprographics rights organization. Enquiries concerning reproduction outside the scope of the above should be sent to the Rights Department, Oxford University Press, at the
address above
You must not circulate this work in any other form and you must impose this same condition on any acquirer
Published in the United States of America by Oxford University Press 198 Madison Avenue, New York, NY 10016, United States of America
British Library Cataloguing in Publication Data Data available
Library of Congress Control Number: 2017936745
ISBN 978–0–19–880319–5 (hbk.) ISBN 978–0–19–880320–1 (pbk.)
Printed and bound by CPI Group (UK) Ltd, Croydon, CR0 4YY
Links to third party websites are provided by Oxford in good faith and for information only. Oxford disclaims any responsibility for the materials
contained in any third party website referenced in this work.

To Diane, Eleanor, Pauline, and Charles

Preface
In the years following the publication of the rst edition, we have frequently discussed producing an updated version, and indeed have been nagged on many occasions by colleagues to do so. Despite its increasingly dated content, with quaint references to micro che, magnetic tapes, and Fortran-77 language examples, the rst edition has continued to sell well for three decades. is is quite surprising, bearing in mind the tremendous development of the eld and the computer technologies on which it is based. To an extent, the material in our book has been complemented by the publication of other books and online resources which help to understand the underlying principles. Also, it is much easier than it used to be to nd technical details in the primary literature, in papers, appendices, and supplementary information. New and improved techniques appear all the time, and the problem is almost that there is too much information, and too much rediscovery of existing methods. e widespread use of simulation packages has provided enormous leverage in this research eld. ere is much to gain by carefully reading the manual for your chosen package, and we strongly recommend it!
Nonetheless, it remains true that ‘ge ing started’ can be a signi cant barrier, and there is always the need to understand properly what is going on ‘under the hood’, so as not to use a packaged technique beyond its range of validity. Many colleagues have rea rmed to us that there is still a need for a general guide book, concentrating on the strengths of the rst edition: providing practical advice and examples rather than too much theory. So, we agreed that an updated version of our book would be of value. We intended to produce this many years ago, and it is a sad fact that the demands of academia and industry le too li le time to make good on these aspirations. We wish to acknowledge the patience of our editor at Oxford University Press, So¨nke Adlung, who has stuck with us over this long period.
Although the eld has grown enormously, we resisted the temptation to change the title of the book. It was always focused on the liquid state, and this encompasses what are now known as complex uids, such as liquid crystals, polymers, some colloidal suspensions, gels, so ma er in general, some biological systems such as uid membranes, and glasses. e techniques will also be of interest outside the aforementioned elds, and there is no well-de ned dividing line, but we try not to stray too far outside our expertise. Rather than give a long list in the title, we hope that ‘Computer Simulation of Liquids’, interpreted with some latitude, is still su ciently descriptive.
e content of the book, although structured in the same way as the rst edition, has changed to re ect the above expansion in the eld, as well as technical advances.
e rst few chapters cover basic material. Molecular dynamics in various ensembles is now regarded as basic, rather than advanced, and we devote whole chapters to the handling of long-range forces and simulating on parallel computers, both of which are now mainstream topics. ere are a few more chapters covering advanced simulation

viii Preface
methods, especially those for studying rare events, mesoscale simulations (including coarse graining), and the study of inhomogeneous systems. Instead of concentrating some scienti c examples in a single chapter, we have sca ered them through the text, to illustrate still further what can be done with the techniques we describe. ese examples very much re ect our personal preferences, and we have tried to resist the temptation to turn our book into a collection of scienti c or technical reviews, so many otherwise suitable ‘highlights’ have been omi ed. To give a balanced overview of such a huge eld would probably be impossible and would certainly have resulted in a very di erent, and much larger, book. We have dropped material, when methods have been superceded (such as predictor–corrector algorithms), or when they were really of limited or specialized interest (such as the use of integral equations to extend correlation functions to longer distance).
e examples of program code which accompanied the rst edition were rst provided on micro che, and later online, courtesy of Cornell University and . We continue to use such code examples to illustrate ideas in the text, and provide them online. We give the individual lenames, the rst few lines of each example, and some guidance on usage, in the book. e full set of codes is available online at

http://www.oup.co.uk/companion/a en ti des ey

Although we stick to Fortran in the main, some online les are also provided in

Python, to widen the accessibility. Some relevant programming considerations may be

found in Appendix A.

We wish to reiterate our thanks to those who supported us at the start of our careers

(see below) and we have many more people to thank now. J. Anwar, P. Carbone, J. H. Hard-

ing, P. A. Madden, S. C. Parker, M. Parrinello, D. igley, P. M. Rodger, M. B. Sweatman,

A. Troisi, and M. R. Wilson all provided advice and/or encouragement during the early

stages of writing. S. Bonella, P. J. Daivis, S. Khalid, P. Malfreyt, B. D. Todd, and R. Vuilleu-

mier advised us on speci c topics. G. Cicco i, A. Humpert, and G. Jackson read and

commented on a complete rst dra . Any mistakes or misconceptions, naturally, remain

our own responsibilities. Our colleagues over the years, at Bristol, Warwick, Southamp-

ton, Imperial College London, Unilever plc, and

Lausanne, have also provided a

stimulating working environment and a challenging intellectual atmosphere. MPA also

wishes to acknowledge helpful study leave periods spent in Germany, at the Universities

of Mainz and Bielefeld, and in Australia, at the Universities of Swinburne, Monash, and

Deakin. DJT acknowledges an important and stimulating collaboration with the chemistry

department at the Universite´ Blaise Pascal, Clermont-Ferrand, France.

Our families have remained an ongoing source of support and inspiration. DJT thanks

Eleanor for her unwavering encouragement, while MPA particularly wishes to thank

Pauline and Charles, whose holidays frequently had to coincide with conferences and

summer schools over the years!

Bristol

MPA

Lausanne

DJT

August 2016

From the Preface to the First Edition ix

From the Preface to the First Edition

is is a ‘how-to-do-it’ book for people who want to use computers to simulate the

behaviour of atomic and molecular liquids. We hope that it will be useful to rst-year

graduate students, research workers in industry and academia, and to teachers and

lecturers who want to use the computer to illustrate the way liquids behave.

Ge ing started is the main barrier to writing a simulation program. Few people begin

their research into liquids by si ing down and composing a program from scratch. Yet

these programs are not inherently complicated: there are just a few pitfalls to be avoided.

In the past, many simulation programs have been handed down from one research group

to another and from one generation of students to the next. Indeed, with a trained eye, it is

possible to trace many programs back to one of the handful of groups working in the eld

20 years ago. Technical details such as methods for improving the speed of the progam

or for avoiding common mistakes are o en buried in the appendices of publications

or passed on by word of mouth. In the rst six chapters of this book, we have tried to

gather together these details and to present a clear account of the techniques, namely

Monte Carlo and molecular dynamics. e hope is that a graduate student could use these

chapters to write his own program.

Both of us were fortunate in that we had expert guidance when starting work in the

eld, and we would like to take this opportunity to thank P. Scho eld (Harwell) and

W. B. Stree (Cornell), who set us on the right road some years ago. is book was largely

wri en and created at the Physical Chemistry Laboratory, Oxford, where both of us have

spent a large part of our research careers. We owe a great debt of gratitude to the head of

department, J. S. Rowlinson, who has provided us with continuous encouragement and

support in this venture, as well as a meticulous criticism of early versions of the manuscript.

We would also like to thank our friends and colleagues in the physics department at Bristol

and the chemistry department at Southampton for their help and encouragement, and

we are indebted to many colleagues, who in discussions at conferences and workshops,

particularly those organized by and

, have helped to form our ideas. We cannot

mention all by name but should say that conversations with D. Frenkel and P. A. Madden

have been especially helpful. We would also like to thank M. Gillan and J. P. Ryckaert, who

made useful comments on certain chapters, and I. R. McDonald who read and commented

on the completed manuscript.

Books are not wri en without a lot of family support. One of us (DJT) wants to thank

the Oaks and the Sibleys of Bicester for their hospitality during many weekends over

the last three years. Our wives, Diane and Pauline, have su ered in silence during our

frequent disappearances, and given us their un agging support during the whole project.

We owe them a great deal.

Bristol

MPA

Southampton

DJT

May 1986

Contents

1 Introduction

1

1.1 A short history of computer simulation

1

1.2 Computer simulation: motivation and applications

4

1.3 Model systems and interaction potentials

5

1.4 Constructing an intermolecular potential from rst principles

25

1.5 Force elds

29

1.6 Studying small systems

35

2 Statistical mechanics

46

2.1 Sampling from ensembles

46

2.2 Common statistical ensembles

52

2.3 Transforming between ensembles

58

2.4 Simple thermodynamic averages

60

2.5 Fluctuations

66

2.6 Structural quantities

69

2.7 Time correlation functions and transport coe cients

73

2.8 Long-range corrections

79

2.9 antum corrections

81

2.10 Constraints

83

2.11 Landau free energy

85

2.12 Inhomogeneous systems

86

2.13 Fluid membranes

90

2.14 Liquid crystals

92

3 Molecular dynamics

95

3.1 Equations of motion for atomic systems

95

3.2 Finite-di erence methods

97

3.3 Molecular dynamics of rigid non-spherical bodies

106

3.4 Constraint dynamics

113

3.5 Multiple-timestep algorithms

120

3.6 Checks on accuracy

121

3.7 Molecular dynamics of hard particles

125

3.8 Constant-temperature molecular dynamics

130

3.9 Constant-pressure molecular dynamics

140

3.10 Grand canonical molecular dynamics

144

3.11 Molecular dynamics of polarizable systems

145

4 Monte Carlo methods

147

4.1 Introduction

147

4.2 Monte Carlo integration

147

xii Contents

4.3 Importance sampling

151

4.4 e Metropolis method

155

4.5 Isothermal–isobaric Monte Carlo

160

4.6 Grand canonical Monte Carlo

164

4.7 Semi-grand Monte Carlo

168

4.8 Molecular liquids

169

4.9 Parallel tempering

177

4.10 Other ensembles

183

5 Some tricks of the trade

185

5.1 Introduction

185

5.2 e heart of the ma er

185

5.3 Neighbour lists

193

5.4 Non-bonded interactions and multiple timesteps

200

5.5 When the dust has se led

201

5.6 Starting up

204

5.7 Organization of the simulation

210

5.8 Checks on self-consistency

214

6 Long-range forces

216

6.1 Introduction

216

6.2 e Ewald sum

217

6.3 e particle–particle particle–mesh method

224

6.4 Spherical truncation

231

6.5 Reaction eld

235

6.6 Fast multipole methods

239

6.7 e multilevel summation method

243

6.8 Maxwell equation molecular dynamics

247

6.9 Long-range potentials in slab geometry

250

6.10 Which scheme to use?

254

7 Parallel simulation

258

7.1 Introduction

258

7.2 Parallel loops

260

7.3 Parallel replica exchange

262

7.4 Parallel domain decomposition

265

7.5 Parallel constraints

269

8 How to analyse the results

271

8.1 Introduction

271

8.2 Liquid structure

272

8.3 Time correlation functions

274

8.4 Estimating errors

281

8.5 Correcting the results

289

9 Advanced Monte Carlo methods

297

9.1 Introduction

297

9.2 Estimation of the free energy

298

Contents xiii

9.3 Smarter Monte Carlo

317

9.4 Simulation of phase equilibria

333

9.5 Reactive Monte Carlo

338

10 Rare event simulation

342

10.1 Introduction

342

10.2 Transition state approximation

343

10.3 Benne –Chandler approach

345

10.4 Identifying reaction coordinates and paths

346

10.5 Transition path sampling

347

10.6 Forward ux and transition interface sampling

350

10.7 Conclusions

354

11 Nonequilibrium molecular dynamics

355

11.1 Introduction

355

11.2 Spatially oscillating perturbations

357

11.3 Spatially homogeneous perturbations

361

11.4 Inhomogeneous systems

370

11.5 Flow in con ned geometry

371

11.6 Nonequilibrium free-energy measurements

376

11.7 Practical points

379

11.8 Conclusions

381

12 Mesoscale methods

382

12.1 Introduction

382

12.2 Langevin and Brownian dynamics

383

12.3 Brownian dynamics, molecular dynamics, and Monte Carlo

387

12.4 Dissipative particle dynamics

390

12.5 Multiparticle collision dynamics

392

12.6 e la ice-Boltzmann method

394

12.7 Developing coarse-grained potentials

397

13 antum simulations

406

13.1 Introduction

406

13.2 Ab-initio molecular dynamics

408

13.3 Combining quantum and classical force- eld simulations

420

13.4 Path-integral simulations

426

13.5 antum random walk simulations

437

13.6 Over our horizon

442

14 Inhomogeneous uids

446

14.1 e planar gas–liquid interface

446

14.2 e gas–liquid interface of a molecular uid

462

14.3 e liquid–liquid interface

464

14.4 e solid–liquid interface

464

14.5 e liquid drop

469

14.6 Fluid membranes

475

14.7 Liquid crystals

479

xiv Contents

Appendix A Computers and computer simulation

481

A.1 Computer hardware

481

A.2 Programming languages

482

A.3 Fortran programming considerations

483

Appendix B Reduced units

487

B.1 Reduced units

487

Appendix C Calculation of forces and torques

491

C.1 Introduction

491

C.2 e polymer chain

491

C.3 e molecular uid with multipoles

494

C.4 e triple-dipole potential

496

C.5 Charged particles using Ewald sum

497

C.6 e Gay–Berne potential

498

C.7 Numerically testing forces and torques

500

Appendix D Fourier transforms and series

501

D.1 e Fourier transform

501

D.2 Spatial Fourier transforms and series

502

D.3 e discrete Fourier transform

504

D.4 Numerical Fourier transforms

505

Appendix E Random numbers

509

E.1 Random number generators

509

E.2 Uniformly distributed random numbers

510

E.3 Generating non-uniform distributions

511

E.4 Random vectors on the surface of a sphere

514

E.5 Choosing randomly and uniformly from complicated regions

515

E.6 Generating a random permutation

516

Appendix F Con gurational temperature

517

F.1 Expression for con gurational temperature

517

F.2 Implementation details

517

List of Acronyms

521

List of Greek Symbols

527

List of Roman Symbols

529

List of Examples

533

List of Codes

534

Bibliography

536

Index

622

1
Introduction
1.1 A short history of computer simulation
What is a liquid? As you read this book, you may be mixing up, drinking down, sailing on, or swimming in, a liquid. Liquids ow, although they may be very viscous. ey may be transparent or they may sca er light strongly. Liquids may be found in bulk, or in the form of tiny droplets. ey may be vaporized or frozen. Life as we know it probably evolved in the liquid phase, and our bodies are kept alive by chemical reactions occurring in liquids.
ere are many fascinating details of liquid-like behaviour, covering thermodynamics, structure, and motion. Why do liquids behave like this?
e study of the liquid state of ma er has a long and rich history, from both the theoretical and experimental standpoints. From early observations of Brownian motion to recent neutron-sca ering experiments, experimentalists have worked to improve the understanding of the structure and particle dynamics that characterize liquids. At the same time, theoreticians have tried to construct simple models which explain how liquids behave. In this book, we concentrate exclusively on atomic and molecular models of liquids, and their analysis by computer simulation. For excellent accounts of the current status of liquid science, the reader should consult the standard references (Barker and Henderson, ; Rowlinson and Widom, ; Barrat and Hansen, ; Hansen and McDonald, ).
Early models of liquids (Morrell and Hildebrand, ) involved the physical manipulation and analysis of the packing of a large number of gelatine balls, representing the molecules; this resulted in a surprisingly good three-dimensional picture of the structure of a liquid, or perhaps a random glass, and later applications of the technique have been described (Bernal and King, ). Assemblies of metal ball bearings, kept in motion by mechanical vibration (Pieranski et al., ), have been used as models of granular materials and show some analogies with molecular systems (Olafsen and Urbach, ). Clearly, the use of large numbers of macroscopic physical objects to represent molecules can be very time-consuming; there are obvious limitations on the types of interactions between them, and the e ects of gravity are di cult to eliminate. However, modern research on colloidal suspensions, where the typical particle size lies in the range 1 nm–1000 nm, with the ability to manipulate individual particles and study large-scale collective behaviour, has greatly revitalized the eld (Pusey and van Megen, ; Ebert et al., ; Lekkerkerker and Tuinier, ; Bechinger et al., ).
Computer Simulation of Liquids. Second Edition. M. P. Allen and D. J. Tildesley. © M. P. Allen and D. J. Tildesley 2017. Published in 2017 by Oxford University Press.

2 Introduction

e natural extension of this approach is to use a mathematical, rather than a physical,

model, and to perform the analysis by computer. It is now over 60 years since the rst

computer simulation of a liquid was carried out at the Los Alamos National Laboratories

in the United States (Metropolis et al., ). e Los Alamos computer, called

,

was at that time one of the most powerful available; it is a measure of the continuing

rapid advance in computer technology that handheld devices of comparable power are

now available to all at modest cost.

Rapid development of computer hardware means that computing power continues to

increase at an astonishing rate. Using modern parallel computer architectures, we can

expect to enjoy exa op computing by (an exa op is 1018 oating-point operations per

second). is is matched by the enormous increases in data storage available to researchers

and the general public. Computer simulations, of the type we describe in this book, are

possible on most machines from laptops to continental supercomputers, and we provide

an overview of some opportunities with respect to architecture and computing languages,

as they relate to the eld, in Appendix A.

e very earliest work (Metropolis et al., ) laid the foundations of modern Monte

Carlo simulation (so-called because of the role that random numbers play in the method).

e precise technique employed in this study is still widely used, and is referred to simply

as ‘Metropolis Monte Carlo’. e original models were highly idealized representations

of molecules, such as hard spheres and disks, but, within a few years, Monte Carlo ( )

simulations were carried out on the Lennard-Jones interaction potential (Wood and Parker,

) (see Section 1.3). is made it possible to compare data obtained from experiments

on, for example, liquid argon, with the computer-generated thermodynamic data derived

from a model.

A di erent technique is required to obtain the dynamic properties of many-particle

systems. Molecular dynamics ( ) is the term used to describe the solution of the

classical equations of motion (Newton’s equations) for a set of molecules. is was rst

accomplished, for a system of hard spheres, by Alder and Wainwright ( ; ). In

this case, the particles move at constant velocity between perfectly elastic collisions, and

it is possible to solve the dynamic problem without making any approximations, within

the limits imposed by machine accuracy. It was several years before a successful a empt

was made to solve the equations of motion for a set of Lennard-Jones particles (Rahman,

). Here, an approximate, step-by-step procedure is needed, since the forces change

continuously as the particles move. Since that time, the properties of the Lennard-Jones

model have been thoroughly investigated (Verlet, ; ; Johnson et al., ).

A er this initial groundwork on atomic systems, computer simulation developed

rapidly. An early a empt to model a diatomic molecular liquid (Harp and Berne, ;

Berne and Harp, ) using molecular dynamics was quickly followed by two ambitious

a empts to model liquid water, rst by (Barker and Wa s, ), and then by

(Rahman and Stillinger, ). Water remains one of the most interesting and di cult

liquids to study by simulation (Morse and Rice, ; McCoustra et al., ; Lynden-Bell,

; Lin et al., ). From early studies of small rigid molecules (Barojas et al., )

and exible hydrocarbons (Ryckaert and Bellemans, ), simulations have developed to

model more complicated systems such as polymers (Binder, ), proteins, lipids, nucleic

acids, and carbohydrates (Monticelli and Salonen, ). Simulations containing half a

A short history of computer simulation 3

106

105

104

Articles

103

102

101

100 1950–59

1960–69

1970–79

1980–89

1990–99

2000–09

Decade

Fig. 1.1 e approximate number of articles concerning the computer simulation of condensed phases published in each complete decade. e search was carried out using the Web of Science® by

searching on Monte Carlo, molecular dynamics, Brownian dynamics, la ice Boltzmann, dynamical

density functional theory, Car–Parrinello, / in both the

and

search elds.

million atoms have been conducted for 50 million timesteps to study the surface tension of a small liquid droplet (van Giessen and Blokhuis, ) and the massive parallel molecular dynamics code, ls1 mardyn, has been used to simulate a trillion Lennard-Jones atoms (Niethammer et al., ). It is now possible to follow the folding of a solvated protein using simulations in the microsecond-to-millisecond range (ca. 109–1012 timesteps) on a special purpose computer (Piana et al., ).
e growth of the eld of computer simulation over the last 60 years, as evidenced by the number of publications in refereed journals, has been dramatic. In Fig. 1.1, we have a empted to calculate the number of papers published in this eld during each complete decade. While bibliometric exercises of this kind will fail to capture some important papers and will o en include some unwanted papers in related disciplines, the overall trend in the number of articles is clear.
is is, in part, due to the continuing and substantial increase in computing power, which follows the celebrated Moore’s law curve over this period (see Appendix A). It is also due to the application of these methods to a wide range of previously intractable problems in the materials and life sciences. However, it is also, in no small part, due to the ingenuity of its practitioners in extending the early methods to areas such as: the calculation of free energies and phase diagrams (Chapter 9); the simulation of rare events (Chapter 10); the development of nonequilibrium methods for calculating transport coe cients (Chapter 11); the development of coarse-grained methods to extend the length and timescales that can be simulated (Chapter 12); and in the extension to include

4 Introduction
quantum mechanical e ects (Chapter 13). is level of activity points to the proposition that computer simulation now sits alongside experiment and theory as a third and equally important tool in modern science. We start by asking: what is a computer simulation? How does it work? What can it tell us?
1.2 Computer simulation: motivation and applications
Some problems in statistical mechanics are exactly soluble. By this, we mean that a complete speci cation of the microscopic properties of a system (such as the Hamiltonian of an idealized model like the perfect gas or the Einstein crystal) leads directly, and perhaps easily, to a set of useful results or macroscopic properties (such as an equation of state like PV = N kBT ). ere are only a handful of non-trivial, exactly soluble problems in statistical mechanics (Baxter, ); the two-dimensional Ising model is a famous example.
Some problems in statistical mechanics, while not being exactly soluble, succumb readily to an analysis based on a straightforward approximation scheme. Computers may have an incidental, calculational, part to play in such work; for example, in the evaluation of cluster integrals in the virial expansion for dilute, imperfect gases (Rosenbluth and Rosenbluth, ; Wheatley, ). e problem is that, like the virial expansion, many ‘straightforward’ approximation schemes simply do not work when applied to liquids. For some liquid properties, it may not even be clear how to begin constructing an approximate theory in a reasonable way. e more di cult and interesting the problem, the more desirable it becomes to have exact results available, both to test existing approximate methods and to point the way towards new approaches. It is also important to be able to do this without necessarily introducing the additional question of how closely a particular model (which may be very idealized) mimics a real liquid, although this may also be a ma er of interest. Computer simulations have a valuable role to play in providing essentially exact results for problems in statistical mechanics which would otherwise only be soluble by approximate methods, or might be quite intractable. In this sense, computer simulation is a test of theories and, historically, simulations have indeed discriminated between well-founded approaches, such as integral equation theories (Hansen and McDonald, ), and ideas that are plausible but, in the event, less successful, such as the old cell theories of liquids (Lennard-Jones and Devonshire, a,b). e results of computer simulations may also be compared with those of real experiments. In the rst place, this is a test of the underlying model used in a computer simulation. Eventually, if the model is a good one, the simulator hopes to o er insights to the experimentalist, and assist in the interpretation of new results. is dual role of simulation, as a bridge between models and theoretical predictions on the one hand, and between models and experimental results on the other, is illustrated in Fig. 1.2. Because of this connection role, and the way in which simulations are conducted and analysed, these techniques are o en termed ‘computer experiments’.
Computer simulation provides a direct route from the microscopic details of a system (the masses of the atoms, the interactions between them, molecular geometry, etc.) to macroscopic properties of experimental interest (the equation of state, transport coe cients, structural order parameters, and so on). As well as being of academic interest, this type of information is technologically useful. It may be di cult or impossible to carry out experiments under extremes of temperature and pressure, while a computer simulation

Real Liquids
Perform Experiments
Experimental Results
Compare

Model systems and interaction potentials 5

Make Models

Model Liquids

Carry Out Computer Simulations
Exact Results for Model

Construct Approximate
eories
eoretical Predictions

Compare

Tests of Models

Tests of eories

Fig. 1.2 e connection between experiment, theory, and computer simulation.
of the material in, say, a shock wave, a high-temperature plasma, a nuclear reactor, or a planetary core, would be perfectly feasible. ite subtle details of molecular motion and structure, for example in heterogeneous catalysis, fast ion conduction, or enzyme action, are di cult to probe experimentally but can be extracted readily from a computer simulation. Finally, while the speed of molecular events is itself an experimental di culty it represents no hindrance to the simulator. A wide range of physical phenomena, from the molecular scale to the galactic (Hockney and Eastwood, ), may be studied using some form of computer simulation.
In most of this book, we will be concerned with the details of carrying out simulations (the central box in Fig. 1.2). In the rest of this chapter, however, we deal with the general question of how to put information in (i.e. how to de ne a model of a liquid) while in Chapter 2 we examine how to get information out (using statistical mechanics).

1.3 Model systems and interaction potentials
1.3.1 Introduction
In most of this book, the microscopic state of a system may be speci ed in terms of the positions and momenta of a constituent set of particles: the atoms and molecules. Within the Born–Oppenheimer ( ) approximation (see also Chapter 13), it is possible to express the Hamiltonian of a system as a function of the nuclear variables, the (rapid) motion of the electrons having been averaged out. Making the additional approximation that a classical description is adequate, we may write the Hamiltonian H of a system of N molecules as a sum of kinetic- and potential-energy functions of the set of coordinates qi

6 Introduction

and momenta pi of each molecule i. Adopting a condensed notation

q = (q1, q2, · · · , qN ) p = (p1, p2, · · · , pN )

we have

H (q, p) = K (p) + V (q).

(1.1a) (1.1b)
(1.2)

Usually, the Hamiltonian will be equal to the total internal energy E of the system. e
generalized coordinates qi may simply be the set of Cartesian coordinates ri of each atom (or nucleus) in the system, but, as we shall see, it is sometimes useful to treat molecules as
rigid bodies, in which case q will consist of the Cartesian coordinates of each molecular
centre of mass together with a set of variables Ωi that specify molecular orientation. In any case, p stands for the appropriate set of conjugate momenta. For a simple atomic
system, the kinetic energy K takes the form

N

K=

pi2α /2mi

i=1 α

(1.3)

where mi is the molecular mass, and the index α runs over the di erent (x, y, z) components of the momentum of atom i. e potential energy V contains the interesting information regarding intermolecular interactions: assuming that V is fairly sensibly behaved, it will be possible to construct, from H , an equation of motion (in Hamiltonian, Lagrangian, or Newtonian form) which governs the entire time-evolution of the system and all its mechanical properties (Goldstein, ). Solution of this equation will generally involve calculating, from V, the forces fi and torques τi acting on the molecules (see Chapter 3). e Hamiltonian also dictates the equilibrium distribution function for molecular positions and momenta (see Chapter 2). us, generally, it is H (or V) which is the basic input to a computer simulation program. e approach used almost universally in computer simulation is to separate the potential energy into terms involving pairs, triplets, etc. of molecules. In the following sections we shall consider this in detail.
Recently, there has been a spectacular growth in the number of simulation studies which avoid the use of e ective potentials by considering the electrons explicitly using density functional theory (Martin, ). In an early approach, the electron density was represented by an extension of the electron gas theory (LeSar and Gordon, ; ; LeSar, ). In most of the current work, the electronic degrees of freedom are explicitly included in the description. e electrons, in uenced by the external eld of the nuclei, are allowed to evolve during the course of the simulation by an auxiliary set of dynamical equations (Car and Parrinello, ). is method, known as ab initio molecular dynamics (Marx and Hu er, ), is now su ciently well developed that it may become the method of choice for simulations in materials and the life sciences as the speed of computers increases. We will consider this approach in more detail in Chapter 13.

Model systems and interaction potentials 7 100

v(r )/kB (K)

0

−100

0.3

0.4

0.5

0.6

0.7

0.8

r (nm)

Fig. 1.3 Argon pair potentials. We illustrate (solid line) a recent pair potential for argon calculated by ab initio methods (see Patkowski and Szalewicz, ). Also shown is the Lennard-Jones 12–6 potential (dashed line) used in computer simulations of liquid argon.
1.3.2 Atomic systems
Consider rst the case of a system containing N atoms. e potential energy may be divided into terms depending on the coordinates of individual atoms, pairs, triplets, etc.:

V = v1 (ri ) +

v2 (ri , rj ) +

v3 (ri , rj , rk ) + . . . .

(1.4)

i

i j >i

i j >i k >j

e i j >i notation indicates a summation over all distinct pairs i and j without counting any pair twice (i.e. as ij and ji); the same care must be taken for triplets. e rst term in eqn (1.4), v1(ri ), represents the e ect of an external eld (including, e.g. the container walls) on the system. e remaining terms represent particle interactions. e second term, v2, the pair potential, is the most important. e pair potential depends only on the magnitude of the pair separation rij = |rij | = |ri − rj |, so it may be wri en v2(rij ). Figure 1.3 shows one of the more recent estimates for the pair potential between two argon atoms, as a function of separation (Patkowski and Szalewicz, ). is potential was determined by ing to very accurate ab initio calculations for the argon dimer. e potential provides a position for the minimum and a well-depth that are very close to the experimental values. It can be used to calculate the spectrum of the isolated argon dimer and it produces a rotational constant and dissociation energy that are in excellent agreement with experiment (Patkowski et al., ). In fact, the computed potential is accurate enough to cast some doubt on the recommended, experimental, values of the second virial coe cient of argon at high temperatures (Dymond and Smith, ).
e potential shows the typical features of intermolecular interactions. ere is an a ractive tail at large separations, essentially due to correlation between the electron clouds surrounding the atoms (‘van der Waals’ or ‘London’ dispersion). In addition, for

8 Introduction

charged species, Coulombic terms would be present. ere is a negative well, responsible for cohesion in condensed phases. Finally, there is a steeply rising repulsive wall at short distances, due to non-bonded overlap between the electron clouds.
e v3 term in eqn (1.4), involving triplets of molecules, is undoubtedly signi cant at liquid densities. Estimates of the magnitudes of the leading, triple-dipole, three-body contribution (Axilrod and Teller, ) have been made for inert gases in their solid-state face centred cubic ( ) la ices (Doran and Zucker, ; Barker and Henderson, ). It is found that up to 10 % of the la ice energy of argon (and more in the case of more polarizable species) may be due to these non-additive terms in the potential; we may expect the same order of magnitude to hold in the liquid phase. Four-body (and higher) terms in eqn (1.4) are expected to be small in comparison with v2 and v3.
Despite the size of three-body terms in the potential, they are only rarely included in computer simulations (Barker et al., ; A ard, ; Marcelli and Sadus, ). is is because, as we shall see shortly, the calculation of any quantity involving a sum over triplets of molecules will be very time-consuming on a computer. In most cases, the pairwise approximation gives a remarkably good description of liquid properties because the average three-body e ects can be partially included by de ning an ‘e ective’ pair potential. To do this, we rewrite eqn (1.4) in the form

V ≈ v1 (ri ) +

v2e (rij ).

(1.5)

i

i j >i

e pair potentials appearing in computer simulations are generally to be regarded as e ective pair potentials of this kind, representing all the many-body e ects; for simplicity, we will just use the notation v(rij ), or v(r ). A consequence of this approximation is that the e ective pair potential needed to reproduce experimental data may turn out to depend on the density, temperature, etc., while the true two-body potential v2(rij ), of course, does not.
Now we turn to the simpler, more idealized, pair potentials commonly used in computer simulations. ese re ect the salient features of real interactions in a general, o en empirical, way. Illustrated, with the accurate argon pair potential, in Fig. 1.3 is a simple Lennard-Jones 12–6 potential

vLJ (r ) = 4ϵ (σ /r )12 − (σ /r )6

(1.6)

which provides a reasonable description of the properties of argon, via computer simulation, if the parameters ϵ and σ are chosen appropriately. e potential has a long-range a ractive tail of the form −1/r 6, a negative well of depth ϵ, and a steeply rising repulsive wall at distances less than r ∼ σ . e well-depth is o en quoted in units of temperature as ϵ/kB, where kB is Boltzmann’s constant; values of ϵ/kB = 120 K and σ = 0.34 nm provide reasonable agreement with the experimental properties of liquid argon. Once again, we must emphasize that these are not the values which would apply to an isolated pair of argon atoms, as is clear from Fig. 1.3.
For the purposes of investigating general properties of liquids, and for comparison with theory, highly idealized pair potentials may be of value. In Fig. 1.4, we illustrate three

(a) v(r ) σ
(c) v(r )

Model systems and interaction potentials 9 (b) v(r )

σ1

σ2

r

r

(d) v(r )

r

r

Fig. 1.4 Idealized pair potentials. (a) e hard-sphere potential; (b) the square-well potential; (c) e so -sphere potential with repulsion parameter ν = 1; (d) e so -sphere potential with repulsion parameter ν = 12. Vertical and horizontal scales are arbitrary.

forms which, although unrealistic, are very simple and convenient to use in computer simulation and in liquid-state theory. ese are: the hard-sphere potential

vHS (r

)

=

∞ 

if r < σ

(1.7)

0 if σ ≤ r ;



the square-well potential

vSW(r ) = ∞−ϵ,

if r < σ1 if σ1 ≤ r < σ2

(1.8)

 0, if σ2 ≤ r ;



and the so -sphere potential

vSS (r ) = ϵ (σ /r )ν = ar −ν ,

(1.9)

where ν is a parameter, o en chosen to be an integer. e so -sphere potential becomes progressively ‘harder’ as ν is increased. So -sphere potentials contain no a ractive part. It is o en useful to divide more realistic potentials into separate a ractive and repulsive

10 Introduction

1.0

vRLJ

0.5

vLJ

v(r )/ε

0.0

−0.5

vALJ

−1.0

0.8

1.2

1.6

2.0

r /σ

Fig. 1.5 e separation of the Lennard-Jones potential vLJ into a ractive and repulsive components, vALJ and vRLJ, respectively. e vertical dashed line shows the position of rmin.
components, and the separation proposed by Weeks et al. ( ) involves spli ing the potential at the minimum. For the Lennard-Jones potential, the repulsive and a ractive parts are, as illustrated in Fig. 1.5,

vRLJ (r

)

=

vLJ (r 

)

+

ϵ

if r < rmin

0

if rmin ≤ r



vALJ (r

)

=

−ϵ

 

vLJ

(r

)



if r < rmin if rmin ≤ r ,

(1.10a) (1.10b)

where rmin = 21/6σ ≈ 1.12σ . In perturbation theory (Weeks et al., ), a hypothetical uid of molecules interacting via the repulsive potential vRLJ is treated as a reference
system and the a ractive part vALJ is the perturbation. It should be noted that the potential vRLJ is signi cantly harder than the inverse twel h power so -sphere potential, which is also sometimes thought of as the ‘repulsive’ part of vLJ(r ).
For ions, of course, these potentials are not su cient to represent the long-range
interactions. A simple approach is to supplement one of these pair potentials with the
Coulomb charge–charge interaction

vqq (rij )

=

qiqj 4πϵ0ri j

(1.11)

where qi , qj are the charges on ions i and j and ϵ0 is the permi ivity of free space (not to be confused with ϵ in eqns (1.6)–(1.10)). For ionic systems, induction interactions are important: the ionic charge induces a dipole on a neighbouring ion. is term is not pairwise additive and hence is di cult to include in a simulation. e shell model is a crude a empt to account for this polarizability (Dixon and Sangster, ; Lindan, ).

Model systems and interaction potentials 11

Each ion is represented as a core surrounded by a shell. Part of the ionic charge is located on the shell and the rest in the core. is division is always arranged so that the shell charge is negative (it represents the electronic cloud). e interactions between ions are just sums of the Coulombic shell–shell, core–core, and shell–core contributions. e shell and core of a given ion are coupled by a harmonic spring potential. e shells are taken to have zero mass. During a simulation, their positions are adjusted iteratively to zero the net force acting on each shell: this process makes the simulations expensive. We shall return to the simulation of polarizable systems in Section 1.3.3.
When a potential depends upon just a few parameters, such as ϵ and σ , it may be possible to choose an appropriate set of units in which these parameters take values of unity. is results in a simpler description of the properties of the model, and there may also be technical advantages within a simulation program. For Coulomb systems, the factor 4πϵ0 in eqn (1.11) is o en omi ed, and this corresponds to choosing a nonstandard unit of charge. We discuss such reduced units in Appendix B. Reduced densities, temperatures, etc. are o en denoted by an asterisk, that is, ρ∗,T ∗ etc.

1.3.3 Molecular systems
In principle, there is no reason to abandon the atomic approach when dealing with molecular systems: chemical bonds are simply interatomic potential-energy terms (Chandler,
). Ideally, we would like to treat all aspects of chemical bonding, including the reactions which form and break bonds, in a proper quantum mechanical fashion. is di cult task has not yet been accomplished but there are two common simplifying approaches. We might treat the bonds as classical harmonic springs (or Morse oscillators) or we could treat the molecule as a rigid or semi-rigid unit, with xed bond lengths and, sometimes,
xed bond angles and torsion angles. Bond vibrations are of very high frequency (and hence di cult to handle, certainly
in a classical simulation). It quite possible that a high-frequency vibration will not be in thermal equilibrium with the uid that surrounds it. ese vibrations are also of low amplitude (and are therefore unimportant for many liquid properties). For these reasons, we prefer the approach of constraining the bond lengths to their equilibrium values. us, a diatomic molecule with a strongly binding interatomic potential-energy surface might be replaced by a dumb-bell with a rigid interatomic bond.
e interaction between the nuclei and electronic charge clouds of a pair of molecules i and j is clearly a complicated function of relative positions ri , rj and orientations Ωi , Ωj (Gray and Gubbins, ). One way of modelling a molecule is to concentrate on the positions and sizes of the constituent atoms (Eyring, ). e much simpli ed ‘atom– atom’ or ‘site–site’ approximation for diatomic molecules is illustrated in Fig. 1.6. e total interaction is a sum of pairwise contributions from distinct sites a in molecule i, at position ria, and b in molecule j, at position rjb :

v(rij , Ωi , Ωj ) =

vab (rab ).

ab

(1.12)

Here a, b take the values 1, 2, vab is the pair potential acting between sites a and b, and rab is shorthand for the inter-site separation rab = |rab | = |ria − rjb |. e interaction sites are usually centred, more or less, on the positions of the nuclei in the real molecule, so as to

12 Introduction

i a=1

a=2

j b=2
b=1

Fig. 1.6 e atom–atom model of a diatomic molecule. e total interaction is a sum of terms involving the distances |ria − rjb |, indicated by dashed lines.

represent the basic e ects of molecular ‘shape’. A very simple extension of the hard-sphere

model is to consider a diatomic composed of two hard spheres fused together (Stree and

Tildesley, ), but more realistic models involve continuous potentials. us, nitrogen,

uorine, chlorine, etc. have been depicted as two ‘Lennard-Jones atoms’ separated by a

xed bond length (Barojas et al., ; Cheung and Powles, ; Singer et al., ).

e description of the molecular charge distribution may be improved somewhat by

incorporating point multipole moments at the centre of charge (Stree and Tildesley,

). ese multipoles may be equal to the known (isolated molecule) values, or may

be ‘e ective’ values chosen simply to yield a be er description of the liquid structure

and thermodynamic properties. A useful collection of the values of multipole moments

is given in Gray and Gubbins ( ). Price et al. ( ) have developed an e cient way

of calculating the multipolar energy, forces and torques between molecules of arbitrary

symmetry up to terms of O(ri−j5). However, it is now generally accepted that such a multipole expansion of the electrostatic potential based around the centre of mass of a

molecule is not rapidly convergent.

A pragmatic alternative approach, for ionic and polar systems, is to use a set of

ctitious ‘partial charges’ distributed ‘in a physically reasonable way’ around the molecule

so as to reproduce the known multipole moments (Murthy et al., ). For example, the

electrostatic part of the interaction between nitrogen molecules may be modelled using

ve partial charges placed along the axis, while for methane, a tetrahedral arrangement

of partial charges is appropriate. ese are illustrated in Fig. 1.7. For the case of N2, taking the molecular axis to lie along z, the quadrupole moment Q is given by (Gray and Gubbins,

)

5
Q = qaza2

(1.13)

a=1

(a) qq

Model systems and interaction potentials 13

−2(q + q )

qq

0.0549 nm 0.0653 nm

(b) q q

−4q

q

q

Fig. 1.7 Partial charge models: (a) A ve-charge model for N2. ere is one charge at the bond centre, two at the positions of the nuclei, and two more displaced beyond the nuclei. Typical values (with e = 1.602 × 10−19 C): q = +5.2366 e, q = −4.0469 e, giving Q = −4.67 × 10−40 C m2 (Murthy et al., ). (b) A ve-charge model for CH4. ere is one charge at the centre and four others at the positions of the hydrogen nuclei. Typical values are CH bond length 0.1094 nm, q = 0.143 e giving O = 5.77 × 10−50 C m3 (Righini et al., ).
with similar expressions for the higher multipoles (all the odd ones vanish for N2). e rst non-vanishing moment for methane is the octopole O

O

=

5 2

5
qaxayaza
a=1

(1.14)

in a coordinate system aligned with the cube shown in Fig. 1.7. e aim of all these

approaches is to approximate the complete charge distribution in the molecule. In a

calculation of the potential energy, the interaction between partial charges on di erent

molecules would be summed in the same way as the other site–site interactions.

e use of higher-order multipoles has enjoyed a renaissance in recent years. is is

because we can obtain an accurate representation of the electrostatic potential by placing

multipoles at various sites within the molecule. ese sites could be at the atom positions,

or at the centres of bonds or within lone pairs, and it is normally su cient to place a

charge, dipole and quadrupole at any particular site. is approach, known as a distributed

multipole analysis (Stone, ; , Chapter ), is illustrated for N2 and CO in Fig. 1.8. In the case of N2 the multipoles are placed at the centre of the bond and on the two nitrogen atoms, with their z-axis along the bond. Each site has a charge and a quadrupole and, in

addition, the two atoms have equal and opposite dipoles. ese are calculated using an

accurate density functional theory

(Martin, ). In atomic units (see Appendix B),

the overall quadrupole of the molecule calculated from this distribution is −1.170 ea02 corresponding to the experimental estimate of (−1.09 ± 0.07) ea02. A similar calculation

for CO produces charges, dipoles and quadrupoles on all three sites (the C and O atoms and

the centre of the bond). e overall dipole and quadrupole moments from this distribution

are 0.036 ea0 and −1.515 ea02 respectively, compared with the experimental estimates of 0.043 ea0 and −1.4 ea02. e electrostatic energy between two molecules is now the sum

of the multipole interactions between the atoms or sites in di erent molecules. e energy

of interaction between two sets of distributed multipoles {qa, µa, Qa } and {qb , µb , Qb },

14 Introduction

1.0337

1.0337

1.066

1.066

N

N

C

O

q

0.427 −0.854 0.427 0.556 −0.832 0.276

µz

0.947 0.0

−0.947 1.159 −0.030 −0.796

Qzz 0.775 0.283 0.775 0.377 0.274 1.068

Fig. 1.8 e distributed multipoles required to represent the electrostatic potential of a N2 and CO molecule calculated using a cc-p-VQZ basis set. Multipoles are placed at the positions of
the atoms (black circles) and at the midpoint of the bond (white circles). e distances are in atomic units, a0 = 0.529 A˚ ; charges, q, are in units of e = 1.602 × 10−19 C; dipoles, µ, are in units of ea0 = 8.478 × 10−30 C m; and quadrupoles, Q, are in units of ea02 = 4.487 × 10−40 C m2 (see
Appendix B). Data from Stone ( ).

on atoms a and b at ra and rb , is given by

vaelbec = T qaqb + Tα (µaα qb − qa µbα )

+ Tα β

1 3

qaQbα

β

−

µaα µbβ

+

1 3

Qa

α

β

qb

+ 13Tα βγ µaα Qb βγ − Qaα β µbγ + 19Tα βγ δ Qaα β Qbγ δ

(1.15)

where we take the sum over repeated Cartesian indices α, β etc.

tensors are given by

Tα, β ...γ

=

(−1)n∇α ∇β

.

.

.

∇γ

1 rab

,

where n is the order of the tensor. us

e interaction or ‘T ’ (1.16)

T

=

1, rab

Tα

=

(rab )α ra3b

,

Tα β

=

3(rab )α (rab )β ra5b

− ra2bδα β ,

(1.17)

and so on. Note that the T tensors are de ned for rab = ra −rb . is is a useful formulation of the electrostatic energy for a computer simulation where the T tensors are readily expressed in terms of the Cartesian coordinates of the atoms. In addition, it is also straightforward to evaluate the derivative of the potential to obtain the force (the eld) or the eld gradient. e electrostatic potential, ϕ, at a distance r from a charge q is ϕ (r ) = q/r and the corresponding electric eld is E = −∇ϕ (r ). e eld is simply the force per unit charge. For example the eld (E) and eld gradient (E ) arising from a charge qb at b are

Eα = −∇α qbT = qbTα Eα β = −∇α ∇βqbT = −qbTα β .
e quadrupole tensor used in eqn (1.15) is de ned to be traceless,

(1.18)

Qαβ =

qa

3 2

(r

a

)α

(rb

)β

−

1 2

ra2δ

α

β

.

a

(1.19)

Model systems and interaction potentials 15
Code 1.1 Calculation of T tensors
is le is provided online. For a pair of linear molecules, electrostatic energies and forces are calculated using both the angles between the various vectors, and the T tensors. ! t_tensor.f90 ! E ectrostatic interactions: T-tensors compared with ang es PROGRAM t_tensor

e components of the dipole and quadrupole will initially be de ned in an atom- xed axis frame centred on an atom (or site) and at any given point in a simulation it will be necessary to transform these properties to the space- xed axis system for use in eqn (1.15) (Dykstra, ; Ponder et al., ). is can be simply achieved with a rotation matrix which we discuss in Section 3.3.1. An example of the calculation of T tensors is given in Code 1.1.
Electronic polarization refers to the distortion of the electronic charge cloud by the electrostatic eld from the other molecules. In a molecular uid it can be an important contribution to the energy. It is inherently a many-body potential and unlike many of the interactions already discussed in this chapter, it cannot be broken down to a sum over pair interactions. For this reason, it is expensive to calculate and was o en omi ed from earlier simulations. In these cases, some compensation was obtained by enhancing the permanent electrostatic interactions in the model. For example, in early simulations of water, the overall permanent dipole of the molecule was set to ca. 2.2 D rather than the gas-phase value 1.85 D (where 1 D = 0.299 79 × 10−30 C m) in order to t to the condensed phase properties in the absence of polarization (Watanabe and Klein, ). Nevertheless, polarization can be included explicitly in a model and there are three common approaches: the induced point multipole model; the uctuating charge model; and the Drude oscillator model (Antila and Salonen, ; Rick and Stuart, ).
e induced multipole approach (Applequist et al., ) is based on a knowledge of the atomic dipole polarizability, ααaβ , on a particular atom a. Consider a molecule containing a set of charges, qa, on each atom. e induced dipole at a contains two terms

∆µγa

=

α

a αγ

Eαa +

Tα β ∆µbβ ,

ba

(1.20)

where we sum over repeated indices α, β. e rst term in the eld, E, comes from the permanent charges at the other atoms and the second term comes from the dipoles that have been induced at these atoms. We ignore contributions from the eld gradient at atom a by se ing the higher-order polarizabilities to zero. Eqn (1.20) can be formally solved for the induced dipoles

∆µαa = Aα β Ebβ
bβ

(1.21)

16 Introduction

Example 1.1 Water, water everywhere

e earliest simulations of molecular liquids focused on water (Barker and Wa s,

; Rahman and Stillinger, ) and, since then, there have been over 80 000

published simulations of the liquid. Considerable e ort and ingenuity have gone into

developing models of the intermolecular potential between water molecules. ere

are three types of classical potential models in use: rigid, exible, and polarizable.

e simplest rigid models use a single Lennard-Jones site to represent the oxygen

atom and three partial charges: at the centre of the oxygen and the position of the

hydrogen atoms. ere are no speci c dispersion interactions involving the H atoms

and the charges are set to model the e ective condensed-phase dipole moment of

water, 2.2 D–2.35 D. Examples include the the and the / models (Berendsen

et al., ; ) used in the

force eld, and the

model (Jorgensen

et al., ) implemented in

and

. e precise geometry and the size of

the charges are di erent in each of these models. ey predict the experimental liquid

densities at a xed pressure but tend to overestimate the di usivity. e addition of

a fourth negative charge along the bisector of the H–O–H bond creates the

model (Jorgensen et al., ) and its generalization / (Abascal and Vega,

). ese models are capable of producing many of the qualitative features of

the complicated water phase diagram. e

potential model (Mahoney and Jor-

gensen, ) supplements the three charges on the atoms with two negative charges

at the position of the lone pairs. is model correctly predicts the density maximum

near 4 ◦C at 1 bar, and the liquid structure obtained from di raction experiments.

Flexibility can be included in models such as / using the intramolecular potential

of Toukan and Rahman ( ), in which anharmonic oscillators are used to represent

the O–H and H–H stretches. ese exible models predict many of the features of

the vibrational spectrum of the liquid (Praprotnik et al., ).

A recent study by Shvab and Sadus ( ) indicates that rigid models underestimate

the water structure and H-bond network at temperatures higher than 400 K and that

none of the models so far discussed can predict the heat capacities or thermal expan-

sion coe cients of the liquid. To improve on this position it is necessary to include

polarization in the potential. Li et al. ( a) show that the Matsuoka–Clementi–

Yoshimine potential ed from quantum calculations can be adapted to include

three-body dispersion interactions for O atoms and uctuating charges to create the

more accurate na model (Shvab and Sadus, ). ese enhancements produce

good agreement with experimental data over the entire liquid range of temperatures.

Jones et al. ( ) have taken a di erent approach by embedding a quantum Drude

oscillator ( ) and using adiabatic path-integral molecular dynamics to simulate

4000 water molecules. Sokhan et al. ( ) show that this approach can produce ac-

curate densities, surface tensions, and structure over a range of temperatures. Models

of water in terms of pseudo-potentials to describe the nuclei and core electrons, and

a model of the exchange correlation function to describe the non-classical electron

repulsion between the valence electrons will be described in Chapter 13.

Model systems and interaction potentials 17

where the relay matrix A = B−1 and

Bα β

=

(α a )−1  −Tα β

if a = b if a b.



(1.22)

Here A and B have dimensions of the number of sites involved in the polarization; this can be a large matrix, so practically eqn (1.20) is solved in a simulation by iterating the induced dipoles until convergence is achieved (Warshel and Levi , ). is method can also be used with the distributed multipole analysis where the eld at a polarizable atom might contain terms from the charge, dipole and quadrupole at a neighbouring atom, while the induction still occurs through the dipole polarizability (Ponder et al., ).
ere is a well-known problem with these point polarizability models in which the elements of A diverge at short separations: the so-called polarization catastrophe. is is caused by the normal breakdown in the multipole expansion at these distances. It can be mitigated by smearing the charges on a particular site ( ole, ). e e ect of this modi cation is to change the interaction tensor to

T˜α β

=

3ft (rab )α (rab )β − 4πϵ0ra5b

fera2bδα β

(1.23)

where fe and ft are two, simple, damping functions. A useful discussion of the various

possible choices for these damping functions is given by Stone ( ). e modi ed tensor T˜α β can now be used in eqn (1.20) to calculate the induced moments. Once the induced

dipole at atom a has been consistently determined then the induction energy associated

with that atom is

vaind

=

−

1 2

Eαa

∆µαa

.

(1.24)

e second method of including polarization in a model is the uctuating charge model, sometimes referred to as the electronegativity equalization model. e partial charges are allowed to uctuate as dynamical quantities. We can illustrate this approach by considering a model for water (Sprik, ). In addition to the three permanent charges normally used to represent the electrostatic moments, four additional uctuating charges are disposed in a tetrahedron around the central oxygen atom (see Fig. 1.9). e magnitudes of the charges qi (t ) uctuate in time, but they preserve overall charge neutrality

4
qi (t ) = 0
i =1

(1.25)

and they produce an induced dipole

4
∆µ = qi (t )ri
i =1

(1.26)

where ri are the vectors describing the positions of the tetrahedral charges with respect to the O atom. If |ri | rOH then the higher moments of the uctuating charge distribution can be neglected. e potential energy from the four charges is the sum of the electrostatic

18 Introduction

H +q
−2q q2 (t )
O q3 (t )

H +q
q1 (t )
q4 (t )

Fig. 1.9 A polarizable model for water (Sprik, ). e oxygen nucleus, O, is at the centre of the small tetrahedron. e three permanent charges +q, +q and −2q, at the lled black spheres, are arranged to model the permanent electrostatic potential of water. e four uctuating charges, qi (t ), located at the white spheres, respond to the surrounding eld and can be used to model the polarization of the molecule.

energy (−∆µ · E) and a self energy term (∆µ2/2αO) where αO is the dipole polarizability associated with the oxygen atom. e uctuating charges in this model can be determined by minimizing the potential energy in a given con guration subject to the constraint of charge neutrality, eqn (1.25),

∂ ∂qi

4 i =1

qi

ri

2αO

2

−

4 i =1

qi ri

·

E

= 0.

(1.27)

is approach can be extended to more complicated molecules by adding the appropriate number of uctuating charges; and simpli ed to study spherical ions by including just two uctuating charges within the spherical core. In these models, the uctuating charges, qi , are a crude representation of the electronic charge density and these can be usefully replaced by more realistic Gaussian charge distributions of width σ

ρi (r) = qi

1 2πσ 2

3/2
exp

2
r − ri − 2σ 2 .

(1.28)

is improves the description of the polarization, particularly at short intermolecular separations (Sprik and Klein, ). We note that these models can be readily included in a molecular dynamics simulation by se ing up separate equations of motions for the
uctuating charges (Sprik and Klein, ; Rick et al., ), and we shall consider this approach in Section 3.11.
e third approach is the Drude oscillator model or shell model. A polarizable site is represented as a heavy core particle of charge qd and a massless or light shell particle of charge −qd. ese two particles are connected by a harmonic spring with a spring constant k. e minimum in the spring potential is obtained when the core and shell are

Model systems and interaction potentials 19

coincident. e small charge qd is in addition to the permanent charge at a particular site. e shell and core can separate to produce an induced dipole moment

∆µ = −qd∆r

(1.29)

where ∆r is the vector from the core to the shell. e repulsion–dispersion interactions associated with a particular site are normally centred on the shell part of the site.
In the adiabatic implementation the shell is massless and at each step of a simulation the positions of the shells are adjusted iteratively to achieve the minimum energy con guration. In the dynamic model the shells are given a low mass (0.5 u) and an extended Lagrangian approach is used to solve the dynamics for short timesteps. In this case the shell particles are coupled to a heat bath at a low temperature (see Section 3.11). For these models, the atomic polarizability is isotropic and given by αa = qd2/k. Procedures are available for parameterizing the shell models to produce the correct molecular polarizabilities and electrostatic moments (Anisimov et al., ).
e model for water, shown in Fig. 1.9, begs the question as to whether we need to use a separate intermolecular potential to represent the hydrogen bond between two molecules. e hydrogen bond, between an H atom in one molecule and a strongly electronegative atom in another, is part permanent electrostatic interaction, part induced interaction, and some charge transfer. e evidence as reviewed by Stone ( ) indicates that the a ractive electrostatic interaction is the most important term in determining the structure of the hydrogen-bonded dimer but that induced interactions will make an important contribution in condensed phases. It should be possible to avoid a separate hydrogen-bond potential by including an accurate representation of the electrostatic interactions (by using, for example, the distributed multipole approach) and by including polarization.
For larger molecules it may not be reasonable to ‘ x’ all the internal degrees of freedom. In particular, torsional motion about bonds, which gives rise to conformational interconversion in, for example, alkanes, cannot in general be neglected (since these motions involve energy changes comparable with normal thermal energies). An early simulation of n-butane, CH3CH2CH2CH3 (Ryckaert and Bellemans, ; Mare´chal and Ryckaert,
), provides a good example of the way in which these features are incorporated in a simple model. Butane can be represented as a four-centre molecule, with xed bond lengths and bond-bending angles, derived from known experimental (structural) data (see Fig. 1.10). A very common simplifying feature is built into this model: whole groups of atoms, such as CH3 and CH2, are condensed into spherically symmetric e ective ‘united atoms’. In fact, for butane, the interactions between such groups may be represented quite well by the ubiquitous Lennard-Jones potential, with empirically chosen parameters. In a simulation, the C1−C2, C2−C3 and C3−C4 bond lengths are held xed by a method of constraints, which will be described in detail in Chapter 3. e angles θ and θ may be
xed by additionally constraining the C1−C3 and C2−C4 distances; that is, by introducing ‘phantom bonds’. If this is done, just one internal degree of freedom, namely the rotation about the C2−C3 bond, measured by the angle ϕ, is le unconstrained; for each molecule, an extra term in the potential energy, vtorsion(ϕ), appears in the Hamiltonian. is potential would have a minimum at a value of ϕ corresponding to the trans conformer of butane, and secondary minima at the gauche conformations. It is easy to see how this approach

20 Introduction

(a) 4
ϕ θ
2
3 θ
1

2 000 (b)
1 500

v(ϕ)/kB (K)

1 000

500

0−π

0

π

ϕ (radians)

Fig. 1.10 (a) Geometry of a model of butane de ning bending angles θ , θ and the torsional angle ϕ (Ryckaert and Bellemans, ). (b) e torsional potential, in the ( ) model of Padilla and Toxvaerd ( ) as reviewed in Dysthe et al. ( ).
may be extended to much larger exible molecules. e consequences of constraining bond lengths and angles will be treated in more detail in Chapters 2 and 4.
As the molecular model becomes more complicated, so too do the expressions for the potential energy, forces, and torques, due to molecular interactions. In Appendix C, we give some examples of these formulae, for rigid and exible molecules, interacting via site–site pairwise potentials, including multipolar terms. We also show how to derive the forces from a simple three-body potential.

1.3.4 Coarse-grained potential models
Coarse graining a potential involves avoiding the full atomic representation of the molecules to nd a description of the interaction at a longer or coarser length scale. We have already seen one simple example of this in the use of a united-atom potential for the methylene and methyl groups in butane. Coarse graining will reduce the number of explicit pairs that are needed for the calculation of the energy and force for a particular system and will reduce the computer time or, alternatively, allow us to study a much larger system. Normally an increase in the characteristic length scale in the model goes hand in hand with an increase in the timestep that we can use in a dynamical simulation of the problem. Coarse graining will allow us to use a longer timestep and to cover more ‘real’ time in our simulation.
One avour of coarse-grained model has been widely used to study liquid crystalline systems, exhibiting some long-range orientational order. For example, for the nematogen quinquaphenyl, a large rigid molecule that forms a nematic phase, a substantial number of sites would be required to model the repulsive core. A crude model, which represented each of the ve benzene rings as a single Lennard-Jones site, would necessitate 25 site–site interactions between each pair of molecules; sites based on each carbon atom would be more realistic but require 900 site–site interactions per pair. An alternative coarse-grained representation of intermolecular potential, introduced by Corner ( ), involves a single

Model systems and interaction potentials 21

site–site interaction between a pair of molecules, characterized by energy and length parameters that depend on the relative orientation of the molecules.
A version of this family of molecular potentials that has been used in computer simulation studies is the Gay–Berne potential (Gay and Berne, ). is is an extension of the Gaussian overlap model generalized to a Lennard-Jones form (Berne and Pechukas,
). e basic potential acting between two linear molecules is

vGB (rij , eˆi , eˆj ) = 4ϵ (rˆ, eˆi , eˆj ) (σs/ρij )12 − (σs/ρij )6 , where ρij = rij − σ (rˆ, eˆi , eˆj ) + σs.

(1.30a) (1.30b)

Here, rij is the distance between the centres of i and j, and rˆ = rij /rij is the unit vector along rij , while eˆi and eˆj are unit vectors along the axis of the molecules. e molecule can be considered (approximately) as an ellipsoid characterized by two diameters σs and σe, the separations at which the side-by-side potential, and the end-to-end potential, respectively, become zero. us

σ (rˆ, eˆi , eˆj ) = σs

1−

χ 2

(eˆi · rˆ + eˆj 1 + χ (eˆi ·

· rˆ)2 eˆj )

+

(eˆi · rˆ − eˆj 1 − χ (eˆi ·

· rˆ)2 eˆj )

−1/2

where

χ

=

κ2 κ2

− +

1 1

,

and κ = σe/σs.

(1.31a) (1.31b)

κ is the elongation and χ is the shape anisotropy parameter (κ = 1, χ = 0 for spherical particles, κ → ∞, χ → 1 for very long rods, and κ → 0, χ → −1 for very thin disks).
e energy term is the product of two functions

ϵ (rˆ, eˆi , eˆj ) = ϵ0 ϵ1ν (eˆi , eˆj ) ϵ2µ (rˆ, eˆi , eˆj )

(1.32a)

where

ϵ1 (eˆi , eˆj ) = 1 − χ 2 (eˆi · eˆj )2 −1/2

ϵ2 (rˆ, eˆi , eˆj )

=

1

−

χ 2

(eˆi · 1+

rˆ + eˆj · rˆ)2 χ (eˆi · eˆj )

+

(eˆi · 1−

rˆ − eˆj · rˆ)2 χ (eˆi · eˆj )

(1.32b) (1.32c)

and the energy anisotropy parameter is

χ

=

κ κ

1/µ 1/µ

− +

1, 1

where κ = ϵss/ϵee.

(1.32d)

ϵss and ϵee are the well depths of the potentials in the side-by-side and end-to-end congurations respectively. e potential is illustrated for these arrangements, as well as
for T-shaped and crossed con gurations, in Fig. 1.11. e original model, with exponents µ = 2, ν = 1, and parameters κ = 3, κ = 5, was used to mimic four collinear Lennard-Jones sites (Gay and Berne, ). e potential and corresponding force and torque can be readily evaluated and the functional form is rich enough to create mesogens of di erent shapes and energy anisotropies that will form the full range of nematic, smectic, and discotic liquid crystalline phases (Luckhurst et al., ; Berardi et al., ; Allen, a;

22 Introduction

4.0

2.0

v(r )/ϵ0

0.0

−2.0

−4.0

1.0

2.0

3.0

4.0

5.0

r /σs

Fig. 1.11 e Gay-Berne potential, with parameters µ = 1, ν = 3, κ = 3, κ = 5 (Berardi et al., ), as a function of centre–centre separation, for various molecular orientations.

Luckhurst, ). It is discussed further in Appendix C. Extensions of the potential, and

its use in modelling liquid crystals, are discussed by Zannoni ( ).

e

approach is a coarse-grained potential developed for modelling lipid

bilayers (Marrink et al., ; ) and proteins (Monticelli et al., ). In this model

the bonded hydrogen atoms are included with their heavier partners, such as C, N, or

O. ese united atoms are then further combined using a 4:1 mapping to create larger

beads (except in the case of rings where the mapping is normally 3:1). For these larger

beads, there are four di erent bead types: charged (Q), polar (P), nonpolar (N), and apolar

(C). Each of these types is further subdivided depending on the bead’s hydrogen-bond

forming propensities or its polarity. Overall there are 18 bead-types and each pair of

beads interacts through a Lennard-Jones potential where the σ and ϵ parameters are

speci c to the atom types involved. Charged beads also interact through Coulombic

potentials. e intramolecular interactions (bonds, angles, and torsions) are derived from

atomistic simulations of crystal structures. is kind of moderate coarse graining has

been successfully applied to simulations of the clustering behaviour of the membrane

bound protein syntaxin-1A (van den Bogaart et al., ) and the simulation of the domain

partitioning of membrane peptides (Scha¨fer et al., ).

It is possible to coarse grain potentials in a way that results in larger beads, that might

contain 1–3 Kuhn chain-segments of a polymer or perhaps ten solvent molecules. We will

consider this approach more fully in Chapter 12. However, at this point, we mention a

very simple coarse-grained model of polymer chains due to Kremer and Grest ( ) and

termed the nitely extensible nonlinear elastic ( ) model. e bonds between beads

Model systems and interaction potentials 23 200

150

v(r )/ϵ

100

50

00.0

0.5

1.0

1.5

r /σ

Fig. 1.12 e potential between bonded atoms in a coarse-grained polymer (solid line) together

with its component parts (dashed lines): the a ractive

potential, eqn (1.33) with R0 = 1.5σ

and k = 30ϵ/σ 2, and the repulsive Lennard-Jones potential, eqn (1.10a). Also shown (do ed line)

is a harmonic potential, ed to the curvature at the minimum. See Kremer and Grest ( ) for

details.

within the chain are represented by the potential energy

vFENE (r )

=

−

1 2

k

R02



ln

1 − (r /R0)2

∞



r < R0 r ≥ R0.

(1.33)

is is combined with the potential vRLJ(r ) of eqn (1.10a), representing the e ects of

excluded volume between every pair of beads (including those that are bonded together).

e key feature of this potential is that it cannot be extended beyond r = R0. is is important when studying entanglement e ects: the simpler harmonic potential could, in

principle, extend enough to let chains pass through one another, in some circumstances.

Finally, there has been considerable e ort to develop a simple, single-site coarse-

grained potential for water. One approach (Molinero and Moore, ; Moore and Mo-

linero, ) has been to abandon the long-range electrostatics conventionally associated

with hydrogen bonds, and use instead short-range directional interactions, of the kind

previously used to model silicon (Stillinger and Weber, ). e resulting monatomic

water (mW) model is very cheap to simulate but surprisingly successful in reproducing

experimental structural and thermodynamic properties. Can one go further? It is di cult

to imagine that a spherical, isotropic potential will be able to capture the strong association

interactions in the uid. Nevertheless, Lobanova et al. ( ) have used a Mie potential, a

versatile form of the standard Lennard-Jones potential, where

vMie (r ) = Cϵ

σ

n
−

σ

m
,

r

r

with

C=

n n −m

n

m / (n −m )
.

m

(1.34)

24 Introduction
A potential with n = 8 and m = 6 can be used with temperature-dependent energy and length parameters to represent the thermophysical properties of water over a broad range of conditions. However, a simpler form where ϵ and σ are independent of temperature can be used to represent water in the calculation of mixture phase diagrams such as CO2/H2O (Mu¨ller and Jackson, ). We brie y discuss this approach to coarse graining in Section 12.7.3. e examples just given are two amongst many a empts to model water in a coarse-grained way (for a review see Hadley and McCabe, ).
1.3.5 Calculating the potential
is is an appropriate point to introduce a piece of computer code, which illustrates the calculation of the potential energy in a system of Lennard-Jones atoms. Simulation programs are wri en in a range of languages: Fortran, C, and C++ are the most common, sometimes with a wrapper wri en in Python or Java. Here we shall use Fortran, which has a compact notation for arrays and array operations, and is simple enough to be read as a ‘pseudo-code’. Appendix A contains some discussion of di erent programming approaches, and a summary of some of the issues a ecting e ciency. We suppose that the coordinate vectors of our atoms are stored in an array r of rank two, with dimensions (3,n), where the rst index covers the x, y, and z components, and the second varies from 1 to n (equal to N , the number of particles). e potential energy will be stored in a variable pot, which is zeroed initially, and is then accumulated in a double loop over all distinct pairs of atoms, taking care to count each pair only once. is is shown in Code 1.2.
e Lennard-Jones parameters ϵ and σ are assumed to be stored in the variables eps j and sigma respectively. e colon ‘:’ is short for an implied loop over the corresponding index, so the statement rij(:) = r(:,i) - r(:,j) stands for the vector assignment rij = ri −rj .
e SUM function simply adds the components of its (array) argument, which in this case gives ri2j = xi2j + yi2j + zi2j . Code 1.2 takes no account of periodic boundary conditions (we return to this in Section 1.6.2). Some measures have been taken here to avoid unnecessary use of computer time. e value of σ 2 is computed once beforehand, and stored in the variable sigma_sq; the factor 4ϵ, which appears in every pair potential term, is multiplied in once, at the very end. e aim is to avoid many unnecessary operations within the crucial ‘inner loop’ over index j. e more general questions of time-saving tricks in this part of the program are addressed in Chapter 5. e extension of this type of double loop to deal with other forms of the pair potential, and to compute forces in addition to potential terms, is straightforward, and examples will be given in later chapters. For molecular systems, the same general principles apply, but additional loops over the di erent sites or atoms in a molecule may be needed. For example, consider the site–site diatomic model of eqn (1.12) and Fig. 1.6. en the intermolecular interactions might be computed as in Code 1.3. Note that, apart from the dependence of the range of the j loop on the index i, the order of nesting of loops is a ma er of choice. Here, we have placed a loop over molecular indices innermost; assuming that n is relatively large, and depending on the machine architecture, this may improve the e ciency of fetching the relevant coordinates from memory (in Fortran, the arrays are stored so that the rst indices vary rapidly, and the last indices vary slowly, so there is usually an advantage in accessing contiguous blocks of memory, or cache, in sequence). Simulations of molecular systems may also

Constructing an intermolecular potential from rst principles 25

Code 1.2 Double loop for Lennard-Jones potential

is code snippet illustrates the calculation of the potential energy for a system of Lennard-Jones atoms, using a double loop over the atomic indices. e declarations at the start are given just to remind us of the types and sizes of variables and arrays (some notes on precision of variables appear in Appendix A).

INTEGER

:: n, i, j

REAL , DIMENSION(3,n) :: r

REAL , DIMENSION(3) :: rij

REAL

:: eps j , sigma , sigma_sq

REAL

:: pot , rij_sq , sr2 , sr6 , sr12

sigma_sq = sigma ** 2

pot = 0.0

DO i = 1, n-1

DO j = i+1, n

rij(:) = r(:,i) - r(:,j)

rij_sq = SUM ( rij ** 2 )

sr2 = sigma_sq / rij_sq

sr6 = sr2 ** 3

sr12 = sr6 ** 2

pot = pot + sr12 - sr6

END DO

END DO

pot = 4.0 * eps j * pot

involve the calculation of intramolecular energies, which, for site–site potentials, will necessitate a triple summation (over i, a, and b).
ese examples are essentially summations over pairs of interaction sites in the system. Any calculation of three-body interactions will, of course, entail triple summations over distinct triplets of indices i, j, and k; these will be much more time consuming than the double summations described here. Even for pairwise-additive potentials, the energy or force calculation is the most expensive part of a computer simulation. We will return to this crucial section of the program in Chapter 5.
1.4 Constructing an intermolecular potential from rst principles
1.4.1 Introduction
ere are two approaches to constructing an intermolecular potential for use in a simulation. For small, simple molecules and their mixtures, it is possible to customize a model, with considerable freedom in choosing the functional form of the potentials and in adjusting the parameters for the problem at hand. For larger molecules such as polymers, proteins, or , either in solution or at a surface, or for multi-component mixtures

26 Introduction

Code 1.3 Site–site potential energy calculation

e coordinates ria of site a in molecule i are stored in the elements r(:,i,a) of a rank-3 array; for a system of diatomic molecules na=2.

INTEGER

:: n, i, j, a, b

REAL , DIMENSION(3,n,na) :: r

REAL , DIMENSION(3)

:: rij

DO a = 1, na

DO b = 1, na

DO i = 1, n - 1

DO j = i + 1, n

rij(:) = r(:,i,a) - r(:,j,b)

... ca cu ate the i-j interaction ...

END DO

END DO

END DO

END DO

containing many di erent types of molecule, then it will be more usual to employ one of the standard force elds (consisting of xed functional forms for the potentials combined with parameters corresponding to the many di erent atom types in the simulation). We will cover the rst aspect of model building in this section and consider force elds in Section 1.5.
ere are essentially two stages in se ing up a model for a realistic simulation of a given system. e rst is ‘ge ing started’ by constructing a rst guess at a potential model. is will allow some preliminary simulations to be carried out. e second is to use the simulation results, in comparison with experiment, to re ne the potential model in a systematic way, repeating the process several times if necessary. We consider the two phases in turn.
1.4.2 Building the model potential
To illustrate the process of building up an intermolecular potential from rst principles, we consider a small molecule, such as N2, OCS, or CH4, which can be modelled using the interaction site potentials discussed in Section 1.3. e essential features of this model will be an anisotropic repulsive core, to represent the shape, an anisotropic dispersion interaction, and some partial charges or distributed multipoles to model the permanent electrostatic e ects. is crude e ective pair potential can then be re ned by using it to calculate properties of the gas, liquid, and solid, and comparing with experiment. Each short-range site–site interaction can be modelled using a Lennard-Jones potential. Suitable energy and length parameters for interactions between pairs of identical atoms in di erent molecules are available from a number of simulation studies. Some of these are given in Table 1.1. e energy parameter ϵ increases with atomic number as the polarizability goes up; σ also increases down a group of the Periodic Table, but decreases

Constructing an intermolecular potential from rst principles 27 Table 1.1 Atom–atom interaction parameters

Atom Source H Murad and Gubbins ( ) He Maitland et al. ( ) C Tildesley and Madden ( ) N Cheung and Powles ( ) O English and Venables ( ) F Singer et al. ( ) Ne Maitland et al. ( ) S Tildesley and Madden ( ) Cl Singer et al. ( ) Ar Maitland et al. ( ) Br Singer et al. ( ) Kr Maitland et al. ( )

ϵ/kB (K) σ (nm)

8.6

0.281

10.2

0.228

51.2

0.335

37.3

0.331

61.6

0.295

52.8

0.283

47.0

0.272

183.0

0.352

173.5

0.335

119.8

0.341

257.2

0.354

164.0

0.383

from le to right across a period with the increasing nuclear charge. For elements which do not appear in Table 1.1, a guide to ϵ and σ might be provided by the polarizability and van der Waals radius respectively. ese values are only intended as a reasonable
rst guess: they take no regard of chemical environment and are not designed to be transferable. For example, the carbon atom parameters in CS2 given in the table are quite di erent from the values appropriate to a carbon atom in graphite (Crowell, ).
Interactions between unlike atoms in di erent molecules can be approximated using the venerable Lorentz–Berthelot combining rules. For example, in CS2 the cross-terms are

σCS =

1 2

σCC + σSS

,

ϵCS = ϵCCϵSS 1/2.

(1.35)

ese rules are approximate; the ϵ cross-term expression, especially, is not expected to be appropriate in the majority of cases (Delhommelle and Millie´, ; Haslam et al., ).
In tackling larger molecules, it may be necessary to model several atoms as a uni ed site. We have seen this for butane in Section 1.3, and a similar approach has been used in a model of benzene (Evans and Wa s, ). e speci cation of an interaction site model is made complete by de ning the positions of the sites within the molecule. Normally, these are located at the positions of the nuclei, with the bond lengths obtained from a standard source (CRC, ).
Rapid progress has been made in ing the parameters for many classical pair potentials using ab initio quantum mechanical calculations. For example, symmetry-adapted perturbation theory, based on a density-functional approach, can be used to calculate separable and transferable parameters for the dispersion and electrostatic interactions (McDaniel and Schmidt, ). Calculations on monomers are used to estimate asymptotic properties such as charge and polarizability, while dimer calculations are used to estimate the parameters depending on charge density overlaps. e resulting parameters can be

28 Introduction
used with simple functional forms in simulations and the technique has recently been applied to the parameterization and simulation of an ionic liquid (Son et al., ).
e site–site Lennard-Jones potentials include an anisotropic dispersion which has the correct r −6 radial dependence at long range. However, this is not the exact result for the anisotropic dispersion from second-order perturbation theory. e correct formula, in an appropriate functional form for use in a simulation, is given by Burgos et al. ( ). Its implementation requires an estimate of the polarizability and polarizability anisotropy of the molecule.
It is also possible to improve the accuracy of the overall repulsion–dispersion interaction by considering an anisotropic site–site potential in place of vab (rab ) in eqn (1.12). In other words, in a diatomic model of a chlorine molecule, the interatomic potential between chlorine atoms in di erent molecules would depend on rab and the angles between rab and intramolecular bonds. is type of model has been used to rationalize the liquid and solid structures of liquid Cl2, Br2, and I2 (Rodger et al., a,b).
e most straightforward way of representing electrostatic interactions is through partial charges as discussed in Section 1.3. To minimize the calculation of site–site distances they can be made to coincide with the Lennard-Jones sites, but this is not always desirable or possible; the only physical constraint on partial charge positions is that they should not lie outside the repulsive core region, since the potential might then diverge if molecules came too close. e magnitudes of the charges can be chosen to duplicate the known gas-phase electrostatic moments (Gray and Gubbins, , Appendix D). Alternatively, the moments may be taken as adjustable parameters. For example, in a simple three-site model of N2 representing only the quadrupole–quadrupole interaction, the best agreement with condensed phase properties is obtained with charges giving a quadrupole 10 %–15 % lower than the gas-phase value (Murthy et al., ). However, a sensible strategy is to begin with the gas-phase values, and alter the repulsive core parameters ϵ and σ before changing the partial charges.
Partial charges can also be developed using theoretical calculations. Bayly et al. ( ) have developed the widely used restrained electrostatic potential ( ) method. In this technique:
(a) a molecule is placed in a 3D grid of points;
(b) the electrostatic potential is calculated at each grid point, outside the repulsive core, using a quantum mechanical calculation;
(c) a charge at each atom of the molecules is adjusted to reproduce the electrostatic potential at the grid points as accurately as possible.
Typically, accurate enough quantum mechanical estimates of the electrostatic eld can be obtained using the 6-31G∗ level of the Gaussian code (Frisch et al., ). In order to make this ing procedure robust and to obtain charges that are transferable between di erent molecules, it is necessary to minimize the magnitude of the charges that will t the eld. is is achieved using a hyperbolic restraint function in the minimization that pulls the magnitude of the charges towards zero.
Distributed multipoles and polarizabilities, for molecules containing up to about 60 atoms, can be calculated from rst principles using the am package developed by Stone and co-workers (Misqui a and Stone, ).

Force elds 29

1.4.3 Adjusting the model potential
e rst-guess potential can be used to calculate a number of properties in the gas, liquid, and solid phases; comparison of these results with experiment may be used to re ne the potential, and the cycle can be repeated if necessary. e second virial coe cient is given by

B (T

)

=

−

2π Ω2

∞
ri2j dri j
0

dΩi

dΩj exp −v(rij , Ωi , Ωj )/kBT − 1

(1.36)

where Ω = 4π for a linear molecule and Ω = 8π2 for a non-linear one. is multidimensional integral (four-dimensional for a linear molecule and six-dimensional for a non-linear one) is easily calculated using a non-product algorithm (Murad, ). Experimental values of B(T ) have been compiled by Dymond and Smith ( ). Trial and error adjustment of the Lennard-Jones ϵ and σ parameters should be carried out, with any bond lengths and partial charges held xed, so as to produce the closest match with the experimental B(T ). is will produce an improved potential, but still one that is based on pair properties.
e next step is to carry out a series of computer simulations of the liquid state, as described in Chapters 3 and 4. e densities and temperatures of the simulations should be chosen to be close to the orthobaric curve of the real system, that is, the liquid– vapour coexistence line. e output from these simulations, particularly the total internal energy and the pressure, may be compared with the experimental values. e coexisting pressures are readily available (Rowlinson and Swinton, ), and the internal energy can be obtained approximately from the known latent heat of evaporation. e energy parameters ϵ are adjusted to give a good t to the internal energies along the orthobaric curve, and the length parameters σ altered to t the pressures. If no satisfactory t is obtained at this stage, the partial charges may be adjusted. It is also possible to adjust potential parameters to reproduce structural properties of the liquid, such as the site–site pair distribution functions (see Section 2.6), which can be extracted from coherent neutron di raction studies using isotopic substitution (Cole et al., ; Zeidler et al., ).
Although the solid state is not the province of this book it o ers a sensitive test of any potential model. Using the experimentally observed crystal structure, and the re ned potential model, the la ice energy at zero temperature can be compared with the experimental value (remembering to add a correction for quantum zero-point motion). In addition, the la ice parameters corresponding to the minimum energy for the model solid can be compared with the values obtained by di raction, and also la ice dynamics calculations (Neto et al., ) used to obtain phonons, librational modes, and dispersion curves of the model solid. Finally, we can ask if the experimental crystal structure is indeed the minimum energy structure for our potential. ese constitute severe tests of our model-building skills (Price, ).

1.5 Force elds
In approaching the simulation of a complicated system, there might be 30 di erent atom types to consider and several hundred di erent intra- and inter-molecular potentials to t. One would probably not want to build the potential model from scratch. Fortunately, it is

30 Introduction

possible to draw on the considerable body of work that has gone into the development of

consistent force elds over the last 50 years (Bixon and Lifson, ; Lifson and Warshel,

; Ponder and Case, ).

A force eld, in the context of a computer simulation, refers to the functional forms

used to describe the intra- and inter-molecular potential energy of a collection of atoms,

and the corresponding parameters that will determine the energy of a given con guration.

ese functions and parameters have been derived from experimental work on single

molecules and from accurate quantum mechanical calculations. ey are o en re ned by

the use of computer simulations to compare calculated condensed phase properties with

experiment. is is precisely the same approach described in Section 1.4.3, but on a bigger

scale, so that the transferable parameters developed can be used with many di erent

molecules. Some examples of widely used force elds are given in Table 1.2. is list is

representative and not complete. e individual force elds in the table are constantly

being updated and extended. For example, the force eld has been re ned to allow

for the modelling of carbohydrates (Kony et al., ) and the and

force elds

have been used as the basis of a new eld for ionic liquids (Lopes et al., ). Extensions

and versions are o en denoted by the XX speci cation following the force eld name. A

short search of the websites of the major force elds will establish the latest version and

the most recent developments.

Force elds are o en divided into three classes. Class I force elds normally have a

functional form of the type

V=

1 2

kr

(ri j

−

r0)2

+

1 2

kθ

(θi

j

k

− θ0)2

bonds

angles

+
torsions

n

kϕ,n [cos(nϕijk

+ δn ) + 1] +
non-bonded

qiqj 4πϵ0ri j

+

Ai j ri1j2

−

Bi j ri6j

.

pairs

(1.37)

e rst term in eqn (1.37) is a sum over all bonds, with an equilibrium bond-length

r0. ere is one term for every pair ij of directly connected atoms. In some force elds

the harmonic potential can be replaced by a more realistic functional form, such as

the Morse potential, or the bonds can be xed at their equilibrium values. e second

term is a sum over all bond angles. ere is one term for each set of three connected

atoms ijk and it usually has a quadratic form. e third term is the sum over all torsions

involving four connected atoms ijk . In principle, this is an expansion in trigonometric

functions with di erent values of n, the multiplicity (i.e. the number of minima in a

rotation of 2π around the j–k bond); many force elds x n = 3. is term can also

include improper torsions, where the four atoms de ning the angle are not all connected

by covalent bonds; such terms serve primarily to enforce planarity around sp2 centres

and use a variety of functional forms (Tuzun et al., ). e fourth term is a sum over

the non-bonded interactions (between molecules and within molecules). In particular, it

describes the electrostatic and repulsion–dispersion interactions. It invariably excludes

1–2 and 1–3 pairs in the same molecule. Some force elds do include a non-bonded 1–4

interaction but the parameters Aij , Bij describing this interaction can be di erent from the values for atoms separated by more than three bonds (a scaling factor of 0.4 is used

in the param force eld of

(Brooks et al., )). In some force elds, the ri−j12

Table 1.2 Force elds and their domains of application. is list is not complete and simply includes representative examples of some of the force elds commonly used in liquid-state simulations.

Force eld

Class Domain of Application

Source

I peptides, small organics

Jorgensen et al. ( )

I proteins with explicit water

Mackerell et al. ( )

I

, , and lipids

Mackerell et al. ( )

I peptides, small organics, charges

Wang et al. ( )

I small organics, drug design

Wang et al. ( )

G a I lipids, micelles

Schuler et al. ( )

II small molecules, polymers

Sun ( )

lay

II hydrated minerals

Cygan et al. ( )

II small organics, coordination compounds

Allinger et al. ( )

II full Periodic Table (including actinides)

Rappe et al. ( )

III polarizable atoms

Cieplak et al. ( )

III polarizable multipoles, distributed multipoles Ponder et al. ( )

III coarse-grained, proteins, lipids, polymers

Marrink et al. ( )

Force elds 31

eax

III chemical reactions

van Duin et al. ( )

32 Introduction

repulsion (associated with the Lennard-Jones potential) is replaced by an ri−j9 repulsion which can produce be er agreement with direct quantum calculations of the repulsion

(Hagler et al., ; Halgren, ). e exponential form of the repulsion (A exp(−Brij ))

was used in earlier versions of the

force elds ( and ) but has now been

replaced by the ri−j12 repulsion. e cross-interactions for the parameters in the repulsion–

dispersion potential are o en described using the Lorentz–Berthelot combining rules or

an alternative such as the Slater–Kirkwood formula (Slater and Kirkwood, ). If these

crossed interactions are important in the model they can be determined directly by ing

to experiment. In class I force elds, a simple Coulombic term is used to describe the

interaction between the partial charges, which represent the electrostatic interactions

between molecules.

Di erent parameters are required for di erent atoms in di erent environments, and

all of the atom types in the model must be speci ed. For example, in the

force

eld G a (Schuler et al., ), there are 12 types of C atoms, six Os, six Ns, four Cls,

three Hs, two Ss, two Cus and one type for each of the remaining common atoms. e

parameters {kr , kθ , kϕ,n, δn, qi , qj , Aij , Bij } are then speci ed for combinations of the atom types. For example, in a peptide chain, which contains C, N, and Cα atom types along the backbone (where C is a carbon additionally double-bonded to an oxygen and Cα is a carbon additionally connected to a hydrogen and a side chain) we would require kr for the C–N stretch, a di erent kr for the N–Cα stretch, kθ for the C–N–Cα bend, kϕ,n, for the C–N–Cα –C torsion, and additional parameters for the other bends and torsion in the

backbone.

All-atom force elds provide parameters for every type of atom in a system, including

hydrogen, while united-atom force elds treat the hydrogen and carbon atoms in each

terminal methyl and each methylene bridge as a single interaction centre.

A class II force eld normally adds cubic or anharmonic terms to the stretching

potentials and de nes explicit o -diagonal elements in the force constant matrix. us,

the force eld will contain terms of the form

vstr–str (r12, r23) = k12,23 (r12 − r12,0) (r23 − r23,0) vbend–str (θ123, r12) = k123,12 (θ123 − θ123,0) (r12 − r12,0)

(1.38)

where r12 and r23 are two adjacent bonds in the molecule, which include the angle θ123.

ese additional potentials represent the fact that bonds, angles and torsions are not inde-

pendent in molecules. Most cross-terms involve two internal coordinates and Dinur and

Hagler ( ) have used quantum mechanical calculations to show that the stretch–stretch,

stretch–bend, bend–bend, stretch–torsion, and bend–bend–torsion are the important

coupling terms. e cross-terms are essential to include in models when a empting to

calculate accurate vibrational frequencies. Despite the additional complexity, Class II force

elds, such as

and , have been used to good e ect in liquid-state simulations

(Peng et al., ; Sun, ).

Class III force elds go beyond the basic prescription to include more accurate rep-

resentations of the electrostatic interactions between molecules and the inclusion of

polarizability (as discussed in Section 1.3.3). For example, the

force eld includes

distributed multipoles and the atom polarizabilities with the ole modi cation of the

Force elds 33

interaction tensor. is class would also include coarse-grained force elds such as

used to model lipids, proteins, and carbohydrates (see Section 1.3.4) and force elds

speci cally designed to model chemical reactions such as eax . eax includes a set of

relationships between the bond distance and the bond order of a particular covalent bond.

Once the bond order is determined, the associated bond energy can be calculated. is

procedure results in proper dissociation of bonds to separated atoms at the appropriate

distances.

A er many decades of force eld development, there are still considerable di erences

between the predictions from even the Class I force elds. In an excellent review of the eld,

Ponder and Case ( ) compare simulations of a solvated dipeptide using

,

, and -aa force elds to map the free energy of the dipeptide as a function of

the two torsional angles, ψ and ϕ. All three force elds exhibit ψ –ϕ maps that are di erent

from one another and di erent from the results of an ab initio simulation of the same

problem. In contrast, in considering the liquid-state properties for butane, methanol, and

N-methylacetamide, Kaminski and Jorgensen ( ) demonstrated reasonable agreement

between the

and force eld, both of which had been ed to liquid-state

properties. In this study the

force eld, that had been optimized for gas-phase

geometries, needed to be adjusted to obtain the same level of agreement when applied

to the liquids. One important point is that it is not possible to mix and match di erent

force elds. ey have been optimized as a whole and one should not a empt to use parts

of one eld with parts of another. is means that devising force elds to simulate very

di erent materials interacting with each other is a particular challenge. As an illustration,

the steps taken to model the adsorption of biomolecules on the surface of metallic gold,

in water, are discussed in Example 1.2.

It is di cult to make blanket recommendations concerning the use of particular force

elds. Individual researchers will need to understand the kind of problems for which the

force eld has been optimized to know if it can be applied to their particular problem.

One sensible strategy would be to check the e ect of using a few of the more common

force elds on the problem to understand the sensitivity of the results to this choice.

An important advantage of the force- eld approach is that that particular elds are

o en associated with large simulation programs. e acronyms

,

, and

can also stand for large molecular dynamics codes which have been designed

to work with the particular forms of a eld and there are many examples of other codes

such as

(Plimpton, ) and oly (Todorov and Smith, ) that can take

standard force elds with some adjustments. ere is also a huge industry of analysis and

data manipulation programmes that have grown with the major force elds and codes.

Of course, using these programmes as black-boxes is never a good idea and we plan in

this book to dig into the principles behind such codes. Equally, if one can take advantage

of the many years of careful development that have gone into producing these packages

in an informed way, an enormous range of complicated and important applications can

be tackled fairly quickly.

34 Introduction

Example 1.2 Peptide–gold potentials

Peptides, short chains of amino acids, may be designed so as to speci cally favour

adsorption on certain material surfaces. is underpins a range of possible bio-

nanotechnology applications (Care et al., ). Understanding this selectivity and

speci city is a great challenge to molecular simulation: clearly the adsorption free

energy depends on many factors, including changes in peptide exibility, its solvation,

and displacement of the water layer at the surface. Measurement of adsorption free

energies requires advanced simulation techniques (see Chapters 4 and 9); modelling

the potential energy of interaction between the surface and individual amino acids is

itself challenging, involving the cross-interaction between two very di erent materi-

als (Di Felice and Corni, ; Heinz and Ramezani-Dakhel, ). Here we focus on

recent a empts to model peptide interactions with the surface(s) of metallic gold.

A simple Lennard-Jones force eld for a range of metals, including gold, has been

proposed (Heinz et al., ): ϵAuAu and σAuAu are chosen to reproduce various experimental bulk and surface properties, under ambient conditions. Water and peptide

atom–Au parameters are obtained by standard combining rules. Feng et al. ( )

have used this potential to study the adsorption of individual amino acids on gold,

while Cannon et al. ( ) have used it to highlight solvent e ects in peptide adsorp-

tion. A di erent parameterization, similar in spirit, has been derived independently

(Vila Verde et al., ; ). e whole method has been generalized to cover a

range of other materials (Heinz et al., ). Compatibility with standard force elds,

such as

, is an advantage of this approach; polarization of the metal, and

chemisorption, however, are neglected.

A purely dispersive potential of this kind may have limitations when one considers

structure: adsorption (of water molecules or peptide atoms) onto hollow sites on

the surface is strongly favoured. On metallic surfaces, however, adsorption on top

of surface atoms is o en preferred, as indicated by rst-principles simulations. In

the ol force eld (Iori et al., ), dynamical polarization of gold atoms is repre-

sented by a rotating dipole, and virtual interaction sites are introduced to tackle the

hollow-site adsorption problem. ol is parameterized using extensive rst-principles

calculations and experimental data, with special consideration given to surface inter-

actions with sp2-hybridized carbons. An extension, ol –

, reparameterized

for compatibility with

, also allows consideration of di erent gold surfaces

(Wright et al., b,a), opening up the study of facet selectivity (Wright et al., ).

In ol , the gold atoms are held xed during the simulation.

Tang et al. ( ) have compared ol results with experimental studies of peptide

adsorption, and with the force eld of Heinz et al. ( ). While both models perform

reasonably well in describing the trend in amino acid adsorption energies, there are

areas such as the prediction of water orientation in the surface layer where ol –

agrees be er with rst-principles simulations (Nadler and Sanz, ). is

approach may allow one to separate the enthalpic contributions to the binding free

energy, and ascribe them to individual residues (Corni et al., ; Tang et al., ).

Studying small systems 35
1.6 Studying small systems
1.6.1 Introduction
Simulations are usually performed on a small number of molecules, 10 ≤ N ≤ 10 000. e size of the system is limited by the available storage on the host computer, and, more crucially, by the speed of execution of the program. e time taken for a double loop used to evaluate the forces or potential energy is proportional to N 2. Special techniques (see Chapter 5) may reduce this dependence to O(N ), for very large systems, but the force/energy loop almost inevitably dictates the overall speed and, clearly, smaller systems will always be less expensive. If we are interested in the properties of a very small liquid drop, or a microcrystal, then the simulation will be straightforward. e cohesive forces between molecules may be su cient to hold the system together unaided during the course of a simulation, otherwise our set of N molecules may be con ned by a potential representing a container, which prevents them from dri ing apart (see Chapter 13). ese arrangements, however, are not satisfactory for the simulation of bulk liquids. A major obstacle to such a simulation is the large fraction of molecules which lie on the surface of any small sample; for 1000 molecules arranged in a 10 × 10 × 10 cube, 83 = 512 lie in the interior, leaving 488 (nearly half!) on the cube faces. Even for N = 1003 = 106 molecules, 6 % of them will lie on the surface. Whether or not the cube is surrounded by a containing wall, molecules on the surface will experience quite di erent forces from those in bulk.
1.6.2 Periodic boundary conditions.
e problem of surface e ects can be overcome by implementing periodic boundary conditions (Born and von Karman, ). e cubic box is replicated throughout space to form an in nite la ice. In the course of the simulation, as a molecule moves in the original box, its periodic image in each of the neighbouring boxes moves in exactly the same way. us, as a molecule leaves the central box, one of its images will enter through the opposite face. ere are no walls at the boundary of the central box, and no surface molecules. is box simply forms a convenient axis system for measuring the coordinates of the N molecules. A two-dimensional version of such a periodic system is shown in Fig. 1.13. e duplicate boxes are labeled A, B, C, etc., in an arbitrary fashion. As particle 1 moves through a boundary, its images 1A, 1B, etc. (where the subscript speci es in which box the image lies) move across their corresponding boundaries. e number density in the central box (and hence in the entire system) is conserved. It is not necessary to store the coordinates of all the images in a simulation (an in nite number!), just the molecules in the central box. When a molecule leaves the box by crossing a boundary, a ention may be switched to the image just entering. It is sometimes useful to picture the basic simulation box (in our two-dimensional example) as being rolled up to form the surface of a three-dimensional torus or doughnut, when there is no need to consider an in nite number of replicas of the system, nor any image particles. is correctly represents the topology of the system, if not the geometry. A similar analogy exists for a three-dimensional periodic system, but this is more di cult to visualize!
It is important to ask if the properties of a small, in nitely periodic, system and the macroscopic system which it represents are the same. is will depend both on the range of the intermolecular potential and the phenomenon under investigation. For a uid of

36 Introduction

DCB

E A 1

4

2

3

5

F GH

L
Fig. 1.13 A two-dimensional periodic system. Molecules can enter and leave each box across each of the four edges. In a three-dimensional example, molecules would be free to cross any of the six cube faces.
Lennard-Jones atoms it should be possible to perform a simulation in a cubic box of side L ≈ 6 σ without a particle being able to ‘sense’ the symmetry of the periodic la ice. If the potential is long range (i.e. v(r ) ∼ r −ν where ν is less than the dimensionality of the system) there will be a substantial interaction between a particle and its own images in neighbouring boxes, and consequently the symmetry of the cell structure is imposed on a
uid which is in reality isotropic. e methods used to cope with long-range potentials, for example in the simulation of charged ions (v(r ) ∼ r −1) and dipolar molecules (v(r ) ∼ r 3), are discussed in Chapter 5. We know that even in the case of short-range potentials the periodic boundary conditions can induce anisotropies in the uid structure (Mandell,
; Impey et al., ). ese e ects are pronounced for small system sizes (N = 100) and for properties such as the 2 light sca ering factor (see Chapter 2), which has a substantial long-range contribution. Pra and Haan ( ) have developed theoretical methods for investigating the e ects of boundary conditions on equilibrium properties.
e use of periodic boundary conditions inhibits the occurrence of long-wavelength uctuations. For a cube of side L, the periodicity will suppress any density waves with a wavelength greater than L. us, it would not be possible to simulate a liquid close

(a)

z

Studying small systems 37

(b)

z

y

x

x

y

Fig. 1.14 Non-cubic, space- lling, simulation boxes. (a) e truncated octahedron and its containing
cube; (b) the rhombic dodecahedron and its containing cube. e axes are those used in Code 1.4
and Code 1.5 of Section 1.6.4.
to the gas–liquid critical point, where the range of critical uctuations is macroscopic. Furthermore, transitions which are known to be rst order o en exhibit the characteristics of higher-order transitions when modelled in a small box, because of the suppression of
uctuations. Examples are the nematic–isotropic transition in liquid crystals (Luckhurst and Simpson, ) and the solid–plastic-crystal transition for N2 adsorbed on graphite (Mouritsen and Berlinsky, ). e same limitations apply to the simulation of longwavelength phonons in model solids, where in addition, the cell periodicity picks out a discrete set of available wavevectors (i.e. k = (nx , ny , nz )2π/L, where nx , ny , nz , are integers) in the rst Brillouin zone (Klein and Weis, ). Periodic boundary conditions have also been shown to a ect the rate at which a simulated liquid nucleates and forms a solid or glass when it is rapidly cooled (Honeycu and Andersen, ).
Despite the preceding remarks, the common experience in simulation work is that periodic boundary conditions have li le e ect on the equilibrium thermodynamic properties and structures of uids away from phase transitions and where the interactions are short-ranged. It is always sensible to check that this is true for each model studied. If the resources are available, it should be standard practice to increase the number of molecules (and the box size, so as to maintain constant density) and rerun the simulations.
e cubic box has been used almost exclusively in computer simulation studies because of its geometrical simplicity. Of the four remaining semi-regular space- lling polyhedra, the rhombic dodecahedron (Wang and Krumhansl, ), and the truncated octahedron (Adams, ; ) have also been studied. ese boxes are illustrated in Fig. 1.14. ey are more nearly spherical than the cube, which may be useful for simulating liquids, whose structure is spatially isotropic. In addition, for a given number density, the distance between periodic images is larger than in the cube. is property is useful in calculating distribution functions and structure factors (see Chapters 2 and 8). As we shall see in Section 1.6.4, they are only slightly more complicated to implement in simulations than cubic boxes.

38 Introduction
(a) Lz
(b)
Fig. 1.15 Periodic boundary conditions used in the simulation of adsorption (see e.g. Severin and Tildesley, ). (a) A side view of the box. ere is a re ecting boundary at height Lz . (b) A top view, showing the rhombic shape (i.e. the same geometry as the underlying graphite la ice). Periodic boundary conditions in this geometry are implemented in Code 1.6.
So far, we have tacitly assumed that there is no external potential, that is, no v1, term in eqns (1.4) and (1.5). If such a potential is present, then either it must have the same periodicity as the simulation box, or the periodic boundaries must be abandoned. In some cases, it is not appropriate to employ periodic boundary conditions in each of the three coordinate directions. In the simulation of CH4 on graphite (Severin and Tildesley, ) the simulation box, shown in Fig. 1.15, is periodic in the plane of the surface. In the z-direction, the graphite surface forms the lower boundary of the box, and the bulk of the adsorbate is in the region just above the graphite. Any molecule in the gas above the surface is con ned by reversing its velocity should it cross a plane at a height Lz above the surface. If Lz is su ciently large, this re ecting boundary will not in uence the behaviour of the adsorbed monolayer. In the plane of the surface, the shape of the periodic box is a rhombus of side L. is conforms to the symmetry of the underlying graphite. Similar boxes have been used in the simulation of the electrical double layer (Torrie and Valleau, ), of the liquid–vapour surface (Chapela et al., ), and of uids in small pores (Subramanian and Davis, ).

Studying small systems 39

DCB

E A 1

4

2

3

5

F GH

Fig. 1.16 e minimum image convention in a two-dimensional system. e central ‘box’ contains ve molecules. e dashed ‘box’ constructed with molecule 1 at its centre also contains ve molecules. e dashed circle represents the cuto .
1.6.3 Potential truncation
Now we must turn to the question of calculating properties of systems subject to periodic boundary conditions. e heart of the and programs involves the calculation of the potential energy of a particular con guration, and, in the case of , the forces acting on all molecules. Consider how we would calculate the force on molecule 1, or those contributions to the potential energy involving molecule 1, assuming pairwise additivity. We must include interactions between molecule 1 and every other molecule i in the simulation box. ere are N − 1 terms in this sum. However, in principle, we must also include all interactions between molecule 1 and images iA, iB, etc. lying in the surrounding boxes. is is an in nite number of terms, and of course is impossible to calculate in practice. For a short-range potential-energy function, we may restrict this summation by making an approximation. Consider molecule 1 to rest at the centre of a region which has the same size and shape as the basic simulation box (see Fig. 1.16). Molecule 1 interacts with all the molecules whose centres lie within this region, that is, with the closest periodic images of the other N − 1 molecules. is is called the ‘minimum

40 Introduction

image convention’: for example, in Fig. 1.16, molecule 1 could interact with molecules 2,

3D, 4E, and 5C. is technique, which is a natural consequence of the periodic boundary

conditions, was rst used in simulation by Metropolis et al. ( ).

In the minimum image convention, then, the calculation of the potential energy due

to

pairwise-additive

interactions

involves

1 2

N

(N

−

1)

terms.

is may still be a very

substantial calculation for a system of (say) 1000 particles. A further approximation

signi cantly improves this situation. e largest contribution to the potential and forces

comes from neighbours close to the molecule of interest, and for short-range forces we

normally apply a spherical cuto . is means se ing the pair potential v(r ) to zero for

r ≥ rc, where rc is the cuto distance. e dashed circle in Fig. 1.16 represents a cuto , and

in this case molecules 2, 4E and 5C contribute to the force on 1, since their centres lie inside

the cuto , whereas molecule 3D does not contribute. In a cubic simulation box of side L,

the number of neighbours explicitly considered is reduced by a factor of approximately

4πrc3/3L3, and this may be a substantial saving. e introduction of a spherical cuto should be a small perturbation, and the cuto distance should be su ciently large to

ensure this. As an example, in the simulation of Lennard-Jones atoms the value of the pair

potential at the boundary of a cuto sphere of typical radius rc = 2.5 σ is just 1.6 % of the

well depth. Of course, the penalty of applying a spherical cuto is that the thermodynamic

(and other) properties of the model uid will no longer be exactly the same as for (say)

the non-truncated, Lennard-Jones uid. As we shall see in Chapter 2, it is possible to

apply long-range corrections to such results so as to recover, approximately, the desired

information.

e cuto

distance

must

be

no

greater

than

1 2

L

for

consistency

with

the

minimum

image convention. In the non-cubic simulation boxes of Fig. 1.14, for a given density and

number of particles, rc may take somewhat larger values than in the cubic case. Looked

at another way, an advantage of non-cubic boundary conditions is that they permit

simulations with a given cuto distance and density to be conducted using fewer particles.

As

an

example,

a

simulation

in

a

cubic

box,

with rc

set

equal

to

1 2

L,

might

involve

N

=

256

molecules; taking the same density, the same cuto could be used in a simulation of 197

molecules in a truncated octahedron, or just 181 molecules in a rhombic dodecahedron.

1.6.4 Computer code for periodic boundaries

How do we handle periodic boundaries and the minimum image convention in a simulation

program? Let us assume that, initially, the N molecules in the simulation lie within a

cubic box of side L, with the origin at its centre, that is, all coordinates lie in the range

(−

1 2

L,

1 2

L).

As

the

simulation

proceeds,

these

molecules

move

about

the

in

nite periodic

system. When a molecule leaves the box by crossing one of the boundaries, it is usual

to switch a ention to the image molecule entering the box by simply adding L to, or

subtracting L from, the appropriate coordinate. One simple way to do this uses an IF

statement to test the positions immediately a er the molecules have been moved (whether

by or ). For example,

IF ( r(1,i) > box2 ) r(1,i) = r(1,i) - box IF ( r(1,i) < -box2 ) r(1,i) = r(1,i) + box

Studying small systems 41

where the rst index 1 selects the x coordinate. Similar statements are applied to the y and z coordinates, or a vector assignment may be applied to all components at once

WHERE ( r(:,i) > box2 ) r(:,i) = r(:,i) - box WHERE ( r(:,i) < -box2 ) r(:,i) = r(:,i) + box

Here,

box

is

a

variable

containing

the

box

length

L,

and

box2

is

just

1 2

L.

An

alternative

to the IF statement is to use arithmetic functions to calculate the correct number of box

lengths to be added or subtracted. For example,

r(:,i) = r(:,i) - box * ANINT ( r(:,i) / box )

e function ANINT(x) returns the nearest integer to x, converting the result back to type REAL; thus ANINT(-0.49) has the value 0.0, whereas ANINT(-0.51) is −1.0. In Fortran, this function returns an array-valued result, computed component by component, if given an array argument. As we shall see in Chapter 5, there are faster ways of coding this up, especially for large system sizes.
By using these methods, we always have available the coordinates of the N molecules that currently lie in the ‘central’ box. It is not strictly necessary to do this; we could, instead, use uncorrected coordinates, and follow the motion of the N molecules that were in the central box at the start of the simulation. Indeed, as we shall see in Chapters 2 and 8, for calculation of transport coe cients it may be most desirable to have a set of uncorrected positions on hand. If it is decided to do this, however, care must be taken that the minimum image convention is correctly applied, so as to work out the vector between the two closest images of a pair of molecules, no ma er how many ‘boxes’ apart they may be. is means, in general, adding or subtracting an integer number of box lengths (rather than just one box length).
e minimum image convention may be coded in the same way as the periodic boundary adjustments. Of the two methods just mentioned, the arithmetic formula is usually preferable, being simpler; the use of IF statements inside the inner loop may reduce program e ciency (see Appendix A). Immediately a er calculating a pair separation vector, the following statements should be applied:

rij(:) = rij(:) - box * ANINT ( rij(:) / box )

is code is guaranteed to yield the minimum image vector, no ma er how many ‘box

lengths’ apart the original images may be. For cuboidal, rather than cubic, boxes, the

variable box may be an array of three elements, holding the x, y, and z box lengths,

without essentially changing the code.

e calculation of minimum image distances is simpli ed by the use of reduced units:

the length of the box is taken to de ne the fundamental unit of length in the simulation.

By se

ing

L

=

1,

with

particle

coordinates

nominally

in

the

range

(−

1 2

,

+

1 2

),

the

minimum

image correction becomes

rij(:) = rij(:) - ANINT ( rij(:) )

which is simpler, and faster, than the code for a general box length. is approach is an alternative to the use of the pair potential to de ne reduced units as discussed in Appendix B, and is more generally applicable. For this reason a simulation box of unit length is adopted in most of the examples given in this book.

42 Introduction

Code 1.4 Periodic boundaries for truncated octahedron

is code snippet applies the truncated octahedron periodic boundary correction to a position vector ri , or equivalently the minimum image convention to a displacement vector rij , provided as the array r. e box is centred at the origin and the containing cube is of unit length (see Fig. 1.14(a)). e Fortran AINT function rounds towards zero, producing a real-valued integer result: for example AINT(-0.51) and AINT(0.51) both have the value 0.0, whereas AINT(-1.8) is −1.0. e result of the Fortran SIGN function has the absolute value of its rst argument and the sign of its second.

REAL , DIMENSION(3) :: r

REAL

:: corr

REAL , PARAMETER

:: r75 = 4.0 / 3.0

r(:) = r(:) - ANINT ( r(:) ) corr = 0.5 * AINT ( r75 * SUM ( ABS ( r(:) ) ) ) r(:) = r(:) - SIGN ( corr , r(:) )

Code 1.5 Periodic boundaries for rhombic dodecahedron

is code snippet applies the rhombic dodecahedron periodic boundary correction to a position vector ri , or equivalently the minimum image convention to a displacement vector rij , provided√as the array r. e box is centred at the origin and the side of the containing cube is 2 (see Fig. 1.14(b)).

REAL , DIMENSION(3) :: r

REAL

:: corr

REAL , PARAMETER :: rt2 = SQRT(2.0), rrt2 = 1.0 / rt2

r(1) = r(1) - ANINT ( r(1) ) r(2) = r(2) - ANINT ( r(2) ) r(3) = r(3) - rt2 * ANINT ( rrt2 * r(3) ) corr = 0.5 * AINT ( ABS(r(1)) + ABS(r(2)) + rt2*ABS(r(3)) ) r(1) = r(1) - SIGN ( corr , r(1) ) r(2) = r(2) - SIGN ( corr , r(2) ) r(3) = r(3) - SIGN ( corr , r(3) ) * rt2

ere are several alternative ways of coding the minimum image corrections, some of which rely on the images being in the same, central box (i.e. on the periodic boundary correction being applied whenever the molecules move). Some of these methods, for cubic boxes, are discussed in Appendix A. We have also mentioned the possibility of conducting simulations in non-cubic periodic boundary conditions. An implementation of the minimum image correction for the truncated octahedron (Adams, a) is given

Studying small systems 43

Code 1.6 Periodic boundaries for rhombus

Here we apply corrections for the rhombic box in two dimensions x, y. In most applications the molecules will be con ned in the z direction by real walls rather than by periodic boundaries, so we assume that this coordinate may be le unchanged.
e box is centred at the origin. e x axis lies along one side of the rhombus, which is of unit length (see Fig. 1.15). e acute angle of the rhombus is 60°.

REAL , DIMENSION(3) :: r

REAL , PARAMETER

:: rt3 = SQRT(3.0), rrt3 = 1.0 / rt3

REAL , PARAMETER

:: rt32 = rt3 / 2.0, rrt32 = 1.0 / rt32

r(1) = r(1) - ANINT ( r(1) - rrt3 * r(2) ) &

&

- ANINT ( rrt32 * r(2) ) * 0.5

r(2) = r(2) - ANINT ( rrt32 * r(2) ) * rt32

in Code 1.4. A similar correction for the rhombic dodecahedron (Smith, ) appears in Code 1.5. is is a li le more complicated than the code for the truncated octahedron, and the gain small, so that the la er is usually preferable. We also give in Code 1.6 the code for the two-dimensional rhombic box o en used in surface simulation.
Now we turn to the implementation of a spherical cuto , that is, we wish to set the pair potential (and all forces) to zero if the pair separation lies outside some distance rc. It is easy to compute the square of the particle separation rij and, rather than waste time taking the square root of this quantity, it is fastest to compare this with the square of rc which might be computed earlier and stored in a variable r_cut_sq. A er computing the minimum image intermolecular vector, the following statements would be employed:
rij_sq = SUM ( rij(:) ** 2 ) IF ( rij_sq < r_cut_sq ) THEN
... compute i-j interaction ... END IF
In a large system, it may be worthwhile to apply separate tests for the x, y, and z directions or some similar scheme.
IF ( ABS ( rij(1) ) < r_cut ) THEN IF ( ABS ( rij(2) ) < r_cut ) THEN IF ( ABS ( rij(3) ) < r_cut ) THEN rij_sq = SUM ( rij(:) ** 2 ) IF ( rij_sq < r_cut_sq ) THEN ... compute i-j interaction ... END IF END IF END IF
END IF

44 Introduction
e time saved in dropping out of this part of the program at any early stage must be weighed against the overheads of extra calculation and testing. In Chapter 5 we discuss the more complicated time-saving tricks used in the simulations of large systems.
1.6.5 Spherical boundary conditions
As an alternative to the standard periodic boundary conditions for simulating bulk liquids, a two-dimensional system may be embedded in the surface of a sphere without introducing any physical boundaries (Hansen et al., ), and the idea may be extended to consider a three-dimensional system as being the surface of a hypersphere (Kratky, ; Kratky and Schreiner, ). e spherical or hyperspherical system is nite: it cannot be considered as part of an in nitely repeating periodic system. In this case, non-Euclidean geometry is an unavoidable complication, and distances between particles are typically measured along the great circle geodesics joining them. However, the e ects of the curved geometry will decrease as the system size increases, and such ‘spherical boundary conditions’ are expected to be a valid method of simulating bulk liquids. Interesting di erences from the standard periodic boundary conditions, particularly close to any solid–liquid phase transition, will result from the di erent topology. Periodic boundaries will be biased in favour of the formation of a solid with a la ice structure which matches the simulation box. Spherical boundaries, on the other hand, are not consistent with periodic la ices, so the liquid state will be thermodynamically favoured in most simulations using this technique, and crystalline phases will inevitably contain defects. Similar considerations may apply to liquid-crystalline phases.
1.6.6 Periodic boundary conditions for three-body potentials
Finally, we note that some care is required when using the minimum image convention with three-body potentials such as the Axilrod–Teller potential (see Appendix C). is problem is illustrated in Fig. 1.17. In Fig. 1.17(a), atom 1 is at the centre of its box, of side L, and atoms 2 and 3E are the two minimum images used in the calculation of the pair potential. However atom 3 is the minimum image of atom 2 and a straightforward application of the minimum image algorithm will lead to the incorrect triplet 123 rather than 123E.
A ard ( ) has shown that this problem can be solved using the following statements for the separation vector
REAL , DIMENSION (3) :: rij , rik , rjk , tij , tik tij(:) = box * ANINT ( rij(:) / box ) tik(:) = box * ANINT ( rik(:) / box ) rij(:) = rij(:) - tij(:) rik(:) = rik(:) - tik(:) rjk(:) = rjk(:) + tij(:) - tik(:)
Normally the three-body potential is set to zero if one side of the triangle is greater than L/2.
Some workers have taken a more brute-force approach (Sadus and Prausnitz, ; Marcelli and Sadus, ). If the potential cuto rc is set to L/4, the only triplets that contribute to the potential are those where all of the three atoms are within a box of side

(a)

E3E 1

2

3

Studying small systems 45

(b)

E 3E

2

3

1

L

L/2

Fig. 1.17 Periodic boundary conditions and the minimum image convention for a triplet interaction: (a) an inconsistency in the triplet con guration for a cuto of L/2; (b) a consistent triplet with a cuto of L/4.
L/2 (as shown in Fig. 1.17(b)). Each of the atoms is then always the unique minimum image of the other two and the triplet is unambiguously determined with the normal minimum image calculation. is method works well. However, at a xed density the simulation will need to include eight times as many atoms in circumstances where the additional calculation of the three-body force is particularly expensive.

2 Statistical mechanics

Computer simulation generates information at the microscopic level (atomic and molecular positions, velocities, etc.) and the conversion of this very detailed information into macroscopic terms (pressure, internal energy, etc.) is the province of statistical mechanics. It is not our aim to provide a text in this eld since many excellent sources are available (Hill, ; Mc arrie, ; Landau and Lifshitz, ; Friedman, ; Chandler, ; Tuckerman, ; Swendsen, ; Hansen and McDonald, ). In this chapter, our aim is to summarize those aspects of the subject which are of most interest to the computer simulator.

2.1 Sampling from ensembles

Let us consider, for simplicity, a one-component macroscopic system; extension to a multicomponent system is straightforward. e thermodynamic state of such a system is usually de ned by a small set of parameters (such as the number of particles N , the temperature T , and the pressure P). Other thermodynamic properties (density ρ, chemical potential µ, heat capacity CV , etc.) may be derived through knowledge of the equations of state and the fundamental equations of thermodynamics. Even quantities such as the di usion coe cient D, the shear viscosity η, and the structure factor S (k ) are state functions: although they clearly say something about the microscopic structure and dynamics of the system, their values are completely dictated by the few variables (e.g. N PT ) characterizing the thermodynamic state, not by the very many atomic positions and momenta that de ne the instantaneous mechanical state. ese positions and momenta can be thought of as coordinates in a multidimensional space: phase space. For a system of N atoms, this space has 6N dimensions. Let us use the abbreviation Γ for a particular point in phase space, and suppose that we can write the instantaneous value of some property A (it might be the potential energy) as a function A(Γ). e system evolves in time so that Γ, and hence A(Γ) will change. It is reasonable to assume that the experimentally observable ‘macroscopic’ property Aobs is really the time average of A (Γ) taken over a long time interval:

Aobs = A time = A Γ(t )

time

=

lim
t obs →∞

1 tobs

tobs
A Γ(t ) dt .
0

(2.1)

e equations governing this time evolution, Newton’s equations of motion in a simple classical system, are of course well known. ey are just a system of ordinary di erential

Computer Simulation of Liquids. Second Edition. M. P. Allen and D. J. Tildesley. © M. P. Allen and D. J. Tildesley 2017. Published in 2017 by Oxford University Press.

Sampling from ensembles 47

equations: solving them on a computer, to a desired accuracy, is a practical proposition

for, say, 105 particles, although not for a truly macroscopic number (e.g. 1023). So far

as the calculation of time averages is concerned, we clearly cannot hope to extend the

integration of eqn (2.1) to in nite time, but might be satis ed to average over a long

nite time τobs. is is exactly what we do in a molecular dynamics simulation. In fact,

the equations of motion are usually solved on a step-by-step basis, that is, a large nite

number τobs of timesteps, of length δt = tobs/τobs, are taken. In this case, we may rewrite

eqn (2.1) in the form

Aobs =

A

=

1

τobs
A Γ(t ) .

time τobs τ =1

(2.2)

In the summation, τ simply stands for an index running over the succession of timesteps. is analogy between the discrete τ and the continuous t is useful, even when, as we shall
see in other examples, τ does not correspond to the passage of time in any physical sense. e practical questions regarding the method are whether or not a su cient region
of phase space is explored by the system trajectory to yield satisfactory time averages within a feasible amount of computer time, and whether thermodynamic consistency can be a ained between simulations with identical macroscopic parameters (density, energy, etc.) but di erent initial conditions (atomic positions and velocities). e answers to these questions are that such simulation runs are indeed within the power of modern computers, and that thermodynamically consistent results for liquid state properties can indeed be obtained, provided that a ention is paid to the selection of initial conditions. We will turn to the technical details of the method in Chapter 3.
e calculation of time averages by is not the approach to thermodynamic properties implicit in conventional statistical mechanics. Because of the complexity of the

time evolution of A Γ(t ) for large numbers of molecules, Gibbs suggested replacing the time average by the ensemble average. Here, we regard an ensemble as a collection of points Γ in phase space. e points are distributed according to a probability density ρ (Γ).
is function is determined by the chosen xed macroscopic parameters (N PT , NVT , etc.), so we use the notation ρN PT , ρNVT , or, in general, ρens. Each point represents a typical system at any particular instant of time. Each system evolves in time, according to the usual mechanical equations of motion, quite independently of the other systems. Consequently, in general, the phase space density ρens(Γ) will change with time. However, no systems are destroyed or created during this evolution, and Liouville’s theorem, which is essentially a conservation law for probability density, states that dρ/dt = 0 where d/dt denotes the total derivative with respect to time (following a state Γ as it moves). As an example, consider a set of N atoms with Cartesian coordinates ri , and momenta pi , in the classical approximation. e total time derivative is

d dt

=

∂ ∂t

+

i

r˙i · ∇ri +

i

=

∂ ∂t

+ r˙

· ∇r

+

p˙ · ∇p.

p˙ i · ∇pi

(2.3a) (2.3b)

In eqn (2.3a), ∂/∂t represents di erentiation, with respect to time, of a function; ∇ri , and ∇pi , are derivatives with respect to atomic position and momentum respectively; and r˙i ,

48 Statistical mechanics
p˙ i , signify the time derivatives of the position and momentum. Equation (2.3b) is the same equation wri en in a more compact way, and the equation may be further condensed by de ning the Liouville operator L

iL =

r˙i · ∇ri + p˙ i · ∇pi = r˙ · ∇r + p˙ · ∇p

(2.4)

i

i

so that d/dt = ∂/∂t + iL and, using Liouville’s theorem, we may write

∂ρens (Γ, t ) ∂t

=

−iLρens (Γ, t ).

(2.5)

is equation tells us that the rate of change of ρens at a particular xed point in phase space is related to the ows into and out of that point. is equation has a formal solution

ρens (Γ, t ) = exp(−iLt ) ρens (Γ, 0)

(2.6)

where the exponential of an operator really means a series expansion

exp(−iLt )

=

1

−

iLt

−

1 2

L2t

2

+

·

·

·

.

(2.7)

e equation of motion of a function like A (Γ), which does not depend explicitly on time, takes a conjugate form (Mc arrie, ):

A˙ Γ(t ) = iLA Γ(t )

(2.8)

or

A Γ(t ) = exp(iLt )A Γ(0) .

(2.9)

To be quite clear: in eqns (2.5) and (2.6) we consider the time-dependence of ρens at a xed
point Γ in phase space; in eqns (2.8) and (2.9), A Γ(t ) is time-dependent because we are following the time evolution Γ(t ) along a trajectory. is relationship is analogous to that between the Schro¨dinger and Heisenberg pictures in quantum mechanics.
If ρens(Γ) represents an equilibrium ensemble, then its time-dependence completely vanishes, ∂ρens/∂t = 0. e system evolution then becomes quite special. As each system leaves a particular state Γ(τ ) and moves on to the next, Γ(τ + 1), another system arrives from state Γ(τ − 1) to replace it. e motion resembles a long and convoluted conga line at a crowded party (see Fig. 2.1). ere might be several such processions, each passing through di erent regions of phase space. However, if these are all connected into just one trajectory that passes through all the points in phase space for which ρens is non-zero (i.e. the procession forms a single, very long, closed circuit) then each system will eventually visit all the state points. Such a system is termed ‘ergodic’ and the time taken to complete a cycle (the Poincare´ recurrence time) is immeasurably long for a many-particle system (and for many parties as well it seems).
One way of answering the question ‘was it a good party?’ would be to interview one of the participants, and ask for their time-averaged impressions. is is essentially what we do in a molecular dynamics simulation, when a representative system evolves deterministically in time. However, as indicated in Fig. 2.1, this time average might not

Sampling from ensembles 49

Fig. 2.1 A schematic representation of phase space. e circles represent di erent state points (q, p), and they are connected by a path representing the classical trajectory, analogous to a conga line at a party. Each state is characterized by some property (e.g. ‘happiness’ at the party). In an ergodic system, the single long trajectory would eventually pass through (or arbitrarily near) all states; in the bo om le corner of the diagram we symbolically indicate a disconnected region of six states which may or may not be practically important.
be representative of the whole trajectory: to be sure, it would have to be long enough to sample all the states. An alternative route to the average properties of our partygoers, would be to take photographs of all of them at the same time, assemble the complete collection of ‘happy’ and ‘sad’ faces, and take an average over them. is corresponds to replacing the time average in eqn (2.1) by an average taken over all the members of the ensemble, ‘frozen’ at a particular time:

Aobs = A ens = A ρens = A (Γ)ρens (Γ).
Γ

(2.10)

e A|ρ notation reminds us of the dependence of the average on both A and ρ: this is important when taking a thermodynamic derivative of Aobs (we must di erentiate both parts) or when considering time-dependent properties (when the Schro¨dinger–Heisenberg analogy may be exploited). Actually, we will be concerned with the practical question of e cient and thorough sampling of phase space, which is not quite the same as the rigorous de nition of ergodicity (for a fuller discussion, see Tolman, ). In terms of our analogy of conga lines, there should not be a preponderance of independent closed circuits (‘cliques’) in which individuals can become trapped and fail fully to sample the available space (this is important in parties as well as in simulations). An simulation which started in the disconnected six-state region of Fig. 2.1, for example, would be disastrous. On the other hand, small non-ergodic regions are less likely to be dangerous and more likely to be recognized if they are unfortunately selected as starting points for

50 Statistical mechanics

a simulation. In a similar way, regions of phase space which act as barriers and cause bo lenecks through which only a few trajectories pass can result in poor sampling by the relatively short simulation runs carried out in practice, even if the system is technically ergodic.
Finally, we might use a di erent kind of evolution to sample the states of the system: a random walk. is is the Monte Carlo approach: it may be more or less e cient than molecular dynamics. It also ts quite well the analogy of a party, in which the participants sample the di erent situations randomly, rather than systematically. Once again, trajectory averages are calculated over a nite duration, so these are not necessarily identical to full ensemble averages, and the approach might or might not alleviate some of the ergodicity issues.
It is sometimes convenient to use, in place of ρens(Γ), a ‘weight’ function wens(Γ), which satis es the following equations:

ρens (Γ)

=

Q

−1 ens

wens (Γ)

Qens = wens (Γ)
Γ

A ens = wens (Γ)A (Γ)
Γ

wens .
Γ

(2.11) (2.12)
(2.13)

e weight function is essentially a non-normalized form of ρens(Γ), with the partition function Qens (also called the sum over states) acting as the normalizing factor. Both wens and Qens contain an arbitrary multiplicative constant, whose choice corresponds to the de nition of a zero of entropy. Qens is simply a function of the macroscopic properties de ning the ensemble, and connection with classical thermodynamics is made by de ning
a thermodynamic potential Ψens (see e.g. Mc arrie, )

Ψens = − ln Qens.

(2.14)

is is the function that has a minimum value at thermodynamic equilibrium. For example, Ψens might be the negative of the entropy S for a system at constant NV E, where V is the volume and E the total internal energy, or the Gibbs function G for a constant-N PT system, where P is the pressure and T the temperature.
roughout the foregoing discussion, although we have occasionally used the language of classical mechanics, we have assumed that the states are discrete (e.g. a set of quantum numbers) and that we may sum over them. If the system were enclosed in a container, there would be a countably in nite set of quantum states. In the classical approximation, Γ represents the set of (continuously variable) particle positions and momenta, and we should replace the summation by a classical phase-space integral. wens and Qens are then usually de ned with appropriate factors included to make them dimensionless, and to match up with the usual semiclassical ‘coarse-grained’ phase-space volume elements. On a computer, of course, all numbers are held to a nite precision and so, technically, positions and momenta are represented by discrete, not continuous, variables; we now have a countable and nite set of states. We assume that the distinction between this case and the classical limit is of no practical importance, and will use whichever representation is most convenient.

Sampling from ensembles 51

One conceivable approach to the computation of thermodynamic quantities, therefore, would be a direct evaluation of Qens for a particular ensemble, using eqn (2.12). is summation, over all possible states, is not feasible for many-particle systems: there are too many states, most of which have a very low weight due to non-physical overlaps between the repulsive cores of the molecules, rendering them unimportant. We would like to conduct the summation so as to exclude this large number of irrelevant states, and include only those with a high probability. Unfortunately, it is generally not possible to estimate Qens directly in this way. However, the underlying idea, that of generating (somehow) a set of states in phase space that are sampled from the complete set in accordance with the probability density ρens(Γ), is central to the Monte Carlo technique.
We proceed by analogy with molecular dynamics in the sense that the ensemble average of eqn (2.13) is replaced by a trajectory average like eqn (2.2). Newton’s equations generate a succession of states in accordance with the distribution function ρNV E for the constant-NV E or microcanonical ensemble. Suppose we wish to investigate other ensembles; experiments in the laboratory, for example, are frequently performed under conditions of constant temperature and pressure, while it is o en very convenient to consider inhomogeneous systems at constant chemical potential. For each such case, let us invent a kind of equation of motion, that is, a means of generating, from one state point Γ(τ ), a succeeding state point Γ(τ + 1). is recipe need have no physical interpretation, and it could be entirely deterministic or could involve a stochastic, random, element. It might be derived by modifying the true equations of motion in some way, or it may have no relation whatever with normal dynamics.
To be useful, this prescription should satisfy some sensible conditions:
(a) the probability density ρens(Γ) for the ensemble of interest should not change as the system evolves;
(b) any ‘reasonable’ starting distribution ρ (Γ) should tend to this stationary solution as the simulation proceeds;
(c) we should be able to argue that ergodicity holds, even though we cannot hope to prove this for realistic systems.
If these conditions are satis ed, then we should be able to generate, from an initial state, a succession of state points which, in the long term, are sampled in accordance with the desired probability density ρens(Γ). In these circumstances, the ensemble average will be equal to a kind of ‘time average’:

Aobs =

A

ens

=

1 τobs

τobs
A
τ =1

Γ(τ )

.

(2.15)

Here τ is an index running over the succession of τobs states or trials generated by our prescription; in a practical simulation, τobs would be a large nite number. is is exactly what we do in Monte Carlo simulations. e trick, of course, lies in the generation of the trajectory through phase space, and the di erent recipes for di erent ensembles will be discussed in Chapter 4. In general, because only a nite number of states can be generated in any one simulation, Monte Carlo results are subject to the same questions of initial condition e ects and satisfactory phase space exploration as are molecular dynamics results.

52 Statistical mechanics

2.2 Common statistical ensembles
Let us consider four ensembles in common use: the microcanonical, or constant-NV E, ensemble just mentioned, the canonical, or constant-NVT , ensemble, the isothermal– isobaric constant-N PT ensemble, and the grand canonical constant-µVT ensemble. For each ensemble, the aforementioned thermodynamic variables are speci ed, that is, xed. Other thermodynamic quantities must be determined by ensemble averaging and, for any particular state point, the instantaneous values of the appropriate phase function will deviate from this average value, that is, uctuations occur.
e probability density for the microcanonical ensemble is proportional to

δ [H (Γ) − E]

where Γ represents the set of particle positions and momenta (or quantum numbers), and H (Γ) is the Hamiltonian. e delta function selects those states of an N -particle system in a container of volume V that have the desired energy E. When the set of states is discrete, δ is just the Kronecker delta, taking values of 0 or 1; when the states are continuous, δ is the Dirac delta function. e microcanonical partition function may be wri en:

QNV E = δ [H (Γ) − E]
Γ

(2.16)

where the summation takes due note of indistinguishability of particles. In the quasiclassical expression for QNV E , for an atomic system, the indistinguishability is handled using a factor of 1/N !

QNV E

=

11 N ! h3N

dr dp δ [H (r, p) − E].

(2.17)

Here, dr dp stands for integration over all 6N phase space coordinates. e appropriate thermodynamic potential is the negative of the entropy

−S/kB = − ln QNV E .

(2.18)

e factor involving Planck’s constant h in eqn (2.17) corresponds to the usual zero of entropy for the ideal gas (the Sackur–Tetrode equation).
For a classical system, Newton’s equations of motion conserve energy and so provide a suitable method (but not the only method (Severin et al., ; Creutz, )) for generating a succession of state points sampled from this ensemble, as discussed in the previous section. In fact, for a system not subjected to external forces, these equations also conserve total linear momentum P, and so molecular dynamics probes a subset of the microcanonical ensemble, namely the constant-NV EP ensemble (for technical reasons, as we shall see in Chapter 3, total angular momentum is not conserved in most simulations). Since it is easy to transform into the centre-of-mass frame, the choice of P is not crucial, and zero momentum is usually chosen for convenience. Di erences between the constant-NV E and constant-NV EP ensembles are minor: for the la er, an additional three constraints exist in that only (N − 1) particle momenta are actually independent of each other.

Common statistical ensembles 53

e probability density for the canonical ensemble is proportional to exp[−H (Γ)/kBT ]
and the partition function is

QNVT = exp[−H (Γ)/kBT ]
Γ
or, in quasi-classical form, for an atomic system

(2.19)

QNVT

=

11 N ! h3N

dr dp exp[−H (r, p)/kBT ].

e appropriate thermodynamic function is the Helmholtz free energy A

(2.20)

A/kBT = − ln QNVT .

(2.21)

In the canonical ensemble, all values of the energy are allowed, and energy uctuations are non-zero. us, although ρNVT (Γ) is indeed a stationary solution of the Liouville equation, the corresponding mechanical equations of motion are not a satisfactory method of sampling states in this ensemble, since they conserve energy: normal time evolution occurs on a set of independent constant-energy surfaces, each of which should be appropriately weighted, by the factor exp[−H (Γ)/kBT ]. Our prescription for generating a succession of states must make provision for transitions between the energy surfaces, so that a single trajectory can probe all the accessible phase space, and yield the correct relative weighting. We shall encounter several ways of doing this in the later chapters.
Because the energy is always expressible as a sum of kinetic (p-dependent) and potential (q-dependent) contributions, the partition function factorizes into a product of kinetic (ideal gas) and potential (excess) parts

QNVT

=

11 N ! h3N

dp exp(−K /kBT ) dq exp(−V/kBT ) = QNidVT QNexVT .

Again, for an atomic system, we see (by taking V = 0)

(2.22)

Q

id NV

T

=

VN N ! Λ3N

Λ being the thermal de Broglie wavelength

Λ = (h2/2πmkBT )1/2.

(2.23) (2.24)

e excess part is

Q

ex NV

T

= V −N

dr exp[−V (r)/kBT ].

Instead

of

Q

ex NV

T

,

we

o

en use the con

guration integral

(2.25)

ZNVT = dr exp[−V (r)/kBT ].

(2.26)

Some workers include are dimensionless, the

a factor N ! in con guration

the de nition of ZNVT . Although Q integral has dimensions of V N . As

id NVT

and

Q

ex NV

T

a consequence

54 Statistical mechanics

of the separation of QNVT , all the thermodynamic properties derived from A can be expressed as a sum of ideal gas and con gurational parts. In statistical mechanics, it is easy to evaluate ideal gas properties (Rowlinson, ), and we may expect most a ention to focus on the con gurational functions. In fact, it proves possible to probe just the con gurational part of phase space according to the canonical distribution, using standard Monte Carlo methods. e corresponding trajectory through phase space has essentially independent projections on the coordinate and momentum sub-spaces. e ideal gas properties are added onto the results of con guration-space Monte Carlo simulations a erwards.
e probability density for the isothermal–isobaric ensemble is proportional to

exp[−(H + PV )/kBT ].
Note that the quantity appearing in the exponent, when averaged, gives the thermodynamic enthalpy H = H + P V . Now the volume V has joined the list of microscopic quantities (r and p) comprising the state point. e appropriate partition function is

QNPT =

exp[−(H + PV )/kBT ] = exp(−PV /kBT )QNVT .

ΓV

V

(2.27)

e summation over possible volumes may also be wri en as an integral, in which case some basic unit of volume V0 must be chosen to render QN PT dimensionless. is choice is not practically important for our purposes, but has been discussed in detail elsewhere (Wood, b; A ard, ; Koper and Reiss, ; Corti and Soto-Campos, ; Han and Son, ). In quasi-classical form, for an atomic system, we write:

QN PT

=

11 N ! h3N

1 V0

dV

dr dp exp[−(H + PV )/kBT ].

e corresponding thermodynamic function is the Gibbs free energy G

(2.28)

G/kBT = − ln QN PT .

(2.29)

e prescription for generating state points in the constant-N PT ensemble must clearly provide for changes in the sample volume as well as energy. Once more, it is possible to separate con gurational properties from kinetic ones, and to devise a Monte Carlo procedure to probe con guration space only. e con guration integral in this ensemble is

ZN PT = dV exp(−PV /kBT ) dr exp[−V (r)/kBT ].

(2.30)

Again some de nitions include N ! and V0 as normalizing factors. e density function for the grand canonical ensemble is proportional to

exp[−(H − µN )/kBT ]
where µ is the speci ed chemical potential. Now the number of particles N is a variable, along with the coordinates and momenta of those particles. e grand canonical partition function is

QµVT =

exp[−(H − µN )/kBT ] = exp(µN /kBT )QNVT .

NΓ

N

(2.31)

Common statistical ensembles 55

In quasi-classical form, for an atomic system,

QµVT =

N

11 N ! h3N

exp(µN /kBT )

dr dp exp(−H /kBT ).

(2.32)

Although it is occasionally useful to pretend that N is a continuous variable, for most purposes we sum, rather than integrate, in eqns (2.31) and (2.32). e appropriate thermodynamic function is just −PV /kBT :

−PV /kBT = − ln QµVT .

(2.33)

Whatever scheme we employ to generate states in the grand ensemble, clearly it must allow for addition and removal of particles. Once more, it is possible to invent a Monte Carlo method to do this and, moreover, to probe just the con gurational part of phase space; however, it turns out to be necessary to include the form of the kinetic partition function in the prescription used.
So far the discussion has been limited to one-component systems but each of the ensembles considered can be readily extended to multi-component mixtures. For example in the grand ensemble, the density function for a c-component mixture containing Ni particles of type i is proportional to

 

c

exp µi Ni /kBT


  i=1

Ni !



 

exp(−H

/kBT

)









(2.34)

where µi is the chemical potential of species i and H is the Hamiltonian of the c-component mixture. In quasi-classical form, for an atomic system, the grand partition function is

Qµ1, µ2, ...µnV T

=

N1, N2 ...Nn

1 h3N

     

c i =1

1 Ni !

 exp(µi Ni /kBT )
  

e appropriate thermodynamic function is

dr dp exp(−H /kBT ). (2.35)

−PV /kBT = − ln Qµ1, µ2, ...µnV T .

(2.36)

It is also useful to study mixtures in the semi-grand ensemble (Ko e and Glandt, ). Here the total number of particles is xed at N but the identities of the individual particles can change. e chemical potential of an arbitrary species, say 1, is de ned as µ1 and the c − 1 chemical potential di erences, (µ2 − µ1) . . . (µn − µ1), are xed. When V and T are also xed, the probability density is proportional to

c



 

exp[(µi − µ1)Ni /kBT ] exp(−H /kBT )





 i=1







where H is the Hamiltonian for a system of N particles (N = i Ni ) and where each particle is de ned to have a speci c identity from 1 to c. In quasi-classical form, for an

56 Statistical mechanics atomic system, the semi-grand partition function is

Q {µi |i 1}NVT =

c

c

···

i1=1 iN =1

1 N !h3N

     

c i =1

exp

(µi − µ1)Ni /kBT

     

dr dp exp(−H /kBT ) (2.37)

where the sums are now over the particle identities (e.g. i1 is the identity of particle 1). e corresponding thermodynamic potential is

−(PV − µ1N )/kBT = − ln Q {µi |i 1}N V T .

(2.38)

It is also possible to develop a semi-grand ensemble at constant pressure rather than constant volume. In this case, the chemical potential di erence is conveniently replaced by the fugacity fraction as the independent variable, as discussed in Section 4.7 (Ko e and Glandt, ; Frenkel and Smit, ). An important advantage of the semi-grand ensemble is that the chemical potential di erence can be de ned as a continuous function. For example, in a polydisperse uid of hard spheres, the distribution µ (σ ) −µ1 as a function of the hard-sphere diameter σ would be xed, and through the semi-grand ensemble we could predict the distribution of particle sizes.
It is possible to construct many more ensembles, some of which are of interest in computer simulation. When comparing molecular dynamics with Monte Carlo, it may be convenient to add the constraint of constant (zero) total momentum, that is, xed centre of mass, to the constant-NVT ensemble. It is also permissible to constrain certain degrees of freedom (e.g. the total kinetic energy (Hoover, b,a), or the energy in a particular chemical bond (Freasier et al., )) while allowing others to uctuate. Also, nonequilibrium ensembles may be set up (see Chapter 11). e possibilities are endless, the general requirements being that a phase-space density ρens(Γ) can be wri en down, and that a corresponding prescription for generating state points can be devised. e remaining questions are ones of practicality.
Not all ensembles are of interest to the computer simulator. e properties of generalized ensembles, such as the constant-µPT ensemble, have been discussed (Hill, ). Here, only intensive parameters are speci ed: the corresponding extensive quantities show unbounded uctuations, that is, the system size can grow without limit. Also, µ, P, and T are related by an equation of state, so, although this equation may be unknown, they are not independently variable. For these reasons, the simulation of the constantµPT ensemble and related pathological examples is not a practical proposition. In all the ensembles dealt with in this book, at least one extensive parameter (usually N or V ) is
xed to act as a limit on the system size. Finally, it is by no means guaranteed that a chosen prescription for generating phase-
space trajectories will correspond to any ensemble at all. It is easy to think of extreme examples of modi ed equations of motion for which no possible function ρens(Γ) is a stationary solution. In principle, some care should be taken to establish which ensemble, if any, is probed by any novel simulation technique.

Common statistical ensembles 57
Example 2.1 Entropy and disorder
At the very least, molecular simulations provide an experimental route to check the predictions of statistical mechanics, as indicated in Fig. 1.2. In addition, however, simulations have provided the impetus to revise some basic ideas, especially in connection with the concept of entropy. Some of the earliest simulations (Wood and Jacobson, ; Alder and Wainwright, ) demonstrated that the hard-sphere system exhibited a phase transition between solid and liquid phases. Because there are no energetic terms in this model, the thermodynamic driving force must be the entropy: at su ciently high density, the ordered, solid, phase has a higher entropy than the liquid. is result was not immediately accepted, as it required a rethinking of the de nition of disorder, and its connection to the entropy. Roughly speaking, the loss in entropy, associated with the localization of particles around positions on a regular la ice, is more than compensated by the entropy gain associated with the increased free volume that may be explored by each particle around its la ice site. On compressing a disordered hard-sphere system, when the volume occupied by the spheres reaches η ≈ 64 %, the free volume becomes zero (random close packing); the freezing transition occurs before this, at η ≈ 49 % (for comparison, in the structure, close packing occurs at η ≈ 74 %). Recent computer simulations of polyhedral hard particles have shown that shape-related entropic e ects alone can give rise to a huge variety of solid structures (Damasceno et al., ; van Anders et al., ). A more fundamental debate concerns the factor N ! appearing in eqns (2.17), (2.20), and usually associated with particle indistinguishability. In quantum mechanics, identical particles are indistinguishable as a ma er of principle. However, our simulations are of classical, distinguishable (i.e. labelled) particles! Also, statistical mechanics is applied successfully to colloidal systems, for which the constituent particles are of mesoscopic size and clearly distinguishable, even when nearly monodisperse. Should the factor N ! be included or not? e question arises whenever two systems, under identical conditions, in a classical simulation or a colloidal experiment, are brought into contact and allowed to exchange particles: is the entropy additive (i.e. extensive) or is there an entropy of mixing term? e answer is that the factor N ! should be present, for N very similar but distinguishable particles, in order to obtain extensive entropy and Helmholtz free energy functions, and hence it is not intimately connected with quantum mechanics. is has been discussed in the context of simulations by Swendsen ( ) and for colloidal systems by Warren ( ) and Swendsen ( ) (see also Frenkel, , and references therein). ese considerations come from rst principles, not computer experiments. However, interestingly, they are crucial in practical a empts to quantify the entropy of a granular system, in terms of the number of ways of realizing a jammed structure, extending ideas of Edwards and Oakesho ( ) and Edwards ( ). Computer simulations (Asenjo et al., ) involving the preparation of jammed con gurations by quenching equilibrated nearly-hard-sphere liquids, have con rmed the need to include the N ! term in order to de ne an extensive granular entropy, even for systems of distinguishable particles.

58 Statistical mechanics
2.3 Transforming between ensembles
Since the ensembles are essentially arti cial constructs, it would be reassuring to know that they produce average properties which are consistent with one another. In the thermodynamic limit (for an in nite system size) and as long as we avoid the neighbourhood of phase transitions, this is believed to be true for the commonly used statistical ensembles (Fisher, ). Since we will be dealing with systems containing a nite number of particles, it is of some interest to see, in a general way, how this result comes about. e method of transformation between ensembles is standard (Hill, ; Lebowitz et al., ; Mu¨nster, ; Landau and Lifshitz, ) and a useful summary for several ensembles of interest has appeared (Graben and Ray, ). We merely outline the procedure here; nonetheless, the development is rather formal, and this section could be skipped on a rst reading.
We shall be interested in transforming from an ensemble in which an extensive thermodynamic variable F is xed to one in which the intensive conjugate variable f is constant. Typical conjugate pairs are (β, E), (βP, V ), (−β µ, N ), where β = 1/kBT . If the old partition function and characteristic thermodynamic potential are QF , and ΨF , respectively, then the new quantities are given by

Qf = dF exp(−F f )QF

(2.39)

Ψf = ΨF + F f .

(2.40)

Equations (2.19)–(2.33) provide speci c examples of these relations. Equation (2.40) corresponds to the Legendre transformation of classical thermodynamics. For example, when moving from a system at constant energy to one at constant temperature (i.e. constant β), the characteristic thermodynamic potential changes from −S/kB to −S/kB + βE = βA. Similarly, on going to constant temperature and pressure, the thermodynamic potential becomes βA + βPV = βG.
e average A f calculated in the constant-f ensemble is related to the average A F calculated at constant F by (Lebowitz et al., )

A f = exp(Ψf ) dF exp(−ΨF − F f ) A F .

(2.41)

e equivalence of ensembles relies on the behaviour of the integrand of this equation
for a large system: it becomes very sharply peaked around the mean value F = F f . In the thermodynamic limit of in nite system size, we obtain simply

Af = AF

(2.42)

where it is understood that F = F f . us, the averages of any quantity calculated in, say, the constant-NV E ensemble and the constant-NVT ensemble, will be equal in the
thermodynamic limit, as long as we choose E and T consistently so that E = E NVT . In

Transforming between ensembles 59

fact, there are some restrictions on the kinds of functions A for which eqn (2.42) holds. A should be, essentially, a sum of single-particle functions,

N
A = Ai
i =1

(2.43)

or, at least, a sum of independent contributions from di erent parts of the uid, which may be added up in a similar way. All of the thermodynamic functions are of this shortranged nature, insofar as they are limited by the range of intermolecular interactions. For long-ranged (e.g. dielectric) properties and long-ranged (e.g. Coulombic) forces, this becomes a more subtle point.
e situation for a nite number of particles is treated by expanding the integrand of eqn (2.41) about the mean value F f . If we write F = F f + δ F then we obtain (Lebowitz et al., ):

Af

=

A

F=

F

f

+

1 2

∂2 ∂F 2 A F F= F f

δF2 f + ··· .

(2.44)

e correction term, which is proportional to the mean-square uctuations δ F 2 of the quantity F in the constant-f ensemble, is expected to be relatively small since, as mentioned earlier, the distribution of F values should be very sharply peaked for a many-particle system. is uctuation term may be expressed as a straightforward thermodynamic derivative. Since F and f are conjugate variables, we have

F f = −∂Ψf /∂ f

(2.45)

δ F 2 f = ∂2Ψf /∂ f 2 = −∂ F f /∂ f .

(2.46)

We may write this simply as −(∂F /∂ f ). Equation (2.44) is most usefully rearranged by taking the last term across to the other side, and treating it as a function of f through the relation F = F f . us

AF=

A

f

−

1 2

δF2

f

∂2 ∂F 2

Af

=

Af

+

1 2

∂F ∂f

∂2 ∂F 2

Af

=

Af

+

1 2

∂ ∂f

∂ ∂F

Af

=

Af

+

1 2

∂ ∂f

∂f ∂F

∂ ∂f

A f.

(2.47)

Bearing in mind that F is extensive and f intensive, the small relative magnitude of the correction term can be seen explicitly: it decreases as O(N −1).
Although the uctuations are small, they are nonetheless measurable in computer
simulations. ey are of interest because they are related to thermodynamic derivatives

60 Statistical mechanics

(like the speci c heat or the isothermal compressibility) by equations such as eqn (2.46). In general, we de ne the root mean square ( ) deviation σ (A) by the equation

σ2(A) =

δ A2 ens =

A2 ens −

A

2 ens

(2.48)

where

δ A = A − A ens.

(2.49)

It is quite important to realize that, despite the δ A2 notation, we are not dealing here with the average of a mechanical quantity like A; the best we can do is to write σ 2(A) as a di erence of two terms, as in eqn (2.48). us, the previous observations on equivalence of ensembles do not apply: uctuations in di erent ensembles are not the same. As an obvious example, energy uctuations in the constant-NV E ensemble are (by de nition) zero, whereas in the constant-NVT ensemble, they are not. e transformation technique may be applied to obtain an equation analogous to eqn (2.47) (Lebowitz et al., ). In the general case of the covariance of two variables A and B the result is

δ Aδ B F = δ Aδ B f +

∂f ∂F

∂ ∂f A f

∂ ∂f

Bf

.

(2.50)

Now the correction term is of the same order as the uctuations themselves. Consider,
once more, energy uctuations in the microcanonical and canonical ensembles, that is,
let A = B = F = E and f = β = 1/kBT . en on the le of eqn (2.50) we have zero, and on the right we have σ 2(E) at constant-NVT and a combination of thermodynamic derivatives which turn out to equal (∂E/∂β ) = −kBT 2CV where CV is the constant-volume heat capacity.

2.4 Simple thermodynamic averages
A consequence of the equivalence of ensembles is that, provided a suitable phase function can be identi ed in each case, the basic thermodynamic properties of a model system may be calculated as averages in any convenient ensemble. Accordingly, we give in this section expressions for common thermodynamic quantities, omi ing the subscripts which identify particular ensembles. ese functions are usually derivatives of one of the characteristic thermodynamic functions Ψens. Examples are P = −(∂A/∂V )NT and β = (1/kBT ) = (1/kB)(∂S/∂E)NV .
e kinetic, potential, and total internal energies may be calculated using the phase functions of eqns (1.1)–(1.3).

E= H = K + V .

(2.51)

e kinetic energy is a sum of contributions from individual particle momenta, while evaluation of the potential contribution involves summing over all pairs, triplets, etc. of molecules, depending upon the complexity of the function as discussed in Chapter 1.

Simple thermodynamic averages 61

e temperature and pressure may be calculated using the virial theorem, which we write in the form of ‘generalized equipartition’ (Mu¨nster, ):

pk ∂H /∂pk = kBT qk ∂H /∂qk = kBT

(2.52a) (2.52b)

for any generalized coordinate qk or momentum pk . ese expressions are valid (to O(N −1)) in any ensemble.

Equation (2.52a) is particularly simple when the momenta appear as squared terms in

the Hamiltonian. For example, in the atomic case, we may sum up 3N terms of the form

pi2α /mi , to obtain

N pi 2/mi = 2 K = 3N kBT .
i =1

(2.53)

is is the familiar equipartition principle: an average energy of kBT /2 per degree of freedom. It is convenient to de ne an instantaneous ‘kinetic temperature’ function

T

= 2K /3N kB

=

1 3N kB

N i =1

pi

2/mi

(2.54)

whose average is equal to T . Obviously, this is not a unique de nition. For a system of rigid molecules, described in terms of centre-of-mass positions and velocities together with orientational variables, the angular velocities may also appear in the de nition of T . Alternatively, it may be useful to de ne separate ‘translational’ and ‘rotational’ temperatures each of which, when averaged, gives T . In eqn (2.52a) it is assumed that the independent degrees of freedom have been identi ed and assigned generalized coordinates qk , and momenta pk . For a system of N atoms, subject to internal molecular constraints, the number of degrees of freedom will be 3N − Nc where Nc is the total number of independent internal constraints ( xed bond lengths and angles) de ned in the molecular model. en, we must replace eqn (2.54) by

T

=

(3N

2K − Nc)kB

=

(3N

1 − Nc)kB

N i =1

pi

2/mi .

(2.55)

We must also include in Nc, any additional global constraints on the ensemble. For example, in the ‘molecular dynamics’ constant-NV EP ensemble, we must include the three extra constraints on centre-of-mass motion.
Equations (2.52) are examples of the general form

∂H A
∂qk

= kBT

∂A ∂qk

,

∂H A
∂pk

= kBT

∂A ∂pk

,

valid for any dynamical variable A, which may be easily derived in the canonical ensemble (see e.g. Landau and Lifshitz, , p , eqn ( . )). ese are generally termed ‘hypervirial’ relations (Hirschfelder, ). Se ing A = ∂H /∂qk = ∂V/∂qk gives an alternative

62 Statistical mechanics

way of calculating the temperature from purely con gurational properties, independent of the momenta. For example, for a simple atomic system

kBT =

(∂V/∂riα )2 ∂2 V /∂ri2α

=

fi2α ∂2 V /∂ri2α

.

(2.56)

Naturally, in a simulation it is usual to average this expression over all atoms i and all coordinate directions α, when the numerator becomes the mean-square force and the denominator becomes the average Laplacian of the potential. is ‘con gurational temperature’ is useful in Monte Carlo simulations, in which the momenta do not appear (Rugh, ), and comparing it with the usual kinetic expression, (2.54), or with the prescribed temperature, is a useful check that a simulation is working properly (Butler et al., ). More details of how to calculate the con gurational temperature appear in Appendix F.
e pressure may be calculated via eqn (2.52b). If we choose Cartesian coordinates, and use Hamilton’s equations of motion (see Chapter 3), it is easy to see that each coordinate derivative in eqn (2.52b) is the negative of a component of the force fi on some molecule i, and we may write, summing over N molecules,

N

N

−

1 3

ri · ∇ri V

=

1 3

ri

·

f

tot i

= −N kBT .

i =1

i =1

(2.57)

We

have

used

the

symbol

f

tot i

because

this

represents

the

sum

of

intermolecular

forces

and external forces. e la er are related to the external pressure, as can be seen by

considering the e ect of the container walls on the system:

N

1 3

ri

·

f

ext i

= −PV .

i =1

If we de ne the ‘internal virial’ W

N

N

−

1 3

ri

·

∇ri V

=

1 3

ri · fi = W

i=

i =1

where now we restrict a ention to intermolecular forces, then

(2.58) (2.59)

PV = N kBT + W .

(2.60)

is suggests that we de ne an instantaneous ‘pressure’ function (Cheung, P = ρkBT + W/V = Pid + Pex

) (2.61)

whose average is simply P. Again, this de nition is not unique; apart from the di erent ways of de ning W which we shall see later, it may be most convenient (say in a constanttemperature ensemble) to use

P = ρkBT + W/V = Pid + Pex

(2.62)

instead. Both P and P give P when averaged, but their uctuations in any ensemble will, in general, be di erent. Note that the preceding derivation is not really valid for the

Simple thermodynamic averages 63

in nite periodic systems used in computer simulation: there are no container walls and no external forces. Nonetheless, the result is the same (Erpenbeck and Wood, ).
For pairwise interactions, W is more conveniently expressed in a form which is explicitly independent of the origin of coordinates. is is done by writing fi as the sum of forces fij on atom i due to atom j

ri · fi =

ri

· fij

=

1 2

ri · fij + rj · f ji .

i

i ji

i ji

(2.63)

e second equality follows because the indices i and j are equivalent. Newton’s third law fij = −f ji is then used to switch the force indices

ri

· fi

=

1 2

rij · fij =

ri j · fi j

i

i ji

i j >i

(2.64)

where rij = ri − rj and the nal form of the summation is usually more convenient. It is essential to use the rij · fij form in a simulation that employs periodic boundary conditions. So we have at last

W

=

1 3

ri j

· fij

=

−

1 3

ri j

·

∇ri j

v(rij )

=

−

1 3

w(rij )

i j >i

i j >i

i j >i

(2.65)

where the intermolecular pair virial function w(r ) is

w(r

)

=

r

dv(r dr

)

.

(2.66)

Like V, W is limited by the range of the interactions, and hence W should be a well-behaved, ensemble-independent function in most cases.
For molecular uids we may write

W

=

1 3

i

ri j

·

fij

=

−

1 3

j >i

i

rij ·
j >i

∇rij V

Ωi, Ωj

=

−

1 3

i

w(rij )
j >i

(2.67)

where rij is the vector between the molecular centres. Here we have made it clear that the pair virial is de ned as a position derivative at constant orientation of the molecules

w(rij ) = rij

∂ v(rij , Ωi , Ωj )

.

∂ rij

Ωi, Ωj

(2.68)

e pressure function P is de ned through eqn (2.61) as before. For interaction site

models, we may treat the system as a set of atoms, and use eqns (2.65), (2.66), with the

summations taken over distinct pairs of sites ia and jb (compare eqn (1.12)). When doing

this, however, it is important to include all intramolecular contributions (forces along the

bonds for example) in the sum. Alternatively, the molecular de nition, eqns (2.67), (2.68)

is still valid. In this case, for computational purposes, eqn (2.68) may be rewri en in the

form

w(rij ) =
a

b

wab (rab ) ra2b

(rab

·

rij )

(2.69)

where rab = ria − rjb is the vector between the sites and wab (rab ) is the site–site pair virial function. is is equivalent to expressing fij in eqn (2.67) as the sum of all the site–site

64 Statistical mechanics

forces acting between the molecules. Whether the atomic or molecular de nition of the virial is adopted, the ensemble average W and hence P = P should be una ected. In inhomogeneous systems, the pressure is a tensor; see Section 2.12.
In systems with discontinuous interactions, such as the hard-sphere model, the usual expressions for the pressure cannot be applied. As we shall see in Chapter 3, in simulations of hard particles, we solve the classical equations of motion for the motion in between discrete collisions; at the moment of collision, an impulse acts between the two colliding particles, and changes their momenta. is is responsible for the non-ideal contribution to the pressure. e virial expression (2.65) can be recast into a form involving a sum over collisions, by time-averaging it:

W

=

1 tobs

tobs
dt
0

1 3

i

ri j · fi j
j >i

=

1 3tobs

ri j
colls

· δ pij

(2.70a)

where i and j represent a pair of molecules colliding at time tij , rij is the vector between the molecular centres at the time of collision, and

ti+j

δ pij = δ pi = −δ pj =

dt fij

ti−j

(2.70b)

is the collisional impulse, that is, the change in momentum. e sum in eqn (2.70a) is over all collisions occurring in time tobs, and the integral in eqn (2.70b) is over an in nitesimal time interval around tij . is expression may also be wri en in terms of the collision rate and the average of rij · δ pij per collision. Equation (2.70a) replaces eqn (2.65) in the average pressure equation (2.60). Further details, including a discussion of the system-size dependence of these formulae may be found elsewhere (Alder and Wainwright, ; Hoover and Alder, ; Erpenbeck and Wood, ). In simulations of hard systems, a less direct approach must be used to estimate P, and this is discussed in Section 5.5.
antities such as N and V are easily evaluated in the simulation of ensembles in which these quantities vary, and derived functions such as the enthalpy are straightforwardly calculated. Now we turn to the question of evaluating entropy-related (‘statistical’) quantities such as the Gibbs and Helmholtz functions, the chemical potential µ, and the entropy itself. A direct approach is to conduct a simulation of the grand canonical ensemble, in which µ, or a related quantity, is speci ed. It must be said at the outset that there are some technical di culties associated with grand canonical ensemble simulations, and we return to this in Chapter 4. ere are also di culties in obtaining these functions in the other common ensembles, since they are related directly to the partition function Q, not to its derivatives. To calculate Q would mean summing over all the states of the system. It might seem that we could use the formula

exp Aex/kBT

=

Q

ex NV

T

−1

=

exp(V/kBT ) NVT

(2.71)

to estimate the excess statistical properties, but, in practice, the distribution ρNVT will be very sharply peaked around the largest values of exp(−V/kBT ), that is, where exp(V/kBT ) is comparatively small. Consequently, any simulation technique that samples according to the equilibrium distribution will be bound to give a poor estimate of A by this route. Special sampling techniques have been developed to evaluate averages of this type (Valleau

Simple thermodynamic averages 65

and Torrie, ) and we return to this in Chapter 9. It is comparatively easy to obtain free-energy di erences for a given system at two di erent temperatures by integrating the internal energy along a line of constant density:

A N kBT

−
2

A N kBT

=
1

β2 β1

E N kBT

dβ β

=−

T2 T1

E N kBT

dT . T

Alternatively, integration of the pressure along an isotherm may be used:

(2.72)

A N kBT

−
2

A N kBT

=
1

ρ2 ρ1

PV N kBT

dρ ρ

=

−

V2 V1

PV N kBT

dV V

.

(2.73)

To use these expressions, it is necessary to calculate ensemble averages at state points along a reversible thermodynamic path. To calculate absolute free energies and entropies, it is necessary to extend the thermodynamic integration far enough to reach a state point whose properties can be calculated essentially exactly. In general, these calculations may be expensive, since accurate thermodynamic information is required for many closely spaced state points.
One fairly direct, and widely applicable, method for calculating µ is based on the thermodynamic identities

exp(−µ/kBT ) = QN +1/QN = QN /QN −1

(2.74)

valid at large N for both the constant-NVT and constant-N PT ensembles. From these
equations we can obtain expressions for the chemical potential in terms of a kind of ensemble average (Widom, ; ). If we de ne the excess chemical potential µex = µ − µid then we can write

µex = −kBT ln exp(−Vtest/kBT )

(2.75)

where Vtest is the potential energy which would result from the addition of a particle (at random) to the system. is is the ‘test particle insertion’ method of estimating µ. Eqn (2.75) also applies in the constant-µVT ensemble (Henderson, ). A slightly di erent formula applies for constant-NV E because of the kinetic temperature uctuations (Frenkel, ):

µex = −kB T ln T −3/2 T 3/2 exp(−Vtest/kBT )

(2.76a)

where T is the instantaneous kinetic temperature. Similarly, for the constant-N PT

ensemble, it is necessary to include the uctuations in the volume V (Shing and Chung,

):

µex = −kBT ln V −1 V exp(−Vtest/kBT ) .

(2.76b)

In all these cases the ‘test particle’, the (N + 1)th, is not actually inserted: it is a ‘ghost’, that is, the N real particles are not a ected by its presence. e Widom method can also be applied to inhomogeneous systems, see Section 2.12.
ere is an alternative formula which applies to the removal of a test particle (selected at random) from the system (Powles et al., ). is ‘test particle’ is not actually removed:

66 Statistical mechanics

it is a real particle and continues to interact normally with its neighbours. In practice, this technique does not give an accurate estimate of µex, and for hard spheres (for example) it
is completely unworkable (Rowlinson and Widom, ). We defer a detailed discussion
of the applicability of these methods and more advanced techniques until Chapter 9.

2.5 Fluctuations

We now discuss the information that can be obtained from the uctuations calculated

as indicated in eqn (2.48). e quantities of most interest are the constant-volume speci c

heat capacity CV = (∂E/∂T )V or its constant-pressure counterpart CP = (∂H /∂T )P , the thermal expansion coe cient αP = V −1(∂V /∂T )P , the isothermal compressibility βT = −V −1(∂V /∂P )T , the thermal pressure coe cient γV = (∂P/∂T )V , and the adiabatic

(constant-S) analogues of the last three. e relationship αP = βT γV means that only two

of these quantities are needed to de ne the third. In part, formulae for these quantities

can be obtained from standard theory of uctuations (Landau and Lifshitz, ), but

in computer simulations we must be careful to distinguish between properly de ned

mechanical quantities such as the energy or Hamiltonian H , the kinetic temperature T

or the instantaneous pressure P, and thermodynamic concepts such as T and P, which

can only be described as ensemble averages or as parameters de ning an ensemble. us,

a standard formula such as σ 2(E) = δE2 = kBT 2CV can be used to calculate the speci c

heat in the canonical ensemble (provided we recognize that E really means H ), whereas

the analogous simple formula σ 2(P ) = δP2 = kBT /V βT will not be so useful (since P is

not the same as P).

Fluctuations are readily computed in the canonical ensemble, and accordingly we

start with this case. As just mentioned, the speci c heat is given by the uctuations in the

energy:

δ H 2 N V T = kBT 2CV .

(2.77)

is can be divided into kinetic and potential contributions which are uncorrelated (i.e.

δ K δ V NVT = 0):

δH2 NVT = δV2 NVT + δK 2 NVT .

(2.78)

e kinetic part can be calculated easily, for example in the case of a system of N atoms:

δK2

NVT

=

3N 2

(kBT )2

=

3N /2β2

(2.79)

yielding the ideal gas part of the speci c heat CVid = (3/2)N kB. For this case, then, potentialenergy uctuations are simply

δV2

NVT

= kBT 2

CV

−

3 2

N

kB

.

Consideration of the cross-correlation of the potential-energy and virial yields an expression for the thermal pressure coe cient γV (Rowlinson,

(2.80)
uctuations )

δ Vδ W N V T = kBT 2 V γV − N kB

(2.81)

where W is de ned in eqns (2.65)–(2.69). In terms of the pressure function de ned in eqn (2.61) this becomes

δ Vδ P N V T = kBT 2 γV − ρkB

(2.82)

Fluctuations 67

once more valid for a system of N atoms. Equation (2.82) also applies if P is replaced by P or by Pex (eqn (2.82)), which is more likely be available in a (con guration-space)
constant-NVT Monte Carlo calculation. Similar formulae may be derived for molecular
systems. When we come to consider uctuations of the virial itself, we must de ne a
further ‘hypervirial’ function

X

=

1 9

ri j · ∇rij rk · ∇rk V

i j >i k >k

(2.83)

which becomes, for a pairwise additive potential

X

=

1 9

x (rij )

i j >i

(2.84)

where

x

(r )

=

r

dw(r ) dr

(2.85)

w(r ) being the intermolecular viral de ned in eqn (2.66). It is then easy to show that

δ W2 N V T = kBT N kBT + W N V T − βT−1V + X N V T

(2.86)

or

δ P2

NVT

=

kBT V

2N kBT 3V

+

P N V T − βT−1 +

X NVT V

.

(2.87)

e average X is a non-thermodynamic quantity. Nonetheless, it can be calculated in a computer simulation, so eqns (2.86) and (2.87) provide a route to the isothermal compressibility βT . Note that Cheung ( ) uses a di erent de nition of the hypervirial function. In terms of the uctuations of P , the analogous formula is

δ Pex2 N V T =

δP

2

NVT

=

kBT V

P

N V T − βT−1 +

X NVT V

(2.88)

and this would be the formula used most in constant-NVT simulations. e desired uctuation expressions for the microcanonical ensemble may best be
derived from the preceding equations, by applying the transformation formula, eqn (2.50) (Lebowitz et al., ; Cheung, ) or directly (Ray and Graben, ). e equivalence of the ensembles guarantees that the values of simple averages (such as X ) are unchanged by this transformation. In the microcanonical ensemble, the energy (of course) is xed, but the speci c heat may be obtained by examining uctuations in the separate potential and kinetic components (Lebowitz et al., ). For N atoms,

δV2 NVE =

δK2

NV E

=

3 2

N

kB2T

2

1

−

3N kB 2CV

.

(2.89)

Cross-correlation of the pressure function and (say) the kinetic energy may be used to obtain the thermal pressure coe cient:

δ PδK

NVE =

δ PδV

NV E

=

N kB2T 2 V

1

−

3V γV 2CV

.

(2.90)

68 Statistical mechanics

Finally the expression for uctuations of P in the microcanonical ensemble yields the
isothermal compressibility, but the formula is made slightly more compact by introducing the adiabatic compressibility βS , and using βS−1 = βT−1 + TVγV2 /CV

δ P2

NVE

=

kBT V

2N kBT 3V

+

P

N V E − βS−1 +

X NVE . V

(2.91)

In eqns (2.89)–(2.91) T is short for T NV E . All these expressions are easily derived using the transformation technique outlined earlier, and they are all valid for systems of N atoms. e same expressions (to leading order in N ) hold in the constant-NV EP ensemble probed by molecular dynamics. Analogous formulae for molecular systems may be derived in a similar way.
Conversion from the canonical to the isothermal–isobaric ensemble is easily achieved. Most of the formulae of interest are very simple since they involve well-de ned mechanical quantities. At constant T and P, both volume and energy uctuations may occur. e volume uctuations are related to the isothermal compressibility

δV 2 N PT = V kBT βT .

(2.92)

e simplest speci c heat formula may be obtained by calculating the ‘instantaneous’ enthalpy H + PV , when we see

δ (H + PV )2 N PT = kBT 2CP .

(2.93)

is equation can be split into the separate terms involving δ H 2 , δV 2 , and δ H δV . Finally the thermal expansion coe cient may be calculated from the cross-correlations of ‘enthalpy’ and volume:

δV δ (H + PV ) N PT = kBT 2V αP .

(2.94)

Other quantities may be obtained by standard thermodynamic manipulations. Finally, to reiterate, although P is xed in these expressions, the functions P and P de ned in eqns (2.61)–(2.62) will uctuate around the average value P.
In the grand canonical ensemble, energy, pressure, and number uctuations occur. e number uctuations yield the isothermal compressibility

δN2

µV T

= kBT (∂N /∂µ)VT

=

N2 V

kBT

βT

.

(2.95)

Expressions for the other thermodynamic derivatives are a li le more complicated (Adams, ). e simplest formula for a speci c heat is obtained by considering (by analogy with the enthalpy) a function H − µN :

δ (H − µN )2 µV T = kBT 2CµV = kBT 2

∂ H − µN ∂T

µV

(2.96)

and the usual speci c heat CV (i.e. CNV ) is obtained by thermodynamic manipulations:

CV

=

3 2

N

kB

+

1 kBT

2

δV2 µVT −

δVδN

2 µV T

δN 2 µVT

.

(2.97)

Structural quantities 69

e thermal expansion coe cient may be derived in the same way:

αP

=

P βT T

−

δVδN µVT N kBT 2

+

V

µVT δN 2 N 2kBT 2

µVT .

Finally, the thermal pressure coe cient is given by

(2.98)

γV

=

N kB V

+

δVδN VT

µV T

1−

N δN 2 µVT

+

δVδW V kBT

µV T 2

.

(2.99)

Except within brackets · · · , N in these equations is understood to mean N µVT and similarly P means P µVT . As emphasized by Adams ( ), when these formulae are used in a computer simulation, it is advisable to cross-check them with the thermodynamic identity αP = βT γV .
e impression may arise that particular thermodynamic derivatives (such as αP ) are best calculated by conducting a simulation in the corresponding ensemble (e.g. constantN PT ). is is not the case, and Lustig ( ) has provided a systematic approach to calculating such quantities in a wide variety of ensembles. Care must be taken in the application of any formulae to the zero-momentum ensemble usually employed in molecular dynamics (C¸ag˘in and Ray, ; Lustig, a,b). Also, it is important to bear in mind that signi cant deviations from the thermodynamic limit will happen when the system size is small (Ray and Graben, ; Shirts et al., ; Uline et al., ).

2.6 Structural quantities
e structure of a simple monatomic uid is characterized by a set of distribution functions for the atomic positions, the simplest of which is the pair distribution function 2(ri , rj ), or 2(rij ) or simply (r ). is function gives the probability of nding a pair of atoms a distance r apart, relative to the probability for a completely random distribution at the same density. To de ne (r ), we integrate the con gurational distribution function over the positions of all atoms except two, incorporating the appropriate normalization factors (Mc arrie, ; Hansen and McDonald, ). In the canonical ensemble

(r1, r2)

=

N (N − 1) ρ 2Z N V T

dr3dr4 · · · drN exp[−βV (r1, r2, · · · rN )].

(2.100)

Obviously the choice i = 1 and j = 2 is arbitrary in a system of identical atoms. An equivalent de nition begins with the pair density

ρ(2) (r + r, r ) =

δ (r + r − ri )δ (r − rj )

i ji

for positions separated by a vector r. is is independent of r in a homogeneous system (we discuss inhomogeneous systems in Section 2.12). (r) is de ned as the ratio

(r)

=

ρ(2) (r + ρ2

r, r

)

=

V2 N2

1 V

dr ρ(2) (r + r, r )

70 Statistical mechanics 3.0

2.0

(r )

1.0

0.00.0 0.5 1.0 1.5 2.0 2.5 3.0 r /σ

Fig. 2.2 Pair distribution function for the Lennard-Jones uid (shi ed, rc = 2.5σ ) close to its triple point (T ∗ = 0.8, ρ∗ = 0.8).

where we average over r . e result of integrating over the delta functions is an ensemble average over pairs

(r )

=

V N2

δ (r − rij ) ,
i ji

where rij = ri − rj ,

(2.101)

and we note that the result depends only on r = |r| in an isotropic liquid. e pair distribution functions of simple model systems, such as hard spheres, may be predicted accurately by integral equation theories, and are frequently used as a basis for modelling the structure of a range of uids (Hansen and McDonald, ); they may be easily calculated (Smith et al., ).
Equation (2.101) may be used in the evaluation of (r ) by computer simulation; in practice, the delta function is replaced by a function which is non-zero in a small range of separations, and a histogram is compiled of all pair separations falling within each such range (see Chapter 8). Fig. 2.2 shows a typical pair distribution function for the Lennard-Jones liquid close to its triple point.
e pair distribution function is useful, not only because it provides insight into the liquid structure, but also because the ensemble average of any pair function may be expressed in the form

a(ri , rj )

=

1 V2

dri drj (ri , rj )a(ri , rj )

(2.102a)

or, in an isotropic uid,

∞

A=

i

a(rij )
j >i

=

1 2

N

ρ

0

a(r ) (r )4πr 2 dr .

(2.102b)

Structural quantities 71

For example, we may write the energy (assuming pair additivity)

∞

E

=

3 2

N

kBT

+

2πN ρ

r 2v(r ) (r ) dr

0

(2.103)

or the pressure

∞

PV

=

N kBT

−

2 3

πN

ρ

r 2w(r ) (r ) dr

0

(2.104)

although in practice, a direct evaluation of these quantities, as discussed in Section 2.4,

will usually be more accurate. Even the chemical potential may be related to (r )

1

∞

µ = kBT ln(ρΛ3) + 4πρ dξ r 2v(r ) (r ; ξ )dr

0

0

(2.105)

with Λ given by eqn (2.24). As usual with the chemical potential, there is a twist: the formula involves a pair distribution function (r , ξ ) which depends upon a parameter ξ coupling the two atoms, and it is necessary to integrate over ξ (Mc arrie, ).
e de nition of the pair distribution function may be extended to the molecular case when the function (rij , Ωi , Ωj ) depends upon the separation between, and orientations of the molecules. is may be evaluated in a simulation by compiling histograms, as in the atomic case, but of course there is now the problem that many more variables are involved, and a very large, multidimensional table will be needed. A number of di erent approaches which give partial descriptions of the orientational ordering have been developed (Gray and Gubbins, ):
(a) sections through (rij , Ωi , Ωj ) are calculated as a function of rij for xed relative orientations (Haile and Gray, );
(b) (rij , Ωi , Ωj ) can be expressed as a spherical harmonic expansion, where the coe cients are functions of rij (Stree and Tildesley, ; Haile and Gray, );
(c) a set of site–site distribution functions ab (rab ) can be calculated in the same way as the atomic (r ) for each type of site.
e rst method proceeds by compiling histograms, just as for (r ), but restricting the accumulation of data to pairs of molecules which are close to a few speci c relative orientations. us for pairs of linear molecules, parallel con gurations and T-shapes might be of interest.
e spherical harmonic expansion for a pair of linear molecules would take the form

∞∞
(rij , Ωi , Ωj ) = 4π
=0 =0 m

m (rij )Y m (Ωi )Y m¯ (Ωj )

(2.106)

where the functions Y m (Ω) ≡ Y m (θ, ϕ) are spherical harmonic functions of the polar angles de ning the direction of the molecular axis, and m¯ = −m. e range of the sum over m values is either (− , ) or (− , ), whichever is the smaller. Note that the orientations are measured relative to the vector rij in each case. In a simulation, the coe cients m would be evaluated by averaging a product of spherical harmonics over a spherical shell around each molecule, as described in Chapter 8. e function 000(r ) is the isotropic component, that is, the pair distribution function for molecular centres averaged over all

72 Statistical mechanics

orientations. is approach is readily extended to non-linear molecules. e expansion can be carried out in a molecule- xed frame (Stree and Tildesley, ) or in a space-
xed frame (Haile and Gray, ). e coe cients can be recombined to give the total distribution function, but this is not pro table for elongated molecules, since many terms are required for the series to converge. Certain observable properties are related to limited numbers of the harmonic coe cients. e angular correlation parameter of rank , , may be expressed in the molecule- xed frame

=

1

+

4πρ 2 +1

(−1)m

m=−

∞ 0

m (r )r 2dr

=

1

+

1 N

P (cos γij )
i ji

(2.107)

where P (cos γ ) is a Legendre polynomial and γij is the angle between the axis vectors of molecules i and j. 1 is related to the dielectric properties of polar molecules, while 2 may be investigated by depolarized light sca ering. Formulae analogous to eqns (2.106) and (2.107) may be wri en for non-linear molecules. ese would involve the Wigner rotation matrices Dmm (Ωi ) instead of the spherical harmonics (Gray and Gubbins, , Appendix ). In liquid crystals, where the isotropic rotational symmetry is broken, these expansions involve many more terms (Zannoni, ).
As an alternative, a site–site description might be more appropriate. Pair distribution functions ab (rab ) are de ned for each pair of sites on di erent molecules, using the same de nition as in the atomic case. e number of independent ab (rab ) functions will depend on the complexity of the molecule. For example, in a three-site model of OCS, the isotropic liquid is described by six independent ab (rab ) functions (for OO, OC, OS, CC, CS, and SS distances), whereas for a ve-site model of CH4, the liquid is described by three functions (CC, CH, HH). While less information is contained in these distribution functions than in the components of (rij , Ωi , Ωj ), they have the advantage of being directly related to the structure factor of the molecular uid (Lowden and Chandler, ) and hence to experimentally observable properties (for example neutron and X-ray sca ering). We return to the calculation of these quantities in Chapter 8.
Finally, we turn to the de nitions of quantities that depend upon wavevector rather than on position. In a simulation with periodic boundaries, we are restricted to wavevectors that are commensurate with the periodicity of the system, that is, with the simulation box. Speci cally, in a cubic box, we may examine uctuations for which k = (2π/L)(nx , ny , nz ) where L is the box length and nx , ny , nz are integers. is is a severe restriction, particularly at low k. One quantity of interest is the spatial Fourier transform of the number density

N
ρ (k) = exp(−ik · ri ).
i =1
Fluctuations in ρ (k) are related to the structure factor S (k )

(2.108)

S (k ) = N −1 ρ (k)ρ (−k)

(2.109)

which may be measured by neutron or X-ray sca ering experiments, and depends only on k = |k| in an isotropic system. us, S (k ) describes the Fourier components of the density

Time correlation functions and transport coe cients 73

uctuations in the liquid. It is related, through a three-dimensional Fourier transform (see Appendix D) to the pair distribution function

S (k ) = 1 + ρhˆ(k ) = 1 + ρ ˆ(k ) = 1 + 4πρ

0

∞

r

2

sin kr kr

(r ) dr

(2.110)

where we have introduced the Fourier transform of the total correlation function h(r ) = (r ) − 1, and have ignored a delta function contribution at k = 0. In a similar way, kdependent orientational functions may be calculated and measured routinely in computer simulations.

2.7 Time correlation functions and transport coe cients
Correlations between two di erent quantities A and B are measured in the usual statistical sense, via the correlation coe cient c AB

c AB = δ Aδ B /σ (A)σ (B)

(2.111)

with σ (A) and σ (B) given by eqn (2.48). Schwartz inequalities guarantee that the absolute value of c AB lies between 0 and 1, with values close to 1 indicating a high degree of correlation. e idea of the correlation coe cient may be extended in a very useful way, by considering A and B to be evaluated at two di erent times. For an equilibrium system, the resulting quantity is a function of the time di erence t: it is a ‘time correlation function’ c AB (t ). For identical phase functions, c AA (t ) is called an autocorrelation function and its time integral (from t = 0 to t = ∞) is a correlation time tA. ese functions are of great interest in computer simulation because:
(a) they give a clear picture of the dynamics in a uid; (b) their time integrals tA may o en be related directly to macroscopic transport
coe cients; (c) their Fourier transforms cˆAA (ω) may o en be related to experimental spectra
measured as a function of frequency ω.
Useful discussions of time correlation functions may be found in the references (Steele, ; ; Berne and Harp, ; Mc arrie, ; Hansen and McDonald, ). A few comments may be relevant here. e non-normalized correlation function is de ned

CA B (t ) = δ A (t )δ B (0) ens = δ A Γ(t ) δ B Γ(0) ens

(2.112)

so that

c AB (t ) = CAB (t )/σ (A)σ (B)

(2.113a)

or

c A A (t ) = CA A (t )/σ 2 (A) = CA A (t )/CA A (0).

(2.113b)

Just like δ Aδ B , CAB (t ) is di erent for di erent ensembles, and eqn (2.50) may be used to transform from one ensemble to another. e computation of CAB (t ) may be thought of as a two-stage process. First, we must select initial state points Γ(0), according to the desired distribution ρens(Γ), over which we will subsequently average. is may be

74 Statistical mechanics

done using any of the prescriptions mentioned in Section 2.1. Second, we must evaluate Γ(t ). is means solving the true (Newtonian) equations of motion. By this means, timedependent properties may be calculated in any ensemble. In practice, the mechanical equations of motion are almost always used for both purposes, that is, we use molecular dynamics to calculate the time correlation functions in the microcanonical ensemble.
Some a ention must be paid to the question of ensemble equivalence, however, since the link between correlation functions and transport coe cients is made through linear response theory, which can be carried out in virtually any ensemble. is actually caused some confusion in the original derivations of the expressions for transport coe cients (Zwanzig, ). In the following, we make some general observations, and refer the reader elsewhere (Mc arrie, ; Frenkel and Smit, ; Tuckerman, ; Hansen and McDonald, ) for a fuller discussion.
Transport coe cients such as the di usion coe cient, thermal conductivity, and shear and bulk viscosities appear in the equations of hydrodynamics, such as the mass and thermal di usion equations and the Navier–Stokes equation. Accordingly, they describe the relaxation of dynamical variables on the macroscopic scale. Provided the long-time and large-length-scale limits are considered carefully, they can be expressed in terms of equilibrium time correlation functions of microscopically de ned variables. Such relations are generally termed Green–Kubo formulae (Hansen and McDonald, ). Linear response theory can be used to provide an interpretation of these formulae in terms of the response of the system to a weak perturbation. By introducing such perturbations into the Hamiltonian, or directly into the equations of motion, their e ect on the distribution function ρens may be calculated. Generally, a time-dependent nonequilibrium distribution ρ (t ) = ρens + δ ρ (t ) is produced. Hence, any nonequilibrium ensemble average (in particular, the desired response) may be calculated. By retaining the linear terms in the perturbation, and comparing the equation for the response with a macroscopic transport equation, we may identify the transport coe cient.
e Green–Kubo expression is usually wri en as the in nite time integral of an equilibrium time correlation function of the form

∞
γ = dt A˙ (t )A˙ (0)
0

(2.114)

where γ is the transport coe cient, and A is the appropriate dynamical variable. Associated with any expression of this kind, there is an ‘Einstein relation’

A (t ) − A (0) 2 = 2tγ ,

as t → ∞, or

γ

=

lim
t →∞

d dt

1 2

A (t ) − A (0) 2

(2.115)

which holds at large t compared with the correlation time of A. e connection between eqns (2.114) and (2.115) may be easily established by integration by parts. Note that only a few genuine transport coe cients exist; that is, for only a few ‘hydrodynamic’ variables A do eqns (2.114) and (2.115) give a non-zero γ (Mc arrie, ).
In computer simulations, transport coe cients may be calculated from equilibrium correlation functions, using eqn (2.114), by observing Einstein relations, eqn (2.115), or indeed going back to rst principles and conducting a suitable nonequilibrium simulation.
e details of the calculation via eqns (2.114), (2.115) will be given in Chapter 8, and we

Time correlation functions and transport coe cients 75

will examine nonequilibrium methods in Chapter 11. For use in equilibrium molecular dynamics, we give here the equations for calculating thermal transport coe cients in the microcanonical ensemble, for a uid composed of N identical molecules.
e di usion coe cient D is given (in three dimensions) by

D

=

1 3

∞
dt
0

vi (t ) · vi (0)

where vi (t ) is the centre-of-mass velocity of a single molecule. relation, valid at long times, is

(2.116) e corresponding Einstein

D

=

lim
t →∞

d dt

1 6

ri (t ) − ri (0) 2

(2.117)

where ri (t ) is the molecule position. ere is also an equally valid ‘intermediate’ form:

D

=

lim
t →∞

1 3

vi (0) ·

ri (t ) − ri (0)

.

(2.118)

In practice, these averages would be computed for each of the N particles in the simulation, the results added together, and divided by N , to improve statistical accuracy. Note that in the computation of eqns (2.117), (2.118), it is important not to switch a ention from one periodic image to another, which is why it is sometimes useful to have available a set of particle coordinates which have not been subjected to periodic boundary conditions during the simulation (see Section 1.6 and Chapter 8).
e shear viscosity is given by

η

=

V kBT

∞
dt Pα β (t )Pα β (0)
0

(2.119)

or Here or

η

=

lim
t →∞

d dt

1 2

V kBT

Qα β (t ) − Qα β (0) 2 .

Pα β

=

1 V

piα pi β /mi + riα fi β

i

i

Pα β

=

1 V

piα pi β /mi +

rijα fijβ

i

i j >i

(2.120) (2.121) (2.122)

is an o -diagonal (α β) element of the pressure tensor (compare the virial expression for the pressure function eqns (2.61) and (2.65)) and

Qα β

=

1 V

riαpiβ .
i

(2.123)

e negative of Pα β is o en called the stress tensor. ese quantities are multi-particle properties, properties of the system as a whole, and no additional averaging over N
particles is possible. Consequently η is subject to much greater statistical imprecision than

76 Statistical mechanics

D. Some improvement is possible by averaging over di erent components α β = xy, yz, zx,
of Pα β . Just as for eqn (2.65), the origin-independent form, eqn (2.122), should be used rather than eqn (2.121), in periodic boundaries, and similar care needs to be taken in the
calculation of Qα β (t ) − Qα β (0) in eqn (2.120). e bulk viscosity is given by a similar expression:

ηV

=

V 9kBT

αβ

∞
dt
0

δ Pα α (t )δ Pβ β (0)

=

V kBT

∞
dt δ P (t )δ P (0)
0

(2.124a)

where we sum over α, β

=

x,y,z

and note that

P

=

1 3

Tr

P

=

1 3

α Pαα . Rotational

invariance leads to the equivalent expression

ηV

+

4 3

η

=

V kBT

∞
dt δ Pα α (t )δ Pα α (0) .
0

(2.124b)

Here the diagonal stresses must be evaluated with care, since a non-vanishing equilibrium average must be subtracted:

δ Pα α (t ) = Pα α (t ) − Pα α = Pα α (t ) − P δ P (t ) = P (t ) − P = P (t ) − P

(2.125a) (2.125b)

with Pα β given by an expression like eqn (2.122). e corresponding Einstein relation is (Alder et al., )

ηV

+

4 3

η

=

lim
t →∞

d dt

1 2

V kBT

Qα α (t ) − Qα α (0) − Pt 2 .

(2.126)

e thermal conductivity λT can be wri en (Hansen and McDonald, )

λT

=

V kBT 2

∞
dt jαϵ (t )jαϵ (0)
0

or

λT

=

lim
t →∞

d dt

1V 2 kBT 2

δϵα (t ) − δϵα (0) 2 .

Here jαϵ is a component of the energy current, that is, the time derivative of

δϵα

=

1 V

riα ϵi − ϵi
i

.

(2.127) (2.128) (2.129)

e term i riα ϵi makes no contribution if i riα = 0, as is the case in a normal onecomponent simulation. In calculating the energy per molecule ϵi , the potential energy

of two molecules (assuming pairwise additive potentials) is taken to be divided equally

between them:

ϵi

=

pi2/2mi

+

1 2

v(rij ).

ji

(2.130)

ese expressions for ηV and λT are ensemble-dependent and the preceding equations hold for the microcanonical case only. A fuller discussion may be found in the standard texts (Mc arrie, ; Zwanzig, ).

Time correlation functions and transport coe cients 77

Transport coe cients are related to the long-time behaviour of correlation functions.

Short-time correlations, on the other hand, may be linked with static equilibrium ensemble

averages by expanding in a Taylor series. For example, the velocity of particle i may be

wri en

vi (t )

=

vi

(0)

+

v˙ i

(0)t

+

1 2

v¨i

(0)t 2

+

·

·

·

.

(2.131)

Multiplying by vi (0) and ensemble averaging yields

vi (t ) · vi (0)

=

vi2

+

1 2

v¨i

· vi

t2 + ··· =

vi2

−

1 2

v˙i2

t2 + ··· .

(2.132)

e vanishing of the term linear in t, and the last step, where we set v¨i · vi = − v˙ i · v˙ i , follow from time-reversal symmetry and stationarity (Mc arrie, ). us, the shorttime velocity autocorrelation function is related to the mean-square acceleration, that is, to the mean-square force. is behaviour may be used to de ne the Einstein frequency ωE

vi (t ) · vi (0)

=

vi2

1

−

1 2

ωE2t

2

+

·

·

·

.

(2.133)

e analogy with the Einstein model, of an atom vibrating in the mean force eld of its
neighbours, with frequency ωE in the harmonic approximation, becomes clear when we replace the mean-square force by the average potential curvature using

fi2α = − fiα ∂V/∂riα = −kBT ∂ fiα /∂riα = kBT ∂2V/∂ri2α

(2.134)

which is another application of A ∂H /∂qk = kBT ∂A/∂qk . e result is

ωE2

=

fi2 mi2 vi2

=

1 3mi

∇2ri V

.

(2.135)

is may be easily evaluated for, say, a pairwise additive potential. Short-time expansions of other time correlation functions may be obtained using similar techniques. e temporal Fourier transform (see Appendix D) of the velocity autocorrelation function is proportional to the density of normal modes in a purely harmonic system, and is o en loosely referred to as the ‘density of states’ in solids and liquids. e velocity autocorrelation function for the Lennard-Jones liquid near the triple point is illustrated in Fig. 2.3.
We can only mention brie y some other autocorrelation functions of interest in computer simulations. e generalization of eqn (2.109) to the time domain yields the intermediate sca ering function I (k, t ).

I (k, t ) = N −1 ρ (k, t )ρ (−k, 0)

(2.136)

with ρ (k, t ) de ned by eqn (2.108). e temporal Fourier transform of this, the dynamic structure factor S (k, ω), in principle may be measured by inelastic neutron sca ering. Spatially Fourier transforming I (k, t ) yields the van Hove function G (r , t ), a generalization of (r ) which measures the probability of nding a particle at position r at time t, given that a particle was at the origin of coordinates at time 0. All of these functions may be divided into parts due to ‘self’ (i.e. single-particle) motion and due to ‘distinct’ (i.e. collective) e ects. Other k-dependent variables may be de ned, and their time correlation

78 Statistical mechanics

1.0

0.8

0.6

c v v (t )

0.4

0.2

0.0

−0.20.0

0.2

0.4

0.6

0.8

1.0

t/ mσ 2/ϵ

Fig. 2.3 e velocity autocorrelation function for the Lennard-Jones uid (shi ed potential, rc = 2.5σ ) close to its triple point (T ∗ = 0.8, ρ∗ = 0.8).

functions are of great interest (Hansen and McDonald, ). For example, longitudinal and transverse momentum components may be de ned

p

(k, t )

=

1 V

i

pix (t ) exp ikxi (t )

p1⊥ (k,

t)

=

1 V

i

piy (t ) exp ikxi (t )

p2⊥ (k,

t)

=

1 V

i

piz (t ) exp ikxi (t )

(2.137a) (2.137b) (2.137c)

where for convenience we take k = (k, 0, 0) in the x direction, and xi = rix . ese quantities are useful for discussing hydrodynamic modes in liquids. ese functions may all be computed routinely in simulations, although, as always, the allowed k-vectors are restricted by small system sizes and periodic boundary conditions.
For systems of rigid molecules, the angular velocity ωi plays a role in reorientational dynamics analogous to that of vi in translation (see Chapter 3). e angular velocity correlation function ωi (t ) · ωi (0) may be used to describe rotation. Time-dependent orientational correlations may be de ned (Gordon, ; Steele, ; ) as straightforward generalizations of the quantities seen earlier. For a linear molecule, the time correlation function of rank- spherical harmonics is

c

(t ) = 4π

Ym

Ωi (t )

Y

∗ m

Ωi (0)

= P cos δγ (t )

(2.138)

where δγ (t ) is the magnitude of the angle turned through in time t. Note that, assuming an isotropic system, there are 2 + 1 rank- functions, all identical in form, corresponding to the di erent values of m.

Long-range corrections 79

Analogous formulae for non-linear molecules involve the Wigner rotation matrices. e starting point is the expansion of the probability density for rotation of the molecule through a set of Euler angles δ Ωi in time t:

ρ (δ Ωi ; t ) =

2+ 8π2

1

c

mm

(t )Dm∗m

(δ Ωi )

mm

which is equivalent to de ning the key correlation functions

c mm (t ) = Dmm δ Ωi (t ) .

e term on the right may be expressed in terms of the molecular orientations at time 0 and time t:

Dmm δ Ωi (t ) =

Dnm∗ Ωi (0) Dnm Ωi (t ) .

n=−

However, in an isotropic uid, averaging over the initial distribution gives an identical result for each of these terms

Dmm δ Ωi (t ) = (2 + 1) Dnm∗ Ωi (0) Dnm Ωi (t )

independent of n, and so (Steele, )

c mm (t ) = (2 + 1) Dnm∗ Ωi (0) Dnm Ωi (t ) .

(2.139)

When the symmetry of the phase is reduced, for example in liquid crystals, many more non-equivalent time correlation functions exist (Zannoni, ). ese quantities are experimentally accessible and the relationships between them are of great theoretical interest. For example, rst-rank autocorrelation functions may be related to infrared absorption, and second-rank functions to light sca ering. Functions of all ranks contribute to inelastic neutron sca ering spectra from molecular liquids.

2.8 Long-range corrections

As explained in Section 1.6, computer simulations frequently use a pair potential with a
spherical cuto at a distance rc. It becomes useful to correct the results of simulations to compensate for the missing long-range part of the potential. Contributions to the energy,
pressure, etc. for r > rc are frequently estimated by assuming that (r ) ≈ 1 in this region, and using eqns (2.103)–(2.105)

∞
Efull ≈ Ec + ELRC = Ec + 2πN ρ r 2v(r ) dr
rc

∞

(PV

)full

=

(PV

)c

+

(PV

)LRC

=

(PV

)c

−

2 3

πN

ρ

r 2w(r ) dr

rc

∞
µfull = µc + µLRC = µc + 4πρ r 2v(r ) dr
rc

(2.140) (2.141) (2.142)

80 Statistical mechanics

where Efull, (PV )full, µfull are the desired values for a liquid with the full potential, and Ec, (PV )c, µc are the values actually determined from a simulation using a potential with a cuto . For the Lennard-Jones potential, eqn (1.6), these equations become

EL∗RC

=

8 9

πN

ρ

∗rc∗−9

−

8 3

πN

ρ

∗rc∗−3

PL∗RC

=

32 9

πρ

∗

2rc∗

−9

−

16 3

πρ ∗ 2r c∗ −3

µ

∗ LRC

=

16 9

πρ

∗rc∗−9

−

16 3

πρ

∗r

∗−3 c

(2.143) (2.144) (2.145)

where we use Lennard-Jones reduced units (see Appendix B). In the case of the constantNV E and constant-NVT ensembles, these corrections can be applied to the results a er a simulation has run. However, if the volume or the number of particles is allowed to
uctuate (e.g. in a constant-N PT or constant-µVT simulation) it is important to apply the corrections to the calculated instantaneous energies, pressures, etc. during the course of a simulation, since they will change as the density uctuates: it is far more tricky to a empt to do this when the simulation is over.
For three-body potentials such as the Axilrod–Teller potential, the potential energy from the three-body term in a homogeneous uid is

V3

=

ρ3 6

dr1dr2dr3 v(3) (r12, r13, r23) (3) (r1, r2, r3).

In a simulation the three-body energy is calculated explicitly for

(2.146)

r12 ≤ rc and r13 ≤ rc and r23 ≤ rc.

(2.147)

For the rest of the space, the long-range part, the three-body distribution function can be approximated using the superposition approximation

(3) (r1, r2, r3) ≈ (2) (r12) (2) (r13) (2) (r23).

(2.148)

ere are six equivalent parts to this region, and the long-range correction is thus

V3 LRC = ρ3

dr1dr2dr3 v(3) (r12, r13, r23) (2) (r13) (2) (r23)

r12r>12r1>3r>cr23

(2.149)

where we have explicitly considered the region in which r12 > rc and thus (r12) = 1 (Barker et al., ). en transforming to bipolar coordinates we have

E3,LRC = V3 LRC = 8π2ρ2N

∞

r12

dr12 r12

dr13 r13

rc

0

r13
dr23 r23 v(3) (r12, r13, r23)

r 12 −r 13

(2) (r13) (2) (r23)

(2.150)

which can be evaluated accurately using the kind of Monte Carlo integration method described in Section 4.2. Note that the approximate estimate of the three-body long-range correction requires the two-body radial distribution function for the uid. is cannot

antum corrections 81

simply be set to one since both r13 and r23 can be less than rc. For the speci c case of

the Axilrod–Teller potential, which is a homogeneous function of order 9 (Graben and

Fowler, )

(PV )3,LRC = 3E3,LRC.

(2.151)

For more general potentials the Monte Carlo evaluation of eqn (2.149) can be adapted to calculate the long-range correction for the pressure and the chemical potential.

2.9 antum corrections
Most of this book will deal with the computer simulation of systems within the classical approximation, although we turn in Chapter 13 to the a empts which have been made to incorporate quantum e ects in simulations. antum e ects in thermodynamics may be measured experimentally via the isotope separation factor, while tunnelling, super uidity, etc. are clear manifestations of quantum mechanics.
Even within the limitations of a classical simulation, it is still possible to estimate quantum corrections of thermodynamic functions. is is achieved by expanding the partition function in powers of Planck’s constant, = h/2π (Wigner, ; Kirkwood,
). For a system of N atoms we have

QNVT

=

1 Λ3N N !

β2 N dr 1 − 24m i=1

∇ri βV (r) 2

exp[−βV (r)]

(2.152)

where Λ is de ned in eqn (2.24). e expansion accounts for the leading quantum-
mechanical di raction e ects; other e ects, such as exchange, are small for most cases of
interest. Additional details may be found elsewhere (Landau and Lifshitz, ; Mc arrie, ). is leads to the following correction to the Helmholtz free energy, ∆A = Aqu − Acl

∆Atrans

=

1 24

N

2β2

fi2 /m.

(2.153)

Here, as in Section 2.7, fi2 is the mean-square force on one atom in the simulation. Obviously, be er statistics are obtained by averaging over all N atoms. An equivalent expression is

∆Atrans

=

N Λ2ρ 48π

dr (r )∇2v(r )

=

N Λ2ρ 12

∞
r 2 (r )
0

d2v(r ) dr 2

+

2 r

dv(r ) dr

dr

(2.154)

assuming pairwise additive interactions. Additional corrections of order 4 can be esti-
mated if the three-body distribution function 3 can be calculated in a simulation (Hansen and Weis, ). Note that for hard systems, the leading quantum correction is of ord√er
: for hard spheres it amounts to replacing the hard sphere diameter σ by σ + Λ/ 8
(Hemmer, ; Jancovici, ). By di erentiating these equations, quantum corrections
for the energy, pressure, etc. can easily be obtained.
Equation (2.153) is also the translational correction for a system of N molecules where it is understood that m now stands for the molecular mass and fi2 is the mean-square

82 Statistical mechanics

force acting on the molecular centre of mass. Additional corrections must be applied for a molecular system, to take account of rotational motion (St. Pierre and Steele, ; Powles and Rickayzen, ). For linear molecules, with moment of inertia I , the additional term is (Gray and Gubbins, )

∆Arot

=

1 24

N

2β2 τi2 /I − N

2/6I

(2.155)

where τi2 is the mean-square torque acting on a molecule. e correction for the general asymmetric top, with three di erent moments of inertia Ixx , Iy y , and Izz , is rather more complicated

∆Arot

=

1 24

N

2β2

τi2x Ixx

+

τi2y Iy y

+

τi2z Izz

N2 −

2 −

Ixx

24 cyclic Ixx Iy y Izz

(2.156)

where the sum is over the three cyclic permutations of x, y, and z. ese results are independent of ensemble, and from them the quantum corrections to any other thermodynamic property can be calculated. Moreover, it is easy to compute the mean-square force and mean-square torque in a simulation.
Another, possibly more accurate, way of estimating quantum corrections has been proposed (Berens et al., ). In this approach, the velocity autocorrelation function is calculated and is Fourier transformed to obtain a spectrum, or density of states,

∞

cˆv v (ω) = dt exp(iωt ) vi (t ) · vi (0) / vi2

−∞

=

m 3kBT

∞
dt exp(iωt ) vi (t ) · vi (0) .
−∞

(2.157)

en, quantum corrections are applied to any thermodynamic quantities of interest, using the approximation that the system behaves as a set of harmonic oscillators, whose frequency distribution is dictated by the measured velocity spectrum. For each thermodynamic function a correction function, which would apply to a harmonic oscillator of frequency ω, may be de ned. e total correction is then obtained by integrating over all frequencies. For the Helmholtz free energy, the correction is given by

∆A = 3N kBT

∞ −∞

dω 2π

cˆv v (ω)

ln

exp

1 2

ω/kBT

− exp

−

1 2

ω/kBT

ω/kBT

(2.158)

which agrees with eqn (2.153) to O( 2). e rationale here is that the harmonic approximation is most accurate for the high-frequency motions that contribute the largest quantum corrections, whereas the anharmonic motions are mainly of low frequency, and thus their quantum corrections are less important. Simulations comparing the simulated and experimental heat capacity (Waheed and Edholm, ) seem to con rm this for liquid water, but anharmonicity for ice near the melting point is signi cant, and the method overestimates the heat of vaporization. is approach has also been applied to liquid methanol (Hawlicka et al., ) and ammonia under extreme conditions (Bethkenhagen

Constraints 83

et al., ). An alternative approach, however, is to incorporate quantum e ects associated with light nuclei, such as hydrogen, into the simulation method directly using the path-integral approach (see Section 13.4).
antum corrections may also be applied to structural quantities such as (r ). e formulae are rather complex and will not be given here, but they are based on the same formula eqn (2.152) for the partition function (Gibson, ). Again, the result is di erent for hard systems (Gibson, a,b).
When it comes to time-dependent properties, there is one quantum correction which is essential to bring the results of classical simulation in line with experiment. antum mechanical autocorrelation functions obey the detailed balance condition

CˆAA (ω) = exp(β ω)CˆAA (−ω)

(2.159)

whereas, of course, classical autocorrelation functions are even in frequency (Berne and

Harp, ). e e ects of detailed balance are clearly visible in experimental spectra, for

example in inelastic neutron sca ering, which probes S (k, ω); in fact experimental results

are o

en

converted

to

the

symmetrized

form

exp(

1 2

βω)S (k, ω) for comparison with

classical theories. Simple empirical measures have been advocated to convert classical

time correlation functions into approximate quantum-mechanical ones. Both the ‘complex

time’ substitutions

CA A (t )

→

C A A (t

−

1 2

i

β)

and CA A (t ) → CA A (t 2 − i βt )1/2

(Scho eld, ) (Egelsta , )

(2.160) (2.161)

result in functions which satisfy detailed balance. e former is somewhat easier to apply, since it equates the symmetrized experimental spectrum with the classical simulated one, while the la er satis es some additional frequency integral relations.

2.10 Constraints
In modelling large molecules such as proteins it may be necessary to include constraints in the potential model, as discussed in Section 1.3.3. is introduces some subtleties into the statistical mechanical description. e system of constrained molecules moves on a well-de ned hypersurface in phase space. e generalized coordinates corresponding to the constraints and their conjugate momenta are removed from the Hamiltonian. is a ects the form of the distribution function, and expressions for ensemble averages, in Cartesian coordinates (see Ryckaert and Cicco i, , Appendix). is system is not equivalent to a uid where the constrained degrees of freedom are replaced by harmonic springs, even in the limit of in nitely strong force constants (Fixman, ; Pear and Weiner, ; Chandler and Berne, ).
To explore this di erence more formally, we consider a set of N atoms grouped into molecules in some arbitrary way by harmonic springs. e Cartesian coordinates of the atoms are the 3N values r = {riα }, i = 1, 2, · · · N , α = x, y, z. e system can be described by 3N generalized coordinates q (i.e. the positions of the centre of mass of each molecule, their orientations, and vibrational coordinates). e potential energy of the system can be separated into a part, Vs, associated with the ‘so ’ coordinates qs (the translations,

84 Statistical mechanics

rotations, and internal conversions) and a part Vh associated with the ‘hard’ coordinates qh (bond stretching and possibly bond angle vibrations)

V (q) = Vs (qs) + Vh (qh).

(2.162)

If the force constants of the hard modes are independent of qs then the canonical ensemble average of some con gurational property A (qs) is (Berendsen and van Gunsteren, )

A (qs) det(G) exp −βVs(qs) dqs

A NVT =

det(G) exp −βVs(qs) dqs

(2.163)

where det(G) is the determinant of the mass-weighted metric tensor G, which is associated with the transformation from Cartesian to generalized coordinates

Gk

N
=
i =1

α

mi

∂r i α ∂qk

∂r i α ∂q

.

(2.164)

G involves all the generalized coordinates and is a 3N × 3N matrix. If the hard degrees of freedom are actually constrained they are removed from the matrix G:

A

s NVT

=

A (qs) det(Gs) exp −βVs (qs) dqs det(Gs) exp −βVs (qs) dqs

(2.165)

where

Gks

N
=
i =1

α

mi

∂r i α ∂qks

∂r i α ∂qs

.

(2.166)

Gs is a sub-matrix of G and has the dimensions of the number of so degrees of freedom.

e simulation of a constrained system does not yield the same average as the simulation

of an unconstrained system unless det(G)/ det(Gs) is independent of the so modes. In

the simulation of large exible molecules, it may be necessary to constrain some of the

internal degrees of freedom, and in this case we would probably require an estimate of

A

NVT

rather than

A

s N

V

T

.

Fixman

(

) has suggested a solution to the problem of

obtaining A NVT in a simulation employing constrained variables. A term,

Vc

=

1 2

kBT

ln det(H)

(2.167)

is added to the potential Vs. det(H) is given by

det(H) = det(G)/ det(Gs).

(2.168)

Substituting Vs + Vc as the potential in eqn (2.165) we recover the unconstrained average of eqn (2.163). e separate calculation of G and Gs to obtain their determinants is di cult. However, det(H) is the determinant of a simpler matrix

Hk

N
=
i =1

α

mi

∂qkh ∂r i α

∂qh ∂r i α

(2.169)

which has the dimensions of the number of constrained (hard) degrees of freedom.

Landau free energy 85

As a simple example of the use of eqn (2.169) consider the case of a butane molecule (see Fig. 1.10). In our simpli ed butane, the four united atoms have the same mass m, the bond angles and torsional angles are free to change but the three bond lengths, C1−C2, C2−C3, and C3−C4 are xed. e 3 × 3 matrix H is
2m −m cos θ 0 −m cos θ 2m −m cos θ
0 −m cos θ 2m

and

det(H) ∝ 2 + sin2 θ + sin2 θ .

(2.170)

Since θ and θ can change, H should be included through eqn (2.167). However, it is possible to use a harmonic bond-angle potential, which keeps the bond angles very close to their equilibrium values. In this case H is approximately constant and might be neglected without seriously a ecting A NVT . If we had also constrained the bond angles in our model of butane, then det(H) would have been a function of the torsional angle ϕ as well as the θ angles. us H can change signi cantly when the molecule converts from the trans to the gauche state and Vc must be included in the potential (van Gunsteren,
). In the case of a completely rigid molecule, det(H) is a constant and need not be included. We shall discuss the consequences of constraining degrees of freedom at the appropriate points in Chapters 3 and 4.

2.11 Landau free energy

e idea of a free energy which depends on certain constrained ‘order parameters’, o en termed a ‘Landau’ free energy because of its prominence in the Landau theory of phase transitions, is very common in molecular simulation. Consider a single parameter q(r), a generalized coordinate, which can be wri en as a function of all the atomic positions r. (In the most general case, it might also depend on momenta, but the con gurational case is by far the most common). en, quite generally, we may write the probability density function for q in the canonical ensemble

ρ (q) = δ [q − q(r)] =

dr δ [q − q(r)] exp[−βV (r)] dr exp[−βV (r)]

≡

Q

ex NV

T

Q

ex NV

(q)
T

.

(2.171)

Here we have de

ned

the

numerator

Q

ex NV

T

(q)

as the excess partition function of a

system which is restricted to con gurations for which the generalized coordinate q(r)

takes the value q. e function ρ (q) is, of course, normalized such that dq ρ (q) = 1.

Taking

logarithms,

and

identifying

A

=

−kBT

ln QNexV T

,

we

may

use

Q

ex NV

T

(q)

to

de

ne a

q-dependent Helmholtz free energy, which we usually write as F (q):

F (q) = A − kBT ln ρ (q) = A − kBT ln δ [q − q(r)] .

(2.172)

e thermodynamic Helmholtz free energy is frequently omi ed from this equation, since usually one is interested in changes in F (q) as a function of q. For example, in discussing phase transitions in the Ising model of ferromagnetic systems, below the critical point, q might be the overall magnetization of the system, and F (q) would typically have

